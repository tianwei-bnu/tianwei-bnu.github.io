<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[公子天的博客]]></title>
  <link href="tianwei.github.io/atom.xml" rel="self"/>
  <link href="tianwei.github.io/"/>
  <updated>2017-03-23T15:41:35+08:00</updated>
  <id>tianwei.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning读书笔记（二）：线性代数]]></title>
    <link href="tianwei.github.io/deeplearning_note2.html"/>
    <updated>2017-03-23T15:17:46+08:00</updated>
    <id>tianwei.github.io/deeplearning_note2.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">基本术语</h2>

<ul>
<li>Scalars：标量。通常是一个实数。</li>
<li>Vectors：向量。由一组标量组成的一维有序数组；若无特殊说明通常是指列向量。通过下标索引可以找到其对应的元素。对应的几何意义是笛卡尔空间的一个点。</li>
<li>Matrices：矩阵（特指二维矩阵）。由一组标量组成的二维数组。</li>
<li>Tensors：张量。一个高维矩阵（\(D \geq 3\)）</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_1">基本运算</h2>

<ul>
<li>Transpose：\((A^\top)_{i, j} = A_{j, i}\)</li>
<li>Broadcasting：将向量的元素按行或按列广播为一个矩阵，从而与另一个矩阵进行计算</li>
<li>Multiplying：

<ol>
<li>matrix product：\((AB)_{i,j}=\sum_k{A_{i,k}B_{k,j}}\)</li>
<li>element-wise product：\((A \odot B)_{i,j}=A_{i,j}B_{i,j}\)</li>
<li>dot product：\(x^\top y\)</li>
<li>matrix-vector product：\(Ax=b\) 对应着一组线性方程组。</li>
</ol></li>
<li>Inverse Matrices：\(A^{-1}A=I_n\) （逆矩阵主要被用于理论分析的工具）</li>
</ul>

<h2 id="toc_2">线性相关和矩阵的Span</h2>

<ul>
<li>线性方程组\(Ax=b, A \in \mathbb{R}^{m \times n}\)的解的可能性：

<ul>
<li>唯一解（对于任意向量\(b \in \mathbb{R}^m\)）</li>
<li>无穷解</li>
<li>无解</li>
</ul></li>
<li>Linear Combination：一组向量的线性组合被定义为是将每一个向量都乘以一个标量系数并求和的结果：\(\sum_i{c_iv^{(i)}}\)

<ul>
<li>矩阵-向量乘积\(Ax\)的意义：\(Ax=\sum_i{x_i A_{:, i}}\) 矩阵列向量的一种线性组合（linear combination）</li>
</ul></li>
<li>Span：一组向量的Span被定义为这组向量所有可能的线性组合所构成的向量的集合。

<ul>
<li>矩阵的列空间（column space）：矩阵列向量的span</li>
<li>判断线性方程组\(Ax=b\)有无解等价于判断向量\(b\)是否在矩阵\(A\)的列空间里</li>
</ul></li>
<li>Linear Dependence：对于一组向量的集合，如果其中存在一个向量是其他向量的线性组合，则称这组向量是线性相关的。

<ul>
<li>线性方程组\(Ax=b\)对任意\(b \in \mathbb{R}^m\)至少有一个解（矩阵\(A \in \mathbb{R}^{m \times n}\)的列空间等于\(\mathbb{R}^m\)）的充要条件：矩阵\(A\)至少有一组\(m\)个线性不相关的列向量</li>
<li>线性方程组\(Ax=b\)对任意\(b \in \mathbb{R}^m\)有唯一解的充要条件：矩阵\(A\)仅有一组\(m\)个线性不相关的列向量（\(n = m\)）。此时，矩阵\(A\)可逆。</li>
</ul></li>
</ul>

<h2 id="toc_3">范式Norms</h2>

<ul>
<li>向量的范式：用于测量一个向量的大小</li>
<li>\(L^p\)范式：\(||x||_p=\sqrt[p]{\sum_i{|x_i|^p}}\) 将一个向量映射为一个非负实数

<ul>
<li>基本性质：

<ol>
<li>\(L^p(x)=0 \Rightarrow x=0\)</li>
<li>\(L^p(x+y) \leq L^p(x) + L^p(y)\)</li>
<li>\(\forall \alpha \in \mathbb{R}, L^p(\alpha x) = |\alpha| L^p(x)\)</li>
</ol></li>
</ul></li>
<li>常见的几种范式：

<ul>
<li>\(L^2\)范式：\(||x||_2=\sqrt{\sum_i{x_i^2}}\) 又称欧几里得范式（度量了向量\(x\)到原点的欧氏距离）。在机器学习领域里更常用的是\(L^2\)范式的平方（即\(x^\top x\)）。其特点是易于对向量\(x\)的每个元素求偏导，缺陷是在原点附近平方值的变化曲率太小。</li>
<li>\(L^1\)范式：\(||x||_1 = \sum_i{|x_i|}\) 在空间的任意点都有着相同的变化率，适用于对原点附近点的大小极为敏感的应用场景。</li>
<li>\(L^\infty\)范式：\(||x||_\infty = \max_i{|x_i|}\)</li>
<li>Frobenius Norm：\(||A||_F = \sqrt{\sum_{i,j}{A^2_{i,j}}}\) 矩阵的\(F\)范式，类似于向量的\(L^2\)范式，常用于在深度学习中测量一个矩阵的大小。</li>
</ul></li>
</ul>

<h2 id="toc_4">几种特殊矩阵和向量</h2>

<ul>
<li>Diagonal Matrices：对角矩阵。仅在对角元上的元素非零。对角矩阵的向量乘法和求逆都十分简单，因此常被用于通用算法中的假设以简化计算。</li>
<li>Symmetric Maxtrix：对称矩阵。满足：\(A=A^\top\)。通常由一个接受双参数且对参数顺序不敏感的函数产生（例如距离函数）</li>
<li>Unit Vector：单元向量。满足：\(||x||_2 = 1\)</li>
<li>Orthogonal Vectors：正交向量。两个向量被称为正交当且仅当这两个向量满足：\(x^\top y = 0\)。在\(\mathbb{R}^n\)空间里，最多只能找到\(n\)个非零范式的相互正交的向量。如果两个向量不仅正交且\(L^2\)范式均为1，称之为正交归一化向量（orthonormal）</li>
<li>Orthogonal Matrix：正交矩阵。是一个行向量相互<strong>正交归一</strong>且列向量相互<strong>正交归一</strong>的方阵：\(A^\top A = A A^\top = I\)（也即\(A^{-1} = A^\top\)）</li>
</ul>

<h2 id="toc_5">特征分解（Eigendecomposition）</h2>

<ul>
<li>特征向量（Eigenvector）：一个矩阵\(A\)的特征向量\(v\)是一个满足\(Av=\lambda v\)的非零向量（矩阵\(A\)作用在这个向量上仅改变其大小，不改变其方向）。其中，\(\lambda\)又被称为矩阵\(A\)的特征值（Eigenvalue）。显然，同一个特征值对应的特征向量有无数多个（如果\(v\)是矩阵\(A\)的特征向量，则\(sv, s \in \mathbb{R}, s \ne 0\)也是一个特征向量）。我们平时所说的特征向量通常是指\(L^2\)范式为\(1\)的单位向量。

<ul>
<li>矩阵与其特征向量的几何关系：矩阵的本质是对其特征向量构成空间的一个尺寸变换（scaling）。设若\(x=v\mu\)，其中\(v\)是矩阵\(A\)的特征向量构成的矩阵，则有：\[Ax=Av\mu=(\lambda v) \mu\]</li>
</ul></li>
<li>特征分解（Eigendecomposition）：将矩阵\(A\)分解为两个特征矩阵乘积的形式：\(A=V diag({\lambda}) V^{-1}\)。其中\(V=[v^{(1)},...,v^{(n)}]\)是由\(n \times n\)矩阵\(A\)的\(n\)个相互独立的特征向量（确保\(V^{-1}\)的存在）按列组成的特征矩阵；\(\lambda = [\lambda_1, ..., \lambda_n]^\top\)是对应的特征值组成的列向量。（这个等式的证明也很简单，只需在等式两边各自右乘一个矩阵\(V\)即可。）

<ul>
<li>并非所有矩阵都存在特征分解</li>
</ul></li>
<li><strong>实对称矩阵</strong>的特征分解：\(A=Q \Lambda Q^\top\)。其中，\(Q\)是一个由实对称矩阵\(A\)的特征向量组成的<strong>正交矩阵</strong>（不仅仅是linear independence）

<ul>
<li>应用： 二次型\(f(x) = x^\top A x\)的极值。其中\(||x||_2=1\)

<ul>
<li>极大值：最大特征值及其对应的特征向量</li>
<li>极小值：最小特征值及其对应的特性向量</li>
</ul></li>
</ul></li>
</ul>

<h2 id="toc_6">奇异值分解（Singular Value Decomposition）</h2>

<ul>
<li>SVD分解：\(A=UDV^\top\)

<ul>
<li>更通用的矩阵分解方式</li>
<li>\(U\)：\(m \times m\)的正交矩阵。其列向量被称为矩阵\(A\)的<em>左奇异向量</em>，同时也是矩阵\(AA^\top\)的特征向量</li>
<li>\(D\)：\(m \times n\)的对角矩阵。对角元上的元素被称为矩阵\(A\)的<em>奇异值</em>，同时也是矩阵\(A^\top A\)和\(AA^\top\)的特征值的平方根</li>
<li>\(V\)：\(n \times n\)的正交矩阵。其列向量被称为矩阵\(A\)的<em>右奇异向量</em>，同时也是矩阵\(A^\top A\)的特征向量</li>
</ul></li>
<li>应用：Moore-Penrose伪逆（对任意矩阵求其伪逆）

<ul>
<li>\(A^+ = V D^+ U^\top\)，其中对角矩阵\(D\)的伪逆是对其非零元素取倒数再转置得到的</li>
</ul></li>
</ul>

<h2 id="toc_7">迹（Trace）和行列式（Determinant）</h2>

<ul>
<li>Trace：\(Tr(A) = \sum_i{A_{i,i}}\)</li>
<li>Determinant：\(det(A) = \prod_i{\lambda_i}\)</li>
</ul>

<h2 id="toc_8">例子：PCA主成分分析</h2>

<ul>
<li>动机：对于一组\(n\)维空间的向量点，找到一个低维的特征表达，并尽可能少地丢失信息：\(x \approx g(f(x))\)。其中\(f(x)\)是编码函数（将高维向量编码到低维空间）；\(g(x)\)是解码函数（将低维向量解码到高维空间）</li>
<li>问题一：编码／解码函数长什么样？

<ul>
<li>假设（解码函数）：\(g(c) = Dc, D \in \mathbb{R}^{n \times l}\)。即用一个\(n \times l\)的矩阵对编码后的点\(c\)进行解码。其中，\(D\)是一个列向量正交且归一的矩阵（由于\(n &gt; l\)故行向量不可能正交）</li>
<li>目标（编码函数）：寻求一个最优的低维空间编码\(c^*\)，使得由\(c^*\)在高维空间重建后的点\(g(c^*)\)与原始的点\(x\)最近，也即最小化\(L^2\)范式的解\(c\)：\[c^*=\arg\min_c{||x - g(c)||_2}\]</li>
<li>最优解（编码函数）：\(f(x) = c^* =D^\top x\)</li>
</ul></li>
<li>问题二：如何找到解码矩阵\(D\)？

<ul>
<li>目标：寻求一个最优的矩阵\(D\)，使得在输入的所有数据上的重建误差最小（Frobenius范式最小）：\[D^* = \arg\min_D{\sqrt{\sum_{i,j}{(x^i_j - DD^\top x^i_j)^2}}}\]
其中约束条件为：\(D^\top D = I_l\)</li>
<li>结论：<strong>\(D\)是由矩阵\(X^\top X\)的\(l\)个最大的特征值对应的特征向量构成的矩阵</strong>（利用到了实对称矩阵对应二次型的极值）</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning读书笔记（一）：导论]]></title>
    <link href="tianwei.github.io/deeplearning_note1.html"/>
    <updated>2017-03-23T15:15:41+08:00</updated>
    <id>tianwei.github.io/deeplearning_note1.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">什么是深度学习</h2>

<ul>
<li><p>背景：两类AI任务</p>

<ul>
<li>Formal Task：易于抽象和描述成计算机所能理解的表达，也因此在AI发展的早期就得到了很好的解决（例如国际象棋）</li>
<li>Intuitive Task：依赖于人类的主观直觉，难于用计算机语言进行规范性描述的任务（例如语音和图像的识别）</li>
</ul>

<span id="more"></span><!-- more --></li>
<li><p>Deep Learning的第一种解读：解决Intuitive Task的尝试</p>

<ol>
<li>Rule-based System：

<ul>
<li>基本思想：尝试人为地将知识“硬编码”为一种规则化语言。AI系统将基于庞大的人工规则库进行逻辑推断，从而解决Intuitive Task。</li>
<li>缺陷：逻辑和知识的表达过于刻板，无法处理多样的任务</li>
<li>代表作：Cyc</li>
</ul></li>
<li>Classical Machine Learning：

<ul>
<li>基本思想：尝试训练AI系统从原始数据中自主发现Pattern并获取对知识的编码。</li>
<li>缺陷：机器学习算法的performance严重依赖于数据的特征表达；然而很多时候我们并不知道什么样的特征是有效的（这就需要特征工程）</li>
<li>代表作：Logistic Regression；Naive Bayes</li>
</ul></li>
<li>Representation Learning：

<ul>
<li>基本思想：训练AI系统不仅可以学习到特征到目标的映射关系，同时还可以学习到特征的表达。</li>
<li>优势：相对于人工设计的特征，机器自我学习到的特征表达有着更好的performance。同时，对于一个全新的task，系统也可以在较短的时间内和较少的人工干预下迅速进化。</li>
<li>代表作：autoencoder</li>
</ul></li>
<li>Deep Learning：

<ul>
<li>基本思想：无论是人工设计特征还是自我学习特征，其核心目标都是尽可能分离出影响观测数据表层变化的底层因子（factors）。而直接抽取high-level的抽象因子的难度不亚于去解决原始问题本身。基于此，深度学习提出了概念层次化的网络结构（the hierarchy of concepts），将high-level的复杂特征用low-level的简单特征表达，从而解决这个问题。</li>
<li>代表作：MLP（multilayer perceptron）</li>
</ul></li>
</ol></li>
<li><p>Deep Learning的第二种解读：a multi-step computer program</p>

<ul>
<li>模型的每一层特征表达都对应着某种计算序列下的内存状态</li>
<li>并非所有的特征都对应着输入数据的因子编码——也有可能是存储了某种计算机状态（例如指针或计数器）</li>
</ul></li>
<li><p>深度真的重要吗？</p>

<ul>
<li>模型深度的度量方法：

<ol>
<li>计算序列的深度（依赖于计算元单元的定义）</li>
<li>概念层级的深度（网络层级中可能存在回路）</li>
</ol></li>
<li>深度学习的核心是用<strong>一组学习函数／概念的层次组合</strong>来表征这个世界</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PRML读书笔记（一）：概率论，决策论，与信息论]]></title>
    <link href="tianwei.github.io/prml_note1.html"/>
    <updated>2017-03-23T15:05:32+08:00</updated>
    <id>tianwei.github.io/prml_note1.html</id>
    <content type="html"><![CDATA[
<p>什么是模式识别（Pattern Recognition）？</p>

<p>按照Bishop的定义，模式识别就是用机器学习的算法从数据中挖掘出有用的pattern。</p>

<p>人们很早就开始学习如何从大量的数据中发现隐藏在背后的pattern。例如，16世纪的Kepler从他的老师Tycho搜集的大量有关于行星运动的数据中发现了天体运行的规律，并直接导致了牛顿经典力学的诞生。然而，这种依赖于人类经验的、启发式的模式识别过程很难复制到其他的领域中。例如手写数字的识别。这就需要机器学习的技术了。（顺便提一下，开普勒定律在物理学中只是一种唯象的理论，它只对物理事实抽象出概括性的描述，而没有解释内在的原因，也就是牛顿的万有引力定律（其实在更高的层次上，万有引力也是一种唯象的理论，它的解释由量子引力理论给出）。这也意味着，在大数据的机器学习时代，我们挖掘出来的知识更多的也只是一种相关性的规律。）</p>

<span id="more"></span><!-- more -->

<p>在手写数字识别的问题中，一部分手写图片作为<strong>训练集（training set）</strong>，被用来学习一个事先给定的模型的参数。另一部分图片作为<strong>测试集（test set）</strong>，被用来评估学习到的模型在新数据上的<strong>泛化能力（generalization）</strong>。训练集与测试集里的手写图片对应的数字都是已知的。这些数字又被称之为<strong>目标向量（target vector）</strong>，是模型学习的分类目标。一个机器学习算法最终得到的结果，就是一个将输入变量\(x\)（这里的手写数字图片）映射到目标向量空间里的函数\(y(x)\)。</p>

<p>一般来说，在模型训练之前，都有一个<strong>数据预处理（preprocess）</strong>的过程。例如，在识别手写数字的问题中，我们需要先将训练集里的图片变换到同一个尺寸下，从而使模型的训练更加统一。这个过程也被称之为<strong>特征提取（feature extraction）</strong>。同时，预处理后的特征往往也会使模型的计算更加高效。要注意的是，如果我们对训练集里的数据进行了预处理，那么对于测试集里的数据，我们也要进行同样的操作。</p>

<p>对于那些训练样本中带有目标向量的机器学习问题，我们称之为<strong>有监督学习（supervised learning）</strong>。例如前面提到的手写数字识别问题。进一步地，如果目标向量是离散的，我们称之为<strong>分类（classification）</strong>问题；反之，如果目标向量是连续的，我们称之为<strong>回归（regression）</strong>问题。</p>

<p>另一类机器学习问题的训练样本仅有输入变量的特征，我们称之为<strong>无监督学习</strong>。例如对数据中相似样本的归并（聚类）问题。</p>

<p>最后一类机器学习问题被称之为<strong>强化学习（reinforcement learning）</strong>。它是寻求在一个给定状态下的行为决策，以最大化最终的收益。象棋AI就是一个典型的强化学习的例子。与有监督学习不同，强化学习要解决的问题没有标记好的输出；但往往会通过与环境的互动来改变自己的状态和收益，并因此而学习到什么是一个好的输出。注意，强化学习的每一步决策不仅会影响到当下的收益，还会影响未来的回报。这就涉及到一个<strong>信用分配（credit assignment）</strong>的问题：即如何将最终的回报分配到导致该回报的每一步的决策上？强化学习另一个特征是在<strong>exploration</strong>和<strong>exploitation</strong>之间的trade－off：对任意一个的过度偏向都会产生较差的结果。</p>

<p>接下来，我们将从一个具体的例子入手展现机器学习过程里的一些基本概念；然后，我们将简单介绍机器学习理论的三大基础：<strong>概率论（probability theory）</strong>，<strong>决策论（decision theory）</strong>，和<strong>信息论（information theory）</strong>。</p>

<h2 id="toc_0">一个例子：多项式拟合</h2>

<p>作为例子，我们通过一个带随机噪声的产生函数\(sin(2{\pi}x)\)来构造训练数据，并期望一个机器学习算法能从有限的数据中学习到这个正弦函数。这种数据的构造方式暗合真实世界的法则：即数据在遵循一条潜在的规律同时也充满了不确定性。<em>（后面我们会看到，我们是如何在一个概率论的框架下描述这种不确定性，同时给出一个最好的预测。）</em></p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902528277140.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>我们可以用一个多项式函数来拟合训练数据（这是因为任意一个连续可导的函数总可以通过多项式展开来逼近）：\[y(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^{M}{w_jx^j}\]</p>

<p>注意，尽管这个函数是关于\(x\)的非线性函数，但却是关于系数\(w\)的线性函数。类似多项式函数这种相对于未知参数是线性的模型，具有一些重要的性质，我们称之为<strong>线性模型（linear model）</strong>。</p>

<p>我们通过最小化一个<strong>误差函数（error function）</strong>来求解模型的参数。误差函数是关于参数\(w\)的一个函数；它衡量我们的预测值\(y(x,w)\)与真实值\(t\)的差距。一个常用的误差函数是误差平方和，它的基本形式如下：\[E(w)=\frac{1}{2}\sum_{n=1}^{N}{\{y(x_n,w)-t_n\}^2}\]<br/>
（<em>在后面介绍到最大似然法时，我们会看到误差函数为什么会取这种形式</em>）</p>

<p>令这个函数对\(w\)的一阶偏导等于0，得到\(M＋1\)个一阶方程组，就可以求解出最优拟合参数\(w^*\)了。</p>

<p>然而，我们还有一个问题：如何确定模型的阶数\(M\)？这也被称之为<strong>模型选择（model comparison or model selection）</strong>问题。我们可以通过评估模型在新数据集上的泛化能力来衡量模型的好坏。具体来说，我们从训练集中抽取一部分数据作为<strong>验证集（validation set）</strong>；对于\(M\)的每一种可能取值，我们先用余下的训练集找到最优的参数\(w^*\)，然后再在验证集上评估这个模型的误差\(E(w^*)\)（或者是root-mean-square error：\(E_{RMS}=\sqrt{{2E(w^*)}/{N}}\)）。我们将选择泛化误差最小的模型作为最终的模型。</p>

<p>通过这种方式，我们发现，\(M\)取值过低，模型有可能会<strong>欠拟合（under-fitting）</strong>；而\(M\)取值过高，模型又有可能会<strong>过拟合（over-fitting）</strong>。</p>

<p>这是为什么呢？</p>

<p>理论上，一个低阶的多项式函数只是更高阶的多项式函数的一个特例；而一个高阶的多项式也更加逼近真实的产生函数\(sin(2{\pi}x)\)。因此，高阶的模型似乎没道理比低阶模型的表现更差。</p>

<p>但是，如果我们仔细观察那些过拟合的高阶模型学习到的参数\(w\)，会发现它们的量纲往往异乎寻常的大。这使得模型虽然可以准确地预估训练集中的数据，但也展现出了巨大的波动性。换句话说，强大的高阶模型过多地学习了训练数据中的噪声。如果我们引入更多的训练数据，这种过拟合的问题会得到一定程度的缓解。</p>

<p>简而言之，虽然模型越复杂，解释能力也越强，但我们往往需要更多的数据来训练一个复杂的模型。<em>（后面会发现，这种过拟合本质上是由于最大似然法这类对模型参数进行点估计的算法所带来的bias造成的。通过一种贝叶斯的方法，我们可以从根本上避免过拟合的问题。）</em></p>

<p>一种在有限的数据集下也能训练一个高阶模型的技术是<strong>正则化（regularization）</strong>，即在传统的误差函数中引入一个对参数\(w\)量纲的惩罚项，以抑制\(w\)的过度发散：\[\tilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}{\{y(x_n,w)-t_n\}^2}+\frac{\lambda}{2}||w||^2\]<br/>
其中，\(||w||^2=w_1^2+w_2^2+...+w_M^2\)，参数\(\lambda\)控制正则项和误差项的相对权重。对于一个合适的\(\lambda\)取值，我们用较少的数据也可以训练出一个泛化的高阶模型。</p>

<p>然而，过低或过高的\(\lambda\)依然会导致过拟合或欠拟合的问题。本质上，<strong>正则化只是将对模型复杂度的控制由模型的阶数\(M\)转移到了惩罚因子\(\lambda\)上</strong>。我们依然需要一个验证集来优化模型的复杂度。</p>

<h2 id="toc_1">概率论</h2>

<p>在模式识别和机器学习的研究领域里，<strong>不确定性（uncertainty）</strong>是一个非常核心的概念。接下来要介绍的概率论为不确定性的量化提供了一个统一的框架，因此也构成了整个机器学习研究的理论基础。</p>

<h3 id="toc_2">基本知识点</h3>

<p>概率的基本定义由频率学派给出：在一系列试验中某一事件发生的频率称之为该事件发生的概率。从这个定义出发，我们很容易得到概率论的两个基本法则：<strong>sum rule</strong>和<strong>product rule</strong>：<br/>
\[p(X)=\sum_{Y}{p(X,Y)}\]<br/>
\[p(X,Y)=p(Y|X)p(X)\]<br/>
其中，\(p(X)\)对应\(X\)发生的<strong>边际概率（marginal probability）</strong>；\(p(X,Y)\)对应\(X\)和\(Y\)同时发生的<strong>联合概率（joint probability）</strong>；\(p(Y|X)\)对应给定\(X\)下\(Y\)发生的<strong>条件概率（conditional probability）</strong>。这两个公式构成了我们将要用到的所有概率论知识的基础。</p>

<p>由product rule出发，同时结合联合概率的对称性，我们立即得到一个十分重要的公式，<strong>贝叶斯定理（Bayes&#39; theorem）</strong>：<br/>
\[p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}\]</p>

<p>贝叶斯定理给出了<strong>后验概率（posterior probability）</strong>——\(p(Y|X)\)和<strong>先验概率（prior probability）</strong>——\(p(Y)\)之间的关系。</p>

<h3 id="toc_3">贝叶斯概率</h3>

<p>然而，并非所有随机事件的发生都是可重复的。因此，我们很难用频率学派的观点解释诸如明天有雨这类事件发生的概率。对这类事件的不确定性的衡量，就是概率的贝叶斯解释（Bayesian view）。</p>

<p>回到之前的多项式拟合的例子。从频率学派的角度来看，我们将目标变量\(t\)视为一个随机变量似乎更加合理（可以通过固定输入变量\(x\)的值统计\(t\)的频率分布）。而从贝叶斯学派的观点来看，我们可以将模型的参数\(w\)（甚至是整个模型）视为一个随机变量，衡量它们的不确定性（尽管\(w\)是不可重复的）。</p>

<p>具体来说，假设我们知道了\(w\)的先验概率分布\(p(w)\)，那么，通过贝叶斯公式，我们可以将这一先验概率转化为给定观测数据后的后验概率\(p(w|D)\)：<br/>
\[p(w|D)=\frac{p(D|w)p(w)}{p(D)}\]<br/>
从而，我们得到了一个已知数据\(D\)下关于模型参数\(w\)的不确定性的定量表达。</p>

<p>在这个公式里，概率分布\(p(D|w)\)又被称为<strong>似然函数（likilihood function）</strong>，它给出了不同\(w\)下数据集\(D\)发生的概率。从而，贝叶斯定理又可以写成：\[posterior \propto likelihood \times prior\]</p>

<p>无论是在频率学派还是贝叶斯学派的框架下，似然函数都扮演着重要的角色。不同的是，频率学派认为，模型的参数是固定的，观测到的数据则是给定参数下的一个随机事件。因此，他们通过一种叫做<strong>最大似然法（maximum likelihood）</strong>的方法来预估参数：即寻找使观测到的数据集\(D\)发生的概率最大的参数\(w\)（<em>后面我们将会看到最大似然法与前面提到的最小化误差函数的关系</em>）。而贝叶斯学派则认为模型的参数服从一个概率分布，因此，似然函数只是获取后验概率的桥梁。</p>

<p><em>（关于频率学派和贝叶斯学派孰优孰劣的问题，各家各执一言，此处不做讨论。只需知道，PRML这本书更多的是介绍贝叶斯学派的观点，而Andrew在Coursera上的课则是频率学派的经典观点。）</em></p>

<h3 id="toc_4">高斯分布</h3>

<p>关于高斯分布的定义和性质，很多概率论的书上都有介绍，这里不再赘述。本小节主要介绍如何运用频率学派的最大似然法从一堆服从高斯分布的数据中拟合出分布的参数：\(\mu\)与\(\sigma^2\)。</p>

<p>由独立同分布假定，我们的数据集\(\mathbf{x} = (x_1,...,x_D)^T\)在给定参数\(\mu\)和\(\sigma^2\)下的条件概率分布为：<br/>
\[p(\mathbf{x}|\mu,\sigma^2)=\prod_{n=1}^N{\mathcal{N}(x_n|\mu,\sigma^2)}\]<br/>
这就是高斯分布的似然函数。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902528500482.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>对这个似然函数取对数（<em>后面我们会看到如何从信息熵的角度来解释这一行为</em>），我们得到：\[\ln{p(\mathbf{x}|\mu,\sigma^2)}=-\frac{1}{2\sigma^2}\sum_{n=1}^N{(x_n-\mu)^2}-\frac{N}{2}\ln{\sigma^2}-\frac{N}{2}\ln{2\pi}\]</p>

<p>将上式分别对\(\mu\)和\(\sigma^2\)求导，并令其等于0，我们得到了\(\mu\)和\(\sigma^2\)的最大似然预估：<br/>
\[\mu_{ML}=\frac{1}{N}\sum_{n=1}^N{x_n}\]<br/>
\[\sigma^2_{ML}=\frac{1}{N}\sum_{n=1}^N{(x_n-\mu_{ML})^2}\]<br/>
等式右边都是关于数据集\(\mathbf{x}\)的函数。</p>

<p>可以证明，通过这种方法得到的参数预估是<strong>有偏（bias）</strong>的：<br/>
\[E[\mu_{ML}]=\mu\]<br/>
\[E[\sigma^2_{ML}]=(\frac{N-1}{N})\sigma^2\]<br/>
也就是说，我们会<strong>低估（underestimate）</strong>真实的分布方差。一般来说，模型的参数越多（复杂度越高），由最大似然法预估出来的参数越不准确。而这种有偏的预估，是导致模型过拟合的根本原因（过度拟合了训练样本，而偏离了真实世界）。不过，注意到随着样本数\(N\)的增长，这种偏差会越来越小。这就解释了，为什么增大训练样本的容量可以一定程度上缓解过拟合的问题。</p>

<h3 id="toc_5">回到多项式拟合</h3>

<p>接下来，我们将回到多项式拟合的例子，看看在统计学的视角下，频率学派和贝叶斯学派各自是如何解决这个问题的。</p>

<p>在多项式拟合的问题中，我们的目标是，<strong>在由\(N\)个输入变量\(\mathbf{x}=(x_1,...,x_N)^T\)及其对应的目标值\(\mathbf{t}=(t_1,...,t_N)^T\)构成的训练集的基础上，对新输入变量\(x\)的目标值\(t\)做出预测。</strong>用概率语言来描述，我们可以将目标变量\(t\)的这种不确定性用一个概率分布来表达。我们假定，<strong>\(t\)服从以\(y(x,w)\)为均值的高斯分布</strong>：<br/>
\[p(t|x,w,\beta)=\mathcal{N}(t|y(x,w),\beta^{-1})\]</p>

<p>有了\(t\)的概率分布，我们就可以用最大似然法在训练集上求解模型的参数（\(w\)和\(\beta\)）:<br/>
\[\ln{p(\mathbf{t}|\mathbf{x},w,\beta)} = -\frac{\beta}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{N}{2}\ln{\beta}-\frac{N}{2}\ln{2\pi}\]<br/>
我们发现，略去与\(w\)无关的后两项后，我们得到了前文提到的误差函数！也就是说，对于\(w\)的求解而言，<strong>最大似然法等价于最小化误差平方和！我们的误差函数是在目标变量服从高斯分布的假定下用最大似然法推导出来的一个自然结果！（注意这一结论与\(y(x,w)\)的具体形式无关）</strong></p>

<p>一旦得到了\(w\)和\(\beta\)的最大似然估计，我们就可以对新变量\(x\)的目标值做出预估。注意，这里是对\(t\)的分布预估（predictive distribution），而非点估计（point estimate）。<em>（在后面的decision theory章节中会介绍如何从预估的分布得到预测值）</em></p>

<p>以上是频率学派对多项式拟合算法的解释。</p>

<p>而在贝叶斯学派看来，参数\(w\)也是一个随机变量。假定\(w\)的先验分布是一个服从均值为0的多元高斯分布：<br/>
\[p(w|\alpha)=\mathcal{N}(w|0,\alpha^{-1}I)\]<br/>
其中，\(\alpha\)被称为模型的<strong>超验参数（hyperparameters）</strong>。<br/>
则由贝叶斯定理，\(w\)在给定训练集上的后验分布为：<br/>
\[p(w|\mathbf{x},\mathbf{t},\alpha,\beta) \propto p(\mathbf{t}|\mathbf{x},w,\alpha,\beta) \times p(w|\alpha)\]</p>

<p>我们可以通过最大化这个后验概率来找出最合适的\(w\)。这个方法被称之为<strong>最大后验概率法（maximum posterior，简称MAP）</strong>。对上式取对数，并代入似然函数和先验概率的分布函数后，我们发现，最大后验概率等价于最小化带正则项的平方误差函数：<br/>
\[\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{\alpha}{2\beta}w^Tw\]<br/>
（这句话也可以这么理解：通过引入均值为0的高斯分布先验函数，我们对\(w\)的大小进行了限定。）</p>

<p>也就是说，<strong>带正则项的误差函数，是在目标变量、模型参数均服从高斯分布的假定下，用最大后验概率法推导出来的一个自然结果！</strong>同样的，这个结论与\(y(x,w)\)的具体形式无关。</p>

<p>不过，虽然我们引入了\(w\)的先验概率分布，MAP依然只是对\(w\)的点估计（类似最大似然法，这种方法最大的问题在于给出的估计值往往是有偏的，即\(E[w_{MAP}] \neq w\)）。而贝叶斯学派的精髓在于，从\(w\)的后验概率分布出发，我们可以进一步得到在给定训练集\(\mathbf{x}\)和\(\mathbf{t}\)的条件下，新变量\(x\)的目标值\(t\)的后验概率分布\(p(t|x,\mathbf{x},\mathbf{t})\)：<br/>
\[p(t|x,\mathbf{x},\mathbf{t})=\int{p(t|x,w)p(w|\mathbf{x},\mathbf{t})dw}\]<br/>
（假定\(\alpha\)和\(\beta\)都是模型的超验参数）</p>

<p>总结一下频率学派和贝叶斯学派的区别：</p>

<table>
<thead>
<tr>
<th>----</th>
<th>频率学派</th>
<th>贝叶斯学派</th>
</tr>
</thead>

<tbody>
<tr>
<td>目标变量\(t\)</td>
<td>\(p(t\|x,w)\)</td>
<td>\(p(t\|x,\mathbf{x},\mathbf{t})\)</td>
</tr>
<tr>
<td>模型参数\(w\)</td>
<td>显式；常量</td>
<td>隐式；随机变量</td>
</tr>
<tr>
<td>模型优化算法</td>
<td>最大似然法</td>
<td>最大后验概率法 <sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></td>
</tr>
<tr>
<td>优化目标函数</td>
<td>\(\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}\)</td>
<td>\(\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{\alpha}{2\beta}w^Tw\)</td>
</tr>
</tbody>
</table>

<h2 id="toc_6">决策理论（Decision Theory）</h2>

<p>一个具体的机器学习问题的解决包括两个过程：<strong>推断（inference）</strong>和<strong>决策（decision）</strong>。前者在概率学的框架下告诉我们\(p(\mathbf{x},t)\)的分布；后者则借助<strong>决策理论（decision theory）</strong>告诉我们在这个联合概率分布下的最优反应（例如对\(t\)的预估等）。</p>

<p>一旦解决了推断的问题，决策的过程就显得异常简单。</p>

<h3 id="toc_7">分类问题的决策</h3>

<p>对于分类问题而言，我们寻求一个将输入变量\(\mathbf{x}\)映射到某一个分类上的法则。这个法则将\(\mathbf{x}\)的向量空间划分成了不同的区域\(\mathcal{R}_k\)：我们称之为<strong>decision region</strong>；位于\(\mathcal{R}_k\)里的点\(\mathbf{x}\)都被映射到类\(C_k\)上。相应的，decision region之间的边界我们称之为<strong>decision boundaries</strong>或<strong>decision surfaces</strong>。</p>

<p>如何找到这个决策面呢？</p>

<p>假定我们的决策目标是使得误分类的概率尽可能地小，借助概率论，我们有：<br/>
\[p(mistake)=1-p(correct)=1-\sum_{k=1}^K{\int_{\mathcal{R}_k}}{p(\mathbf{x},C_k)d\mathbf{x}}\]<br/>
显然，为了让\(p(mistake)\)尽可能地小，\(\mathcal{R}_k\)应是由那些使得\(p(\mathbf{x},C_k)\)最大的点构成的集合（即对于\(\mathbf{x} \in \mathcal{R}_k\)，满足\(p(\mathbf{x},C_k) \geq p(\mathbf{x},C_j)\)）。由于\(p(\mathbf{x},C_k)＝p(C_k|\mathbf{x})p(\mathbf{x})\)，也就是说我们应将\(\mathbf{x}\)映射到后验概率\(p(C_k|\mathbf{x})\)最大的类\(C_k\)上。</p>

<p>但真实的问题也许要更复杂一些。例如对癌症患者的诊断：错误的将一个癌症患者诊断为健康，远比将一个健康的人误诊为癌症要严重的多。因此，就有了<strong>损失矩阵（loss function）</strong>的概念：损失矩阵\(L\)中的元素\(L_{kj}\)用来评估当类\(C_k\)被误分类为\(C_j\)时带来的损失。显然，对任意的类\(C_k\)，总有\(L_{kk}=0\)。</p>

<p>此时，我们的决策目标是最小化期望损失函数。同样地借助概率论的知识，我们有：<br/>
\[E[L] = \sum_k \sum_j \int_{\mathcal{R}_j}{L_{kj}p(\mathbf{x},C_k)d\mathbf{x}}\]<br/>
类似地，我们选取那些使\(\sum_k L_{kj}p(\mathbf{x},C_k)\)最小的点\(\mathbf{x}\)构成的集合为类\(C_j\)的decision region \(\mathcal{R}_j\)（即对于\(\mathcal{R}_j\)里的\(\mathbf{x}\)，总有\(\sum_k L_{kj}p(\mathbf{x},C_k) \leq \sum_k L_{ki}p(\mathbf{x},C_k)\)）。</p>

<h3 id="toc_8">再次回顾推断过程</h3>

<p>在前面对分类问题决策阶段的讨论中，我们发现，我们可以在推断阶段预估\({\mathbf{x}}\)和\(C_k\)的联合概率分布\(p(\mathbf{x},C_k)\)，也可以预估\(C_k\)的后验条件概率\(p(C_k|\mathbf{x})\)。事实上，我们也可以将两个阶段整合为一个过程，直接学习\({\mathbf{x}}\)到\(C_k\)的映射。这三种不同的解决问题的思路，对应了三种不同的模型：</p>

<ol>
<li><strong>产生式模型（generative models）</strong>：产生式模型是指那些在推断阶段隐式或显式地对输入变量\(\mathbf{x}\)的分布进行了建模的模型。由贝叶斯公式，我们可以用\(\mathbf{x}\)的条件概率\(p(\mathbf{x}|C_k)\)和\(C_k\)的先验概率\(p(C_k)\)计算\(C_k\)的后验概率：\[p(C_k|\mathbf{x})=\frac{p(\mathbf{x}|C_k)p(C_k)}{p(\mathbf{x})}\]在得到\(C_k\)的后验概率后，进入决策阶段分类。这个过程等价于对联合概率分布\(p(\mathbf{x},C_k)\)进行建模（上式中的分子）。</li>
<li><strong>判别式模型（discriminative models）</strong>：判别式模型是指那些在推断阶段直接对目标变量的后验概率进行建模的模型。例如前面提到的多项式拟合模型。</li>
<li><strong>判别式函数（discriminant function）</strong>：通过一个判别式函数\(f(\mathbf{x})\)可以将输入变量\(\mathbf{x}\)直接映射到一个类标上；省去了对概率分布的预估。Andrew在Coursera上的课程里提到的假说（Hypothesis）就是这类函数。</li>
</ol>

<p>总的来说，三类模型各有利弊。相对来讲，产生式模型最为复杂，也需要最多的训练数据；但是由于可以对输入变量的分布进行预估，功能上也最为强大。而我们用的最多的一般是判别式模型。由于引入了对后验概率的预估，使得我们可以方便的处理非平衡样本问题，以及多模型的融合。这是简单的判别式函数所不具备的。</p>

<h3 id="toc_9">回归问题的决策</h3>

<p>与分类问题类似，回归问题的决策阶段也是要从目标变量的概率分布中选择一个特定的预估值，以最小化某个损失函数。回归问题的损失函数为：\[E[L] = \int\int L(t,y(\mathbf{x}))p(\mathbf{x},t)d\mathbf{x}dt\] 积分项的意思是，对于输入变量\(\mathbf{x}\)和真实目标值\(t\)，我们的预估目标值\(y(\mathbf{x})\)带来的期望损失。一般\(L(t,y(\mathbf{x}))\)被定义为平方误差和的形式。</p>

<p>最小化这个损失函数我们得到解：\(y(\mathbf{x})=E_t[t|\mathbf{x}]\)。这个条件期望也被称之为<strong>回归函数（regression function）</strong>。在前面的多项式拟合的问题中，我们假定了目标变量\(t\)服从一个高斯分布，并用最大似然法（或Bayesian的方法）估计出了模型的参数；而通过这里的回归函数，我们才最终得到了\(t\)的一个预估值。</p>

<p>类似地，回归问题也有三种建模方式：产生式模型，判别式模型，和判别式函数。这里不再赘述。</p>

<h2 id="toc_10">信息论（Information Theory）</h2>

<p>如果说概率论给出了对不确定性的一个描述，信息论则给出了对不确定性程度的一种度量。这种度量就是信息熵。</p>

<h3 id="toc_11">信息熵</h3>

<p>我们可以从三种不同的角度理解信息熵的定义。</p>

<ol>
<li>概率学。如果我们问一个随机事件的发生会传递多少信息量？显然，一个确定事件的发生传递的信息量为0，而一个小概率事件的发生会传递更多的信息量。因此，我们对信息量（记作\(h(x)\)）的定义应该与随机事件发生的概率\(p(x)\)有关。另一方面，两个独立事件同时发生所传递的信息量应该是这两个事件各自发生的信息量之和，即\(h(x,y)=h(x)+h(y)\)。由这两个性质出发，我们可以证明，信息量\(h(x)\)必然遵循\(p(x)\)的对数形式：\[h(x) ＝ -log_{2}p(x)\] 进一步地，我们定义一个随机变量\(x\)的信息熵为其传递的信息量的期望值：\[H[x]=-\sum p(x)log_{2}p(x)\]</li>
<li>信息学。在对信息熵的第一种解读中，我们并没有解释为什么对信息量的定义采用了以自然数2为底的对数。Shannon给出了一个信息学的解释：信息熵等价于编码一个随机变量的状态所需的最短位数（bits）。对于一个均匀分布的随机变量，这是显而易见的；而对于非均匀分布的随机变量，我们可以通过对高频事件采用短编码，低频事件采用长编码的方式，来缩短平均编码长度。</li>
<li>物理学。在热力学中我们也接触到熵的定义：一种对系统无序程度的度量。熵越大，系统的无序程度越高。这里，熵被定义为一个宏观态对应微观态个数的对数。与热力学熵类似，信息熵度量了一个随机变量的不确定程度。信息熵越大，随机变量的不确定性越高。</li>
</ol>

<p>我们也可以将信息熵的定义拓展到连续型的随机变量，定义为<strong>微分熵（differential entropy）</strong>：\[H[x]=-\int{p(x)\ln{p(x)}}dx\] （如果从信息熵的原始定义出发，我们会发现微分熵的定义和信息熵相差了一个极大项：\(-\ln{\Delta}\)；这意味着连续变量本质上不可能做到精确的编码。）</p>

<p>结合拉格朗日法，我们可以推导出信息熵和微分熵最大时对应的分布：对于离散的随机变量，信息熵最大时对应着一个均匀分布；对于连续的随机变量，微分熵最大时对应着一个高斯分布（注意对于微分熵的最大值求解需要增加一阶矩和二阶矩两个约束条件）。</p>

<h3 id="toc_12">条件熵，交叉熵，与互信息</h3>

<p>从信息熵的第一定义，我们很容易写出<strong>条件信息熵</strong>的计算公式：\[H[y|x]=-\int\int p(y,x)\ln{p(y|x)}dydx\]<br/>
容易证明，\(H[x,y]=H[y|x]+H[x]\)。也就是说，\(x\)、\(y\)联合分布的不确定度，等于\(x\)的不确定度与知道\(x\)后\(y\)的不确定度之和。</p>

<p><strong>交叉熵（relative entropy）</strong>，又被称为<strong>KL divergence</strong>，衡量了两个分布的不相似性。它定义了用一个预估分布\(q(x)\)近似一个未知分布\(p(x)\)时，对\(x\)进行编码所需要的额外期望信息量：\[KL[p\|q]=-\int p(x)\ln{q(x)}dx-(-\int p(x)\ln{p(x)}dx)=-\int p(x)\ln{\{\frac{q(x)}{p(x)}\}dx}\]</p>

<p>交叉熵越大，两个分布越不相似。可以证明，交叉熵不满足对称性（\(KL[p\|q] \neq KL[q\|p]\)）；且\(KL[p\|q]\geq 0\)（当且仅当\(q(x) = p(x)\)时等号成立）。</p>

<p>我们可以用交叉熵的概念来对一个机器学习模型的参数进行预估。假设我们有一堆抽样自某未知分布\(p(x)\)的数据，我们试图用一个带参数的模型\(q(x|\theta)\)去拟合它。一条可行的思路是选择使\(p(x)\)和\(q(x|\theta)\)的交叉熵尽可能小的参数\(\theta\)。虽然我们不知道\(p(x)\)的真实分布，但由蒙特卡洛方法，我们可以从抽样数据中得到\(KL(p\|q)\)的近似值：<br/>
\[KL(p\|q) \simeq \sum_{n=1}^{N}\{-\ln{q(x_n|\theta)}+\ln{p(x_n)}\}\]<br/>
略去与\(\theta\)无关项，我们发现，我们得到的是一个似然函数的负对数！也就是说，<strong>最小化模型预估分布与真实分布的交叉熵等价于前面提到的最大似然法！</strong>这也从信息熵的角度解释了为什么要对似然函数取对数的原因。</p>

<p>另一个重要的概念是<strong>互信息（mutual information）</strong>，用来衡量两个随机变量\(x\)、\(y\)的相关性。互信息有两种定义方式：<br/>
1. \(p(x,y)\)和\(p(x)p(y)\)的交叉熵定义：\[I[x,y]=KL(p(x,y)\|p(x)p(y))=-\int\int{p(x,y)\ln{(\frac{p(x)p(y)}{p(x,y)})}}dxdy\] 当\(x\)、\(y\)相互独立时，\(p(x,y)=p(x)p(y)\)；此时，\(I[x,y]=0\)。<br/>
2. 由KL定义出发，我们可以推导出互信息的条件熵定义：\[I[x,y]=H[x]-H[x|y]=H[y]-H[y|x]\] 因此，\(x\)、\(y\)的互信息可以理解为知道其中一个随机变量的取值后，另一个随机变量的不确定度的降低（当\(I[x,y]=H[x]\)时意味着\(y\)的发生确定了\(x\)的发生，即两个随机变量完全相关）。</p>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>通常，贝叶斯学派不会对模型的参数进行点估计，因此也不会用MAP算法优化模型。把最大后验概率算法和下面的优化目标函数放在这里，只是为了和频率学派的最大似然法进行对比。&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[word2vec前世今生]]></title>
    <link href="tianwei.github.io/word2vec.html"/>
    <updated>2017-03-23T13:03:18+08:00</updated>
    <id>tianwei.github.io/word2vec.html</id>
    <content type="html"><![CDATA[
<p>2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">Statistical Language Model</h2>

<p>在深入word2vec算法的细节之前，我们首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。</p>

<p>统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列\(S=w_1, w_2, ... , w_T\)，它的概率可以表示为：<br/>
\[P(S)=P(w_1, w_2, ..., w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, ..., w_{t-1})\]<br/>
即将序列的联合概率转化为一系列条件概率的乘积。问题变成了如何去预测这些给定previous words下的条件概率\(p(w_t|w_1,w_2,...,w_{t-1})\)。</p>

<p>由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型：<br/>
\[p(w_t|w_1, w_2, ..., w_{t-1}) \approx p(w_t|w_{t-n+1}, ..., w_{t-1})\]<br/>
常见的如bigram模型（\(N=2\)）和trigram模型（\(N=3\)）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑\(N&gt;3\)的模型。</p>

<p>我们可以用最大似然法去求解Ngram模型的参数——等价于去统计每个Ngram的条件词频。</p>

<p>为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为\(0\)），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram）和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。此处不再赘述。感兴趣者可进一步阅读相关的文献<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。</p>

<h2 id="toc_1">Distributed Representation</h2>

<p>不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（\(N&gt;3\)）。其次，它没有考虑词与词之间内在的联系性。例如，考虑&quot;the cat is walking in the bedroom&quot;这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。然而， Ngram模型做不到。</p>

<p>这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是\(1\)，其余方向上都是\(0\)）。例如，对于一个大小为\(5\)的词典：{&quot;I&quot;, &quot;love&quot;, &quot;nature&quot;, &quot;luaguage&quot;, &quot;processing&quot;}，“nature”对应的one-hot向量为：\([0, 0, 1, 0, 0]\)。显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality）</p>

<p>于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。</p>

<p>事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。</p>

<p>VSM是基于一种Statistical Semantics Hypothesis<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。</p>

<p>那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？</p>

<p>简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵\(A\)：矩阵的行\(A_{i,:}\)对应着词典里的一个word；矩阵的列\(A_{:,j}\)对应着训练语料里的一篇文档；矩阵里的元素\(A_{ij}\)代表着word \(w_i\)在文档\(D_j\)中出现的次数（或频率）。那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。</p>

<p>类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。</p>

<p>注意，这两类矩阵的行向量所计算的相似度有着细微的差异：term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。</p>

<p>不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。</p>

<p>关于VSM更多的介绍，可以进一步阅读文末的参考文献<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>。</p>

<h2 id="toc_2">Neural Network Language Model</h2>

<p>接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。</p>

<p>NNLM模型的基本思想可以概括如下：<br/>
1. 假定词表中的每一个word都对应着一个连续的特征向量；<br/>
2. 假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；<br/>
3. <strong>同时学习</strong>词向量的权重和概率模型里的参数。</p>

<p>值得注意的一点是，这里的词向量也是要学习的参数。</p>

<p>在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络\(f(w_{t-n+1},...,w_{t})\)来拟合一个词序列的条件概率\(p(w_t|w_1,w_2,...,w_{t-1})\)。整个模型的网络结构见下图：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902454895705.jpg" alt="the neural network language model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>我们可以将整个模型拆分成两部分加以理解：<br/>
1. 首先是一个线性的embedding层。它将输入的\(N-1\)个one-hot词向量，通过一个共享的\(D \times V\)的矩阵\(C\)，映射为\(N-1\)个分布式的词向量（distributed vector）。其中，\(V\)是词典的大小，\(D\)是embedding向量的维度（一个先验参数）。\(C\)矩阵里存储了要学习的word vector。<br/>
2. 其次是一个简单的前向反馈神经网络\(g\)。它由一个tanh隐层和一个softmax输出层组成。通过将embedding层输出的\(N-1\)个词向量映射为一个长度为\(V\)的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：<br/>
\[p(w_i|w_1,w_2,...,w_{t-1}) \approx f(w_i, w_{t-1}, ..., w_{t-n+1}) = g(w_i, C(w_{t-n+1}), ..., C(w_{t-1}))\]</p>

<p>我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数\(\theta\)：<br/>
\[L(\theta)=\frac{1}{T}\sum_t{\log{f(w_t, w_{t-1}, ..., w_{t-n+1})}}+R(\theta)\]</p>

<p>其中，模型的参数\(\theta\)包括了embedding层矩阵\(C\)的元素，和前向反馈神经网络模型\(g\)里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。</p>

<p>然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？</p>

<p>仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率\(p(w_t|context)\)的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。另一方面，以条件概率\(p(w_t|context)\)为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。</p>

<h2 id="toc_3">CBoW &amp; Skip-gram Model</h2>

<p>铺垫了这么多，终于要轮到主角出场了。</p>

<p>不过在主角正式登场前，我们先看一下NNLM存在的几个问题。</p>

<p>一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度\(N\)提高到了\(5\)，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。</p>

<p>因此，Mikolov等人在2010年提出了一种RNNLM模型<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。</p>

<p>另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。</p>

<p>这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：<br/>
1. 用一个简单模型训练出连续的词向量；<br/>
2. 基于词向量的表达，训练一个连续的Ngram神经网络模型。<br/>
而NNLM模型的计算瓶颈主要是在第二步。</p>

<p>如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？</p>

<p>Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。</p>

<p>下面，我将带领大家简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。</p>

<p>首先，我们对原始的NNLM模型做如下改造：<br/>
1. 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；<br/>
2. 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；<br/>
3. 将future words纳入上下文环境</p>

<p>得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902460210953.jpg" alt="CBoW Model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。</p>

<p>CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902460526207.jpg" alt="Skip-gram Model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。</p>

<p>如果将Skip-gram模型的前向计算过程写成数学形式，我们得到：<br/>
\[p(w_o|w_i)=\frac{e^{U_o \cdot V_i}}{\sum_j{e^{U_j \cdot V_i}}}\]<br/>
其中，\(V_i\)是embedding层矩阵里的列向量，也被称为\(w_i\)的input vector。\(U_j\)是softmax层矩阵里的行向量，也被称为\(w_j\)的output vector。</p>

<p>因此，Skip-gram模型的本质是<strong>计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化</strong>。我们要学习的模型参数正是这两类词向量。</p>

<p>然而，直接对词典里的\(V\)个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。</p>

<h3 id="toc_4">Hierarchical Softmax<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup></h3>

<p>层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式：<br/>
\[p(v|context)=\prod_{i=1}^m{p(b_i(v)|b_1(v), ..., b_{i-1}(v), context)}\]<br/>
其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对\(V\)个词的概率归一化问题，转化成了对\(\log{V}\)个词的概率拟合问题。</p>

<p>我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典\(D\)划分为两个子集\(D_1\)、\(D_2\)，并假设在给定context下，target word属于子集\(D_1\)的概率\(p(w_t \in D_1|context)\)服从logistical function的形式：<br/>
\[p(w_t \in D_1|context)=\frac{1}{1+e^{-U_{D_{root}} \cdot V_{w_t}}}\]<br/>
其中，\(U_{D_{root}}\)和\(V_{w_t}\)都是模型的参数。</p>

<p>接下来，我们可以对子集\(D_1\)和\(D_2\)进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为\(V\)的字典\(D\)转换成了一颗深度为\(\log V\)的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。</p>

<p>同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word \(w_t\)，假设其对应的二叉树编码为\(\{1, 0, 1, ..., 1\}\)，则我们构造的似然函数为：<br/>
\[p(w_t|context)=p(D_1=1|context)p(D_2=0|D_1=1)\dots p(w_t|D_k=1)\]<br/>
乘积中的每一项都是一个逻辑回归的函数。</p>

<p>我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。</p>

<p>层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的\(V\)降低到了\(\log V\)的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。</p>

<h3 id="toc_5">Negative Sampling<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup></h3>

<p>负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。</p>

<p>以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：<br/>
\[J(\theta)=-\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}\]<br/>
式中的\(p(w_{t+j}|w_t)\)是一个在整个字典上归一化了的概率。</p>

<p>而在NCE算法中，我们构造了这样一个问题：对于一组训练样本<context, word>，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：<br/>
\[p(D=1|w, context)=\frac{p(w|context)}{p(w|context)+kp_n(w)}=\sigma (\log p(w|context) - \log kp_n(w))\]<br/>
这个式子给出了一个target word \(w\)来自于context驱动的概率。其中，\(k\)是一个先验参数，表明噪声的采样频率。\(p(w|context)\)是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。\(p_n(w)\)则是背景噪声的词分布。通常采用word的unigram分布。</p>

<p>通过对噪声分布的\(k\)采样，我们得到一个新的数据集：<context, word, label>。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。</p>

<p>而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算：<br/>
\[p(D=1|w_o, w_i)=\sigma (U_o \cdot V_i)\]<br/>
此时，模型相应的目标函数变为：<br/>
\[J(\theta) = \log \sigma(U_o \cdot V_i) + \sum_{j=1}^k{E_{w_j \sim p_n(w)}[\log \sigma(- U_j \cdot V_i)]}\]</p>

<p>除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：<br/>
\[p_{discard}(w) = 1 - \sqrt{\frac{t}{f(w)}}\]<br/>
其中，\(t\)是一个先验参数，一般取为\(10^{-5}\)。\(f(w)\)是\(w\)在语料中出现的频率。</p>

<p>实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。</p>

<h2 id="toc_6">Beyond the Word Vector</h2>

<p>介绍完word2vec模型的算法和原理，我们来讨论一些轻松点的话题——模型的应用。</p>

<p>13年word2vec模型横空出世后，人们最津津乐道的是它学到的向量在语义和语法相似性上的应用——尤其是这种相似性居然对数学上的加减操作有意义<sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup>！最经典的一个例子是，\(v(&quot;King&quot;) - v(&quot;Man&quot;) + v(&quot;Woman&quot;) = v(&quot;Queen&quot;)\)。然而，这种例子似乎并没有太多实际的用途。</p>

<p>除此之外，word2vec模型还被应用于机器翻译和推荐系统领域。</p>

<h3 id="toc_7">Machine Translation<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup></h3>

<p>与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。</p>

<p>具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系\(W\)。构造的损失函数为：<br/>
\[J(W)=\sum_{i=1}^n{||Wx_i - z_i||^2}\]</p>

<p>在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵\(W\)映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。</p>

<p>其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵\(W\)本质上是一种空间对齐的线性变换。</p>

<h3 id="toc_8">Item2Vec<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup></h3>

<p>本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。</p>

<p>比如，推荐系统领域的协同过滤算法。</p>

<p>协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。</p>

<p>关于word2vec更多应用的介绍，可以进一步参考这篇文献<sup id="fnref9"><a href="#fn9" rel="footnote">9</a></sup>。</p>

<h2 id="toc_9">Word Embedding</h2>

<p>最后，我想简单阐述下我对word embedding的几点思考。不一定正确，也欢迎大家提出不同的意见。</p>

<p>Word embedding最早出现于Bengio在03年发表的开创性文章中<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。</p>

<p>Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。</p>

<p>无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。</p>

<p>相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。</p>

<p>Word embedding的另一个研究方向是在更高层次上对sentence的embedding向量进行建模。</p>

<p>我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。</p>

<p>显然，这种简单粗暴的方法会丢失很多信息。</p>

<p>另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练<sup id="fnref10"><a href="#fn10" rel="footnote">10</a></sup>。这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。</p>

<p>个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。</p>

<p>这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。</p>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

<li id="fn2">
<p>Turney, P. D., &amp; Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a></p>
</li>

<li id="fn3">
<p>Mikolov, T., Karafiát, M., Burget, L., &amp; Cernocký, J. (2010). Recurrent neural network based language model. Interspeech.&nbsp;<a href="#fnref3" rev="footnote">&#8617;</a></p>
</li>

<li id="fn4">
<p>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.&nbsp;<a href="#fnref4" rev="footnote">&#8617;</a></p>
</li>

<li id="fn5">
<p>Mnih, A., &amp; Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265–2273.&nbsp;<a href="#fnref5" rev="footnote">&#8617;</a></p>
</li>

<li id="fn6">
<p>Mikolov, T., Yih, W., &amp; Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.&nbsp;<a href="#fnref6" rev="footnote">&#8617;</a></p>
</li>

<li id="fn7">
<p>Mikolov, T., Le, Q. V., &amp; Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.&nbsp;<a href="#fnref7" rev="footnote">&#8617;</a></p>
</li>

<li id="fn8">
<p>Barkan, O., &amp; Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.&nbsp;<a href="#fnref8" rev="footnote">&#8617;</a></p>
</li>

<li id="fn9">
<p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.&nbsp;<a href="#fnref9" rev="footnote">&#8617;</a></p>
</li>

<li id="fn10">
<p>Le, Q. V., &amp; Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org.&nbsp;<a href="#fnref10" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
</feed>
