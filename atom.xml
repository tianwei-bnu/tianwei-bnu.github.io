<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[公子天的网络日志]]></title>
  <link href="whiskytina.github.io/atom.xml" rel="self"/>
  <link href="whiskytina.github.io/"/>
  <updated>2017-03-24T23:31:48+08:00</updated>
  <id>whiskytina.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning读书笔记（二）：线性代数]]></title>
    <link href="whiskytina.github.io/14902534668711.html"/>
    <updated>2017-03-23T15:17:46+08:00</updated>
    <id>whiskytina.github.io/14902534668711.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">基本术语</h2>

<ul>
<li>Scalars：标量。通常是一个实数。</li>
<li>Vectors：向量。由一组标量组成的一维有序数组；若无特殊说明通常是指列向量。通过下标索引可以找到其对应的元素。对应的几何意义是笛卡尔空间的一个点。</li>
<li>Matrices：矩阵（特指二维矩阵）。由一组标量组成的二维数组。</li>
<li>Tensors：张量。一个高维矩阵（\(D \geq 3\)）</li>
</ul>

<span id="more"></span><!-- more -->

<h2 id="toc_1">基本运算</h2>

<ul>
<li>Transpose：\((A^\top)_{i, j} = A_{j, i}\)</li>
<li>Broadcasting：将向量的元素按行或按列广播为一个矩阵，从而与另一个矩阵进行计算</li>
<li>Multiplying：

<ol>
<li>matrix product：\((AB)_{i,j}=\sum_k{A_{i,k}B_{k,j}}\)</li>
<li>element-wise product：\((A \odot B)_{i,j}=A_{i,j}B_{i,j}\)</li>
<li>dot product：\(x^\top y\)</li>
<li>matrix-vector product：\(Ax=b\) 对应着一组线性方程组。</li>
</ol></li>
<li>Inverse Matrices：\(A^{-1}A=I_n\) （逆矩阵主要被用于理论分析的工具）</li>
</ul>

<h2 id="toc_2">线性相关和矩阵的Span</h2>

<ul>
<li>线性方程组\(Ax=b, A \in \mathbb{R}^{m \times n}\)的解的可能性：

<ul>
<li>唯一解（对于任意向量\(b \in \mathbb{R}^m\)）</li>
<li>无穷解</li>
<li>无解</li>
</ul></li>
<li>Linear Combination：一组向量的线性组合被定义为是将每一个向量都乘以一个标量系数并求和的结果：\(\sum_i{c_iv^{(i)}}\)

<ul>
<li>矩阵-向量乘积\(Ax\)的意义：\(Ax=\sum_i{x_i A_{:, i}}\) 矩阵列向量的一种线性组合（linear combination）</li>
</ul></li>
<li>Span：一组向量的Span被定义为这组向量所有可能的线性组合所构成的向量的集合。

<ul>
<li>矩阵的列空间（column space）：矩阵列向量的span</li>
<li>判断线性方程组\(Ax=b\)有无解等价于判断向量\(b\)是否在矩阵\(A\)的列空间里</li>
</ul></li>
<li>Linear Dependence：对于一组向量的集合，如果其中存在一个向量是其他向量的线性组合，则称这组向量是线性相关的。

<ul>
<li>线性方程组\(Ax=b\)对任意\(b \in \mathbb{R}^m\)至少有一个解（矩阵\(A \in \mathbb{R}^{m \times n}\)的列空间等于\(\mathbb{R}^m\)）的充要条件：矩阵\(A\)至少有一组\(m\)个线性不相关的列向量</li>
<li>线性方程组\(Ax=b\)对任意\(b \in \mathbb{R}^m\)有唯一解的充要条件：矩阵\(A\)仅有一组\(m\)个线性不相关的列向量（\(n = m\)）。此时，矩阵\(A\)可逆。</li>
</ul></li>
</ul>

<h2 id="toc_3">范式Norms</h2>

<ul>
<li>向量的范式：用于测量一个向量的大小</li>
<li>\(L^p\)范式：\(||x||_p=\sqrt[p]{\sum_i{|x_i|^p}}\) 将一个向量映射为一个非负实数

<ul>
<li>基本性质：

<ol>
<li>\(L^p(x)=0 \Rightarrow x=0\)</li>
<li>\(L^p(x+y) \leq L^p(x) + L^p(y)\)</li>
<li>\(\forall \alpha \in \mathbb{R}, L^p(\alpha x) = |\alpha| L^p(x)\)</li>
</ol></li>
</ul></li>
<li>常见的几种范式：

<ul>
<li>\(L^2\)范式：\(||x||_2=\sqrt{\sum_i{x_i^2}}\) 又称欧几里得范式（度量了向量\(x\)到原点的欧氏距离）。在机器学习领域里更常用的是\(L^2\)范式的平方（即\(x^\top x\)）。其特点是易于对向量\(x\)的每个元素求偏导，缺陷是在原点附近平方值的变化曲率太小。</li>
<li>\(L^1\)范式：\(||x||_1 = \sum_i{|x_i|}\) 在空间的任意点都有着相同的变化率，适用于对原点附近点的大小极为敏感的应用场景。</li>
<li>\(L^\infty\)范式：\(||x||_\infty = \max_i{|x_i|}\)</li>
<li>Frobenius Norm：\(||A||_F = \sqrt{\sum_{i,j}{A^2_{i,j}}}\) 矩阵的\(F\)范式，类似于向量的\(L^2\)范式，常用于在深度学习中测量一个矩阵的大小。</li>
</ul></li>
</ul>

<h2 id="toc_4">几种特殊矩阵和向量</h2>

<ul>
<li>Diagonal Matrices：对角矩阵。仅在对角元上的元素非零。对角矩阵的向量乘法和求逆都十分简单，因此常被用于通用算法中的假设以简化计算。</li>
<li>Symmetric Maxtrix：对称矩阵。满足：\(A=A^\top\)。通常由一个接受双参数且对参数顺序不敏感的函数产生（例如距离函数）</li>
<li>Unit Vector：单元向量。满足：\(||x||_2 = 1\)</li>
<li>Orthogonal Vectors：正交向量。两个向量被称为正交当且仅当这两个向量满足：\(x^\top y = 0\)。在\(\mathbb{R}^n\)空间里，最多只能找到\(n\)个非零范式的相互正交的向量。如果两个向量不仅正交且\(L^2\)范式均为1，称之为正交归一化向量（orthonormal）</li>
<li>Orthogonal Matrix：正交矩阵。是一个行向量相互<strong>正交归一</strong>且列向量相互<strong>正交归一</strong>的方阵：\(A^\top A = A A^\top = I\)（也即\(A^{-1} = A^\top\)）</li>
</ul>

<h2 id="toc_5">特征分解（Eigendecomposition）</h2>

<ul>
<li>特征向量（Eigenvector）：一个矩阵\(A\)的特征向量\(v\)是一个满足\(Av=\lambda v\)的非零向量（矩阵\(A\)作用在这个向量上仅改变其大小，不改变其方向）。其中，\(\lambda\)又被称为矩阵\(A\)的特征值（Eigenvalue）。显然，同一个特征值对应的特征向量有无数多个（如果\(v\)是矩阵\(A\)的特征向量，则\(sv, s \in \mathbb{R}, s \ne 0\)也是一个特征向量）。我们平时所说的特征向量通常是指\(L^2\)范式为\(1\)的单位向量。

<ul>
<li>矩阵与其特征向量的几何关系：矩阵的本质是对其特征向量构成空间的一个尺寸变换（scaling）。设若\(x=v\mu\)，其中\(v\)是矩阵\(A\)的特征向量构成的矩阵，则有：\[Ax=Av\mu=(\lambda v) \mu\]</li>
</ul></li>
<li>特征分解（Eigendecomposition）：将矩阵\(A\)分解为两个特征矩阵乘积的形式：\(A=V diag({\lambda}) V^{-1}\)。其中\(V=[v^{(1)},...,v^{(n)}]\)是由\(n \times n\)矩阵\(A\)的\(n\)个相互独立的特征向量（确保\(V^{-1}\)的存在）按列组成的特征矩阵；\(\lambda = [\lambda_1, ..., \lambda_n]^\top\)是对应的特征值组成的列向量。（这个等式的证明也很简单，只需在等式两边各自右乘一个矩阵\(V\)即可。）

<ul>
<li>并非所有矩阵都存在特征分解</li>
</ul></li>
<li><strong>实对称矩阵</strong>的特征分解：\(A=Q \Lambda Q^\top\)。其中，\(Q\)是一个由实对称矩阵\(A\)的特征向量组成的<strong>正交矩阵</strong>（不仅仅是linear independence）

<ul>
<li>应用： 二次型\(f(x) = x^\top A x\)的极值。其中\(||x||_2=1\)

<ul>
<li>极大值：最大特征值及其对应的特征向量</li>
<li>极小值：最小特征值及其对应的特性向量</li>
</ul></li>
</ul></li>
</ul>

<h2 id="toc_6">奇异值分解（Singular Value Decomposition）</h2>

<ul>
<li>SVD分解：\(A=UDV^\top\)

<ul>
<li>更通用的矩阵分解方式</li>
<li>\(U\)：\(m \times m\)的正交矩阵。其列向量被称为矩阵\(A\)的<em>左奇异向量</em>，同时也是矩阵\(AA^\top\)的特征向量</li>
<li>\(D\)：\(m \times n\)的对角矩阵。对角元上的元素被称为矩阵\(A\)的<em>奇异值</em>，同时也是矩阵\(A^\top A\)和\(AA^\top\)的特征值的平方根</li>
<li>\(V\)：\(n \times n\)的正交矩阵。其列向量被称为矩阵\(A\)的<em>右奇异向量</em>，同时也是矩阵\(A^\top A\)的特征向量</li>
</ul></li>
<li>应用：Moore-Penrose伪逆（对任意矩阵求其伪逆）

<ul>
<li>\(A^+ = V D^+ U^\top\)，其中对角矩阵\(D\)的伪逆是对其非零元素取倒数再转置得到的</li>
</ul></li>
</ul>

<h2 id="toc_7">迹（Trace）和行列式（Determinant）</h2>

<ul>
<li>Trace：\(Tr(A) = \sum_i{A_{i,i}}\)</li>
<li>Determinant：\(det(A) = \prod_i{\lambda_i}\)</li>
</ul>

<h2 id="toc_8">例子：PCA主成分分析</h2>

<ul>
<li>动机：对于一组\(n\)维空间的向量点，找到一个低维的特征表达，并尽可能少地丢失信息：\(x \approx g(f(x))\)。其中\(f(x)\)是编码函数（将高维向量编码到低维空间）；\(g(x)\)是解码函数（将低维向量解码到高维空间）</li>
<li>问题一：编码／解码函数长什么样？

<ul>
<li>假设（解码函数）：\(g(c) = Dc, D \in \mathbb{R}^{n \times l}\)。即用一个\(n \times l\)的矩阵对编码后的点\(c\)进行解码。其中，\(D\)是一个列向量正交且归一的矩阵（由于\(n &gt; l\)故行向量不可能正交）</li>
<li>目标（编码函数）：寻求一个最优的低维空间编码\(c^*\)，使得由\(c^*\)在高维空间重建后的点\(g(c^*)\)与原始的点\(x\)最近，也即最小化\(L^2\)范式的解\(c\)：\[c^*=\arg\min_c{||x - g(c)||_2}\]</li>
<li>最优解（编码函数）：\(f(x) = c^* =D^\top x\)</li>
</ul></li>
<li>问题二：如何找到解码矩阵\(D\)？

<ul>
<li>目标：寻求一个最优的矩阵\(D\)，使得在输入的所有数据上的重建误差最小（Frobenius范式最小）：\[D^* = \arg\min_D{\sqrt{\sum_{i,j}{(x^i_j - DD^\top x^i_j)^2}}}\]
其中约束条件为：\(D^\top D = I_l\)</li>
<li>结论：<strong>\(D\)是由矩阵\(X^\top X\)的\(l\)个最大的特征值对应的特征向量构成的矩阵</strong>（利用到了实对称矩阵对应二次型的极值）</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning读书笔记（一）：导论]]></title>
    <link href="whiskytina.github.io/14902533416687.html"/>
    <updated>2017-03-23T15:15:41+08:00</updated>
    <id>whiskytina.github.io/14902533416687.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">什么是深度学习</h2>

<ul>
<li><p>背景：两类AI任务</p>

<ul>
<li>Formal Task：易于抽象和描述成计算机所能理解的表达，也因此在AI发展的早期就得到了很好的解决（例如国际象棋）</li>
<li>Intuitive Task：依赖于人类的主观直觉，难于用计算机语言进行规范性描述的任务（例如语音和图像的识别）</li>
</ul>

<span id="more"></span><!-- more --></li>
<li><p>Deep Learning的第一种解读：解决Intuitive Task的尝试</p>

<ol>
<li>Rule-based System：

<ul>
<li>基本思想：尝试人为地将知识“硬编码”为一种规则化语言。AI系统将基于庞大的人工规则库进行逻辑推断，从而解决Intuitive Task。</li>
<li>缺陷：逻辑和知识的表达过于刻板，无法处理多样的任务</li>
<li>代表作：Cyc</li>
</ul></li>
<li>Classical Machine Learning：

<ul>
<li>基本思想：尝试训练AI系统从原始数据中自主发现Pattern并获取对知识的编码。</li>
<li>缺陷：机器学习算法的performance严重依赖于数据的特征表达；然而很多时候我们并不知道什么样的特征是有效的（这就需要特征工程）</li>
<li>代表作：Logistic Regression；Naive Bayes</li>
</ul></li>
<li>Representation Learning：

<ul>
<li>基本思想：训练AI系统不仅可以学习到特征到目标的映射关系，同时还可以学习到特征的表达。</li>
<li>优势：相对于人工设计的特征，机器自我学习到的特征表达有着更好的performance。同时，对于一个全新的task，系统也可以在较短的时间内和较少的人工干预下迅速进化。</li>
<li>代表作：autoencoder</li>
</ul></li>
<li>Deep Learning：

<ul>
<li>基本思想：无论是人工设计特征还是自我学习特征，其核心目标都是尽可能分离出影响观测数据表层变化的底层因子（factors）。而直接抽取high-level的抽象因子的难度不亚于去解决原始问题本身。基于此，深度学习提出了概念层次化的网络结构（the hierarchy of concepts），将high-level的复杂特征用low-level的简单特征表达，从而解决这个问题。</li>
<li>代表作：MLP（multilayer perceptron）</li>
</ul></li>
</ol></li>
<li><p>Deep Learning的第二种解读：a multi-step computer program</p>

<ul>
<li>模型的每一层特征表达都对应着某种计算序列下的内存状态</li>
<li>并非所有的特征都对应着输入数据的因子编码——也有可能是存储了某种计算机状态（例如指针或计数器）</li>
</ul></li>
<li><p>深度真的重要吗？</p>

<ul>
<li>模型深度的度量方法：

<ol>
<li>计算序列的深度（依赖于计算元单元的定义）</li>
<li>概念层级的深度（网络层级中可能存在回路）</li>
</ol></li>
<li>深度学习的核心是用<strong>一组学习函数／概念的层次组合</strong>来表征这个世界</li>
</ul></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gensim进阶教程：训练word2vec与doc2vec模型]]></title>
    <link href="whiskytina.github.io/14902628595488.html"/>
    <updated>2017-03-23T17:54:19+08:00</updated>
    <id>whiskytina.github.io/14902628595488.html</id>
    <content type="html"><![CDATA[
<p>本篇博客是Gensim的进阶教程，主要介绍用于词向量建模的word2vec模型和用于长文本向量建模的doc2vec模型在Gensim中的实现。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">Word2vec</h2>

<p>在Gensim中实现word2vec模型非常简单。首先，我们需要将原始的训练语料转化成一个sentence的迭代器；每一次迭代返回的sentence是一个word（utf8格式）的列表：</p>

<pre><code class="language-python">class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences(&#39;/some/directory&#39;) # a memory-friendly iterator
</code></pre>

<p>接下来，我们用这个迭代器作为输入，构造一个Gensim内建的word2vec模型的对象（即将原始的one-hot向量转化为word2vec向量）：</p>

<pre><code class="language-python">model = gensim.models.Word2Vec(sentences)
</code></pre>

<p>如此，便完成了一个word2vec模型的训练。</p>

<p>我们也可以指定模型训练的参数，例如采用的模型（Skip-gram或是CBoW）；负采样的个数；embedding向量的维度等。具体的参数列表在<a href="http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec">这里</a></p>

<p>同样，我们也可以通过调用<code>save()</code>和<code>load()</code>方法完成word2vec模型的持久化。此外，word2vec对象也支持原始bin文件格式的读写。</p>

<p>Word2vec对象还支持online learning。我们可以将更多的训练数据传递给一个已经训练好的word2vec对象，继续更新模型的参数：</p>

<pre><code class="language-python">model = gensim.models.Word2Vec.load(&#39;/tmp/mymodel&#39;)
model.train(more_sentences)
</code></pre>

<p>若要查看某一个word对应的word2vec向量，可以将这个word作为索引传递给训练好的模型对象：</p>

<pre><code class="language-python">model[&#39;computer&#39;]  # raw NumPy vector of a word
</code></pre>

<h2 id="toc_1">Doc2vec</h2>

<p>Doc2vec是Mikolov在word2vec基础上提出的另一个用于计算长文本向量的工具。它的工作原理与word2vec极为相似——只是将长文本作为一个特殊的token id引入训练语料中。在Gensim中，doc2vec也是继承于word2vec的一个子类。因此，无论是API的参数接口还是调用文本向量的方式，doc2vec与word2vec都极为相似。</p>

<p>主要的区别是在对输入数据的预处理上。Doc2vec接受一个由LabeledSentence对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence是Gensim内建的一个类，它接受两个List作为其初始化的参数：word list和label list。</p>

<pre><code class="language-python">from gensim.models.doc2vec import LabeledSentence
sentence = LabeledSentence(words=[u&#39;some&#39;, u&#39;words&#39;, u&#39;here&#39;], labels=[u&#39;SENT_1&#39;])
</code></pre>

<p>类似地，可以构造一个迭代器对象，将原始的训练数据文本转化成LabeledSentence对象：</p>

<pre><code class="language-python">class LabeledLineSentence(object):
    def __init__(self, filename):
        self.filename = filename
        
    def __iter__(self):
        for uid, line in enumerate(open(filename)):
            yield LabeledSentence(words=line.split(), labels=[&#39;SENT_%s&#39; % uid])
</code></pre>

<p>准备好训练数据，模型的训练便只是一行命令：</p>

<pre><code class="language-python">from gensim.models import Doc2Vec
model = Doc2Vec(dm=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=4)
</code></pre>

<p>该代码将同时训练word和sentence label的语义向量。如果我们只想训练label向量，可以传入参数<code>train_words=False</code>以固定词向量参数。更多参数的含义可以参见这里的<a href="http://radimrehurek.com/gensim/models/doc2vec.html">API文档</a>。</p>

<p>注意，在目前版本的doc2vec实现中，每一个Sentence vector都是常驻内存的。因此，模型训练所需的内存大小同训练语料的大小正相关。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gensim入门教程]]></title>
    <link href="whiskytina.github.io/14902627439324.html"/>
    <updated>2017-03-23T17:52:23+08:00</updated>
    <id>whiskytina.github.io/14902627439324.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">What is Gensim?</h2>

<p><a href="https://github.com/RaRe-Technologies/gensim">Gensim</a>是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">基本概念</h2>

<ul>
<li>语料（Corpus）：一组原始文本的集合，用于无监督地训练文本主题的隐层结构。语料中不需要人工标注的附加信息。在Gensim中，Corpus通常是一个可迭代的对象（比如列表）。每一次迭代返回一个可用于表达文本对象的稀疏向量。</li>
<li>向量（Vector）：由一组文本特征构成的列表。是一段文本在Gensim中的内部表达。</li>
<li>稀疏向量（Sparse Vector）：通常，我们可以略去向量中多余的0元素。此时，向量中的每一个元素是一个(key, value)的tuple。</li>
<li>模型（Model）：是一个抽象的术语。定义了两个向量空间的变换（即从文本的一种向量表达变换为另一种向量表达）。</li>
</ul>

<h2 id="toc_2">Step 1. 训练语料的预处理</h2>

<p>训练语料的预处理指的是将文档中原始的字符文本转换成Gensim模型所能理解的稀疏向量的过程。</p>

<p>通常，我们要处理的原生语料是一堆文档的集合，每一篇文档又是一些原生字符的集合。在交给Gensim的模型训练之前，我们需要将这些原生字符解析成Gensim能处理的稀疏向量的格式。</p>

<p>由于语言和应用的多样性，Gensim没有对预处理的接口做出任何强制性的限定。通常，我们需要先对原始的文本进行分词、去除停用词等操作，得到每一篇文档的特征列表。例如，在词袋模型中，文档的特征就是其包含的word：</p>

<pre><code class="language-python">texts = [[&#39;human&#39;, &#39;interface&#39;, &#39;computer&#39;],
 [&#39;survey&#39;, &#39;user&#39;, &#39;computer&#39;, &#39;system&#39;, &#39;response&#39;, &#39;time&#39;],
 [&#39;eps&#39;, &#39;user&#39;, &#39;interface&#39;, &#39;system&#39;],
 [&#39;system&#39;, &#39;human&#39;, &#39;system&#39;, &#39;eps&#39;],
 [&#39;user&#39;, &#39;response&#39;, &#39;time&#39;],
 [&#39;trees&#39;],
 [&#39;graph&#39;, &#39;trees&#39;],
 [&#39;graph&#39;, &#39;minors&#39;, &#39;trees&#39;],
 [&#39;graph&#39;, &#39;minors&#39;, &#39;survey&#39;]]
</code></pre>

<p>其中，corpus的每一个元素对应一篇文档。</p>

<p>接下来，我们可以调用Gensim提供的API建立语料特征（此处即是word）的索引字典，并将文本特征的原始表达转化成词袋模型对应的稀疏向量的表达。依然以词袋模型为例：</p>

<pre><code class="language-python">from gensim import corpora
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
print corpus[0] # [(0, 1), (1, 1), (2, 1)]
</code></pre>

<p>到这里，训练语料的预处理工作就完成了。我们得到了语料中每一篇文档对应的稀疏向量（这里是bow向量）；向量的每一个元素代表了一个word在这篇文档中出现的次数。值得注意的是，虽然词袋模型是很多主题模型的基本假设，这里介绍的<code>doc2bow</code>函数并不是将文本转化成稀疏向量的唯一途径。在下一小节里我们将介绍更多的向量变换函数。</p>

<p>最后，出于内存优化的考虑，Gensim支持文档的流式处理。我们需要做的，只是将上面的列表封装成一个Python迭代器；每一次迭代都返回一个稀疏向量即可。</p>

<pre><code class="language-python">class MyCorpus(object):
    def __iter__(self):
        for line in open(&#39;mycorpus.txt&#39;):
            # assume there&#39;s one document per line, tokens separated by whitespace
            yield dictionary.doc2bow(line.lower().split())
</code></pre>

<h2 id="toc_3">Step 2. 主题向量的变换</h2>

<p>对文本向量的变换是Gensim的核心。通过挖掘语料中隐藏的语义结构特征，我们最终可以变换出一个简洁高效的文本向量。</p>

<p>在Gensim中，每一个向量变换的操作都对应着一个主题模型，例如上一小节提到的对应着词袋模型的<code>doc2bow</code>变换。每一个模型又都是一个标准的Python对象。下面以TF-IDF模型为例，介绍Gensim模型的一般使用方法。</p>

<p>首先是模型对象的初始化。通常，Gensim模型都接受一段训练语料（注意在Gensim中，语料对应着一个稀疏向量的迭代器）作为初始化的参数。显然，越复杂的模型需要配置的参数越多。</p>

<pre><code class="language-python">from gensim import models
tfidf = models.TfidfModel(corpus)
</code></pre>

<p>其中，corpus是一个返回bow向量的迭代器。这两行代码将完成对corpus中出现的每一个特征的IDF值的统计工作。</p>

<p>接下来，我们可以调用这个模型将任意一段语料（依然是bow向量的迭代器）转化成TFIDF向量（的迭代器）。需要注意的是，这里的bow向量必须与训练语料的bow向量共享同一个特征字典（即共享同一个向量空间）。</p>

<pre><code class="language-python">doc_bow = [(0, 1), (1, 1)]
print tfidf[doc_bow] # [(0, 0.70710678), (1, 0.70710678)]
</code></pre>

<p>注意，同样是出于内存的考虑，<code>model[corpus]</code>方法返回的是一个迭代器。如果要多次访问<code>model[corpus]</code>的返回结果，可以先讲结果向量序列化到磁盘上。</p>

<p>我们也可以将训练好的模型持久化到磁盘上，以便下一次使用：</p>

<pre><code class="language-python">tfidf.save(&quot;./model.tfidf&quot;)
tfidf = models.TfidfModel.load(&quot;./model.tfidf&quot;)
</code></pre>

<p>Gensim内置了多种主题模型的向量变换，包括LDA，LSI，RP，HDP等。这些模型通常以bow向量或tfidf向量的语料为输入，生成相应的主题向量。所有的模型都支持流式计算。关于Gensim模型更多的介绍，可以参考这里：<a href="https://radimrehurek.com/gensim/apiref.html">API Reference</a></p>

<h2 id="toc_4">Step 3. 文档相似度的计算</h2>

<p>在得到每一篇文档对应的主题向量后，我们就可以计算文档之间的相似度，进而完成如文本聚类、信息检索之类的任务。在Gensim中，也提供了这一类任务的API接口。</p>

<p>以信息检索为例。对于一篇待检索的query，我们的目标是从文本集合中检索出主题相似度最高的文档。</p>

<p>首先，我们需要将待检索的query和文本放在同一个向量空间里进行表达（以LSI向量空间为例）：</p>

<pre><code class="language-python"># 构造LSI模型并将待检索的query和文本转化为LSI主题向量
# 转换之前的corpus和query均是BOW向量
lsi_model = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
documents = lsi_model[corpus]
query_vec = lsi_model[query]
</code></pre>

<p>接下来，我们用待检索的文档向量初始化一个相似度计算的对象：</p>

<pre><code class="language-python">index = similarities.MatrixSimilarity(documents)
</code></pre>

<p>我们也可以通过<code>save()</code>和<code>load()</code>方法持久化这个相似度矩阵：</p>

<pre><code class="language-python">index.save(&#39;/tmp/deerwester.index&#39;)
index = similarities.MatrixSimilarity.load(&#39;/tmp/deerwester.index&#39;)
</code></pre>

<p>注意，如果待检索的目标文档过多，使用<code>similarities.MatrixSimilarity</code>类往往会带来内存不够用的问题。此时，可以改用<code>similarities.Similarity</code>类。二者的接口基本保持一致。</p>

<p>最后，我们借助<code>index</code>对象计算任意一段query和所有文档的（余弦）相似度：</p>

<pre><code class="language-python">sims = index[query_vec] # return: an iterator of tuple (idx, sim)
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[特征选择中的常用指标]]></title>
    <link href="whiskytina.github.io/14902626300821.html"/>
    <updated>2017-03-23T17:50:30+08:00</updated>
    <id>whiskytina.github.io/14902626300821.html</id>
    <content type="html"><![CDATA[
<p>在特征选择中，有三类常用的指标：信息增益；信息增益率；和基尼系数。这些指标也是决策树算法的基础。现摘录这三个指标的定义如下：</p>

<ol>
<li><p>信息增益（Information Gain）<br/>
特征\(T\)为分类系统\(C\)带来的信息增益为：<br/>
\[\begin{aligned}<br/>
GainEntropy(T) &amp;= Entropy(C) − Entropy(C|T) \\<br/>
&amp;= Entropy(C) − \sum_i{p(T_i)Entropy(C|T_i)} \\<br/>
\end{aligned}\]<br/>
其中，\(Entropy(C)\)为系统关于分类属性\(C\)的信息熵，定义了系统描述某一个分类所需要的平均信息量。计算公式为：<br/>
\[Entropy(C) = -\sum_i{p(C_i)\log_2{p(C_i)}}\]<br/>
\(Entropy(C|T_i )\)为固定特征\(T=T_i\)下系统关于分类属性\(C\)的条件信息熵，计算公式为：<br/>
\[Entropy(C|T_i) = -\sum_j{p(C_j|T_i)\log_2{p(C_j|T_i)}}\]</p>

<span id="more"></span><!-- more --></li>
<li><p>信息增益率（Information Gain Ratio）<br/>
特征\(T\)的信息增益是与特征\(T\)的取值个数有着密切联系的。一般来说，特征的取值个数越多，信息增益往往也会越大（极端的例子如用户ID特征）。为了避免这种特征取值个数的不同对特征选择时泛化能力的影响，考虑对信息增益进行一定程度的归一化。这就是信息增益率。<br/>
\[GainEntropyRatio(T)=\frac{GainEntropy(T)}{SplitInformation(T)}\]<br/>
其中，\(SplitInformation(T)\)为系统关于特征\(T\)的信息熵：<br/>
\[SplitInformation(T) = Entropy(T) = -\sum_i{p(T_i)\log_2{p(T_i)}}\]<br/>
也就是说，系统描述一个特征所需的信息量越大（表现为特征的分布越分散越平均），这个特征的泛化能力被认为越弱，也就不适合作为模型的特征被选择进来。</p></li>
<li><p>基尼系数（Gini coefficient）<br/>
我们定义一个\(K\)分类系统\(C\)的基尼系数为：<br/>
\[Gini(C) = \sum_{i=1}^K{p(C_i)(1-p(C_i))} = 1 - \sum_{i=1}^K{p^2(C_i)}\]<br/>
类似信息熵，基尼系数衡量了一个系统的不确定性。高基尼系数意味着一个不确定性高的系统：更加分散和平均的类别分布，和更高的信息量需求（来确定某一个类别）。<br/>
我们可以类似的定义基尼系数增益来进行特征选择：<br/>
\[GainGini(T) = Gini(C) - \sum_i{p(T_i)Gini(C|T_i)}\]<br/>
也可以从基尼系数出发定义基尼系数增益率。这里不再赘述。</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iTerms2结合Expect实现SSH自动登录]]></title>
    <link href="whiskytina.github.io/14902607196341.html"/>
    <updated>2017-03-23T17:18:39+08:00</updated>
    <id>whiskytina.github.io/14902607196341.html</id>
    <content type="html"><![CDATA[
<p>iTerm2是Mac上一款十分强大的终端，然而与Windows上常用的SecureCRT或是XShell相比，iTerm2本身无法记录SSH登录名和密码，在实际使用中颇有不便。不过我们可以在iTerm2的Profiles中结合Expect脚本实现类似的功能。</p>

<span id="more"></span><!-- more -->

<ul>
<li><p>（预备工作）配置ssh config实现跳板机自动登录<br/>
由于公司的限制，必须通过跳板机才能登录上内部的开发机。而我们公司跳板机的密码又是PIN＋动态TOKEN的形式，因此只能通过共享SSH渠道来实现免密登录：</p>

<ol>
<li>打开<code>.ssh/config</code>文件（若不存在则新建一个）</li>
<li><p>配置SSH渠道共享：</p>

<pre><code class="language-bash">Host *
ControlMaster Auto
ControlPath ~/.ssh/%h-%p-%r
ControlPersist yes
Host baidu
hostname 跳板机
user username
</code></pre></li>
</ol>

<p>这样，在首次执行<code>ssh baidu</code>并输入PIN＋TOKEN后，我们就可以通过同一个命令随时免密登录跳板机。</p></li>
</ul>

<ul>
<li><p>配置Expect脚本实现SSH免密登录<br/>
实现免密登录跳板机后，我们可以通过一个Expect脚本实现SSH免密登录（由于跳板机上无法保存文件，因此无法通过配置秘钥来实现这一功能）：</p>

<pre><code>#!/usr/bin/expect
# 参数1: username; 参数2：远程host；参数3：远程password
if { $argc != 3 } {
    send_user &quot;Usage: user host password\n&quot;
    exit
}

set user [lindex $argv 0]
set host [lindex $argv 1]
set pw [lindex $argv 2]

# 登录跳板机
catch {spawn ssh xxx}

# 登录远程机器
expect &quot;*ogin:*&quot; { send &quot;ssh -l $user $host\r&quot; }
expect &quot;*assword:*&quot; { send &quot;$pw\r&quot;}

interact
</code></pre>

<p>该脚本接收三个参数：用户名，主机名，和登录密码。<br/>
将该脚本命名为login.exp，并保存在系统的PATH路径下（比如<code>/usr/local/bin/</code>路径）<br/>
然后，在iTerm2的Profiles中配置登录时的脚本命令：<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902624504403.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>使用时，首先调用<code>ssh baidu</code>并输入PIN＋TOKEN后，即可通过Profiles免密登录内网的开发机。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[word2vec前世今生]]></title>
    <link href="whiskytina.github.io/14902453987041.html"/>
    <updated>2017-03-23T13:03:18+08:00</updated>
    <id>whiskytina.github.io/14902453987041.html</id>
    <content type="html"><![CDATA[
<p>2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">Statistical Language Model</h2>

<p>在深入word2vec算法的细节之前，我们首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？之所为称其为一个基本问题，是因为它在很多NLP任务中都扮演着重要的角色。例如，在机器翻译的问题中，如果我们知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子做为翻译结果返回。</p>

<p>统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列\(S=w_1, w_2, ... , w_T\)，它的概率可以表示为：<br/>
\[P(S)=P(w_1, w_2, ..., w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, ..., w_{t-1})\]<br/>
即将序列的联合概率转化为一系列条件概率的乘积。问题变成了如何去预测这些给定previous words下的条件概率\(p(w_t|w_1,w_2,...,w_{t-1})\)。</p>

<p>由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么卵用。我们更多的是采用其简化版本——Ngram模型：<br/>
\[p(w_t|w_1, w_2, ..., w_{t-1}) \approx p(w_t|w_{t-n+1}, ..., w_{t-1})\]<br/>
常见的如bigram模型（\(N=2\)）和trigram模型（\(N=3\)）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑\(N&gt;3\)的模型。</p>

<p>我们可以用最大似然法去求解Ngram模型的参数——等价于去统计每个Ngram的条件词频。</p>

<p>为了避免统计中出现的零概率问题（一段从未在训练集中出现过的Ngram片段会使得整个序列的概率为\(0\)），人们基于原始的Ngram模型进一步发展出了back-off trigram模型（用低阶的bigram和unigram代替零概率的trigram）和interpolated trigram模型（将条件概率表示为unigram、bigram、trigram三者的线性函数）。此处不再赘述。感兴趣者可进一步阅读相关的文献<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。</p>

<h2 id="toc_1">Distributed Representation</h2>

<p>不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（\(N&gt;3\)）。其次，它没有考虑词与词之间内在的联系性。例如，考虑&quot;the cat is walking in the bedroom&quot;这句话。如果我们在训练语料中看到了很多类似“the dog is walking in the bedroom”或是“the cat is running in the bedroom”这样的句子，那么，即使我们没有见过这句话，也可以从“cat”和“dog”（“walking”和“running”）之间的相似性，推测出这句话的概率<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。然而， Ngram模型做不到。</p>

<p>这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量（除了一个词典索引的下标对应的方向上是\(1\)，其余方向上都是\(0\)）。例如，对于一个大小为\(5\)的词典：{&quot;I&quot;, &quot;love&quot;, &quot;nature&quot;, &quot;luaguage&quot;, &quot;processing&quot;}，“nature”对应的one-hot向量为：\([0, 0, 1, 0, 0]\)。显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（the curse of dimensionality）</p>

<p>于是，人们就自然而然地想到，能否用一个连续的稠密向量去刻画一个word的特征呢？这样，我们不仅可以直接刻画词与词之间的相似度，还可以建立一个从向量到概率的平滑函数模型，使得相似的词向量可以映射到相近的概率空间上。这个稠密连续向量也被称为word的distributed representation<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。</p>

<p>事实上，这个概念在信息检索（Information Retrieval）领域早就已经被广泛地使用了。只不过，在IR领域里，这个概念被称为向量空间模型（Vector Space Model，以下简称VSM）。</p>

<p>VSM是基于一种Statistical Semantics Hypothesis<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>：语言的统计特征隐藏着语义的信息（Statistical pattern of human word usage can be used to figure out what people mean）。例如，两篇具有相似词分布的文档可以被认为是有着相近的主题。这个Hypothesis有很多衍生版本。其中，比较广为人知的两个版本是Bag of Words Hypothesis和Distributional Hypothesis。前者是说，一篇文档的词频（而不是词序）代表了文档的主题；后者是说，上下文环境相似的两个词有着相近的语义。后面我们会看到，word2vec算法也是基于Distributional的假设。</p>

<p>那么，VSM是如何将稀疏离散的one-hot词向量映射为稠密连续的distributional representation的呢？</p>

<p>简单来说，基于Bag of Words Hypothesis，我们可以构造一个term-document矩阵\(A\)：矩阵的行\(A_{i,:}\)对应着词典里的一个word；矩阵的列\(A_{:,j}\)对应着训练语料里的一篇文档；矩阵里的元素\(A_{ij}\)代表着word \(w_i\)在文档\(D_j\)中出现的次数（或频率）。那么，我们就可以提取行向量做为word的语义向量（不过，在实际应用中，我们更多的是用列向量做为文档的主题向量）。</p>

<p>类似地，我们可以基于Distributional Hypothesis构造一个word-context的矩阵。此时，矩阵的列变成了context里的word，矩阵的元素也变成了一个context窗口里word的共现次数。</p>

<p>注意，这两类矩阵的行向量所计算的相似度有着细微的差异：term-document矩阵会给经常出现在同一篇document里的两个word赋予更高的相似度；而word-context矩阵会给那些有着相同context的两个word赋予更高的相似度。后者相对于前者是一种更高阶的相似度，因此在传统的信息检索领域中得到了更加广泛的应用。</p>

<p>不过，这种co-occurrence矩阵仍然存在着数据稀疏性和维度灾难的问题。为此，人们提出了一系列对矩阵进行降维的方法（如LSI／LSA等）。这些方法大都是基于SVD的思想，将原始的稀疏矩阵分解为两个低秩矩阵乘积的形式。</p>

<p>关于VSM更多的介绍，可以进一步阅读文末的参考文献<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup>。</p>

<h2 id="toc_2">Neural Network Language Model</h2>

<p>接下来，让我们回到对统计语言模型的讨论。鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM），并首次提出了word embedding的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。</p>

<p>NNLM模型的基本思想可以概括如下：<br/>
1. 假定词表中的每一个word都对应着一个连续的特征向量；<br/>
2. 假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；<br/>
3. <strong>同时学习</strong>词向量的权重和概率模型里的参数。</p>

<p>值得注意的一点是，这里的词向量也是要学习的参数。</p>

<p>在03年的论文里，Bengio等人采用了一个简单的前向反馈神经网络\(f(w_{t-n+1},...,w_{t})\)来拟合一个词序列的条件概率\(p(w_t|w_1,w_2,...,w_{t-1})\)。整个模型的网络结构见下图：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902454895705.jpg" alt="the neural network language model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>我们可以将整个模型拆分成两部分加以理解：<br/>
1. 首先是一个线性的embedding层。它将输入的\(N-1\)个one-hot词向量，通过一个共享的\(D \times V\)的矩阵\(C\)，映射为\(N-1\)个分布式的词向量（distributed vector）。其中，\(V\)是词典的大小，\(D\)是embedding向量的维度（一个先验参数）。\(C\)矩阵里存储了要学习的word vector。<br/>
2. 其次是一个简单的前向反馈神经网络\(g\)。它由一个tanh隐层和一个softmax输出层组成。通过将embedding层输出的\(N-1\)个词向量映射为一个长度为\(V\)的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：<br/>
\[p(w_i|w_1,w_2,...,w_{t-1}) \approx f(w_i, w_{t-1}, ..., w_{t-n+1}) = g(w_i, C(w_{t-n+1}), ..., C(w_{t-1}))\]</p>

<p>我们可以通过最小化一个cross-entropy的正则化损失函数来调整模型的参数\(\theta\)：<br/>
\[L(\theta)=\frac{1}{T}\sum_t{\log{f(w_t, w_{t-1}, ..., w_{t-n+1})}}+R(\theta)\]</p>

<p>其中，模型的参数\(\theta\)包括了embedding层矩阵\(C\)的元素，和前向反馈神经网络模型\(g\)里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）。计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）。</p>

<p>然而，抛却复杂的参数空间，我们不禁要问，为什么这样一个简单的模型会取得巨大的成功呢？</p>

<p>仔细观察这个模型就会发现，它其实在同时解决两个问题：一个是统计语言模型里关注的条件概率\(p(w_t|context)\)的计算；一个是向量空间模型里关注的词向量的表达。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上缓解数据稀疏性和维度灾难的问题。另一方面，以条件概率\(p(w_t|context)\)为学习目标去更新词向量的权重，具有更强的导向性，同时也与VSM里的Distributional Hypothesis不谋而合。</p>

<h2 id="toc_3">CBoW &amp; Skip-gram Model</h2>

<p>铺垫了这么多，终于要轮到主角出场了。</p>

<p>不过在主角正式登场前，我们先看一下NNLM存在的几个问题。</p>

<p>一个问题是，同Ngram模型一样，NNLM模型只能处理定长的序列。在03年的论文里，Bengio等人将模型能够一次处理的序列长度\(N\)提高到了\(5\)，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。</p>

<p>因此，Mikolov等人在2010年提出了一种RNNLM模型<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>，用递归神经网络代替原始模型里的前向反馈神经网络，并将embedding层与RNN里的隐藏层合并，从而解决了变长序列的问题。</p>

<p>另一个问题就比较严重了。NNLM的训练太慢了。即便是在百万量级的数据集上，即便是借助了40个CPU进行训练，NNLM也需要耗时数周才能给出一个稍微靠谱的解来。显然，对于现在动辄上千万甚至上亿的真实语料库，训练一个NNLM模型几乎是一个impossible mission。</p>

<p>这时候，还是那个Mikolov站了出来。他注意到，原始的NNLM模型的训练其实可以拆分成两个步骤：<br/>
1. 用一个简单模型训练出连续的词向量；<br/>
2. 基于词向量的表达，训练一个连续的Ngram神经网络模型。<br/>
而NNLM模型的计算瓶颈主要是在第二步。</p>

<p>如果我们只是想得到word的连续特征向量，是不是可以对第二步里的神经网络模型进行简化呢？</p>

<p>Mikolov是这么想的，也是这么做的。他在2013年一口气推出了两篇paper，并开源了一款计算词向量的工具——至此，word2vec横空出世，主角闪亮登场。</p>

<p>下面，我将带领大家简单剖析下word2vec算法的原理。有了前文的基础，理解word2vec算法就变得很简单了。</p>

<p>首先，我们对原始的NNLM模型做如下改造：<br/>
1. 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的embedding layer与输出层的softmax layer连接；<br/>
2. 忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个embedding layer；<br/>
3. 将future words纳入上下文环境</p>

<p>得到的模型称之为CBoW模型（Continuous Bag-of-Words Model），也是word2vec算法的第一个模型：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902460210953.jpg" alt="CBoW Model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>从数学上看，CBoW模型等价于一个词袋模型的向量乘以一个embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。</p>

<p>CBoW模型依然是从context对target word的预测中学习到词向量的表达。反过来，我们能否从target word对context的预测中学习到word vector呢？答案显然是可以的：</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902460526207.jpg" alt="Skip-gram Model " class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>这个模型被称为Skip-gram模型（名称源于该模型在训练时会对上下文环境里的word进行采样）。</p>

<p>如果将Skip-gram模型的前向计算过程写成数学形式，我们得到：<br/>
\[p(w_o|w_i)=\frac{e^{U_o \cdot V_i}}{\sum_j{e^{U_j \cdot V_i}}}\]<br/>
其中，\(V_i\)是embedding层矩阵里的列向量，也被称为\(w_i\)的input vector。\(U_j\)是softmax层矩阵里的行向量，也被称为\(w_j\)的output vector。</p>

<p>因此，Skip-gram模型的本质是<strong>计算输入word的input vector与目标word的output vector之间的余弦相似度，并进行softmax归一化</strong>。我们要学习的模型参数正是这两类词向量。</p>

<p>然而，直接对词典里的\(V\)个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax（Hierarchical Softmax）和负采样（Negative Sampling）。</p>

<h3 id="toc_4">Hierarchical Softmax<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup></h3>

<p>层次Softmax的方法最早由Bengio在05年引入到语言模型中。它的基本思想是将复杂的归一化概率分解为一系列条件概率乘积的形式：<br/>
\[p(v|context)=\prod_{i=1}^m{p(b_i(v)|b_1(v), ..., b_{i-1}(v), context)}\]<br/>
其中，每一层条件概率对应一个二分类问题，可以通过一个简单的逻辑回归函数去拟合。这样，我们将对\(V\)个词的概率归一化问题，转化成了对\(\log{V}\)个词的概率拟合问题。</p>

<p>我们可以通过构造一颗分类二叉树来直观地理解这个过程。首先，我们将原始字典\(D\)划分为两个子集\(D_1\)、\(D_2\)，并假设在给定context下，target word属于子集\(D_1\)的概率\(p(w_t \in D_1|context)\)服从logistical function的形式：<br/>
\[p(w_t \in D_1|context)=\frac{1}{1+e^{-U_{D_{root}} \cdot V_{w_t}}}\]<br/>
其中，\(U_{D_{root}}\)和\(V_{w_t}\)都是模型的参数。</p>

<p>接下来，我们可以对子集\(D_1\)和\(D_2\)进一步划分。重复这一过程，直到集合里只剩下一个word。这样，我们就将原始大小为\(V\)的字典\(D\)转换成了一颗深度为\(\log V\)的二叉树。树的叶子节点与原始字典里的word一一对应；非叶节点则对应着某一类word的集合。显然，从根节点出发到任意一个叶子节点都只有一条唯一路径——这条路径也编码了这个叶子节点所属的类别。</p>

<p>同时，从根节点出发到叶子节点也是一个随机游走的过程。因此，我们可以基于这颗二叉树对叶子节点出现的似然概率进行计算。例如，对于训练样本里的一个target word \(w_t\)，假设其对应的二叉树编码为\(\{1, 0, 1, ..., 1\}\)，则我们构造的似然函数为：<br/>
\[p(w_t|context)=p(D_1=1|context)p(D_2=0|D_1=1)\dots p(w_t|D_k=1)\]<br/>
乘积中的每一项都是一个逻辑回归的函数。</p>

<p>我们可以通过最大化这个似然函数来求解二叉树上的参数——非叶节点上的向量，用来计算游走到某一个子节点的概率。</p>

<p>层次Softmax是一个很巧妙的模型。它通过构造一颗二叉树，将目标概率的计算复杂度从最初的\(V\)降低到了\(\log V\)的量级。不过付出的代价是人为增强了词与词之间的耦合性。例如，一个word出现的条件概率的变化，会影响到其路径上所有非叶节点的概率变化，间接地对其他word出现的条件概率带来不同程度的影响。因此，构造一颗有意义的二叉树就显得十分重要。实践证明，在实际的应用中，基于Huffman编码的二叉树可以满足大部分应用场景的需求。</p>

<h3 id="toc_5">Negative Sampling<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup></h3>

<p>负采样的思想最初来源于一种叫做Noise-Contrastive Estimation的算法<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>，原本是为了解决那些无法归一化的概率模型的参数预估问题。与改造模型输出概率的层次Softmax算法不同，NCE算法改造的是模型的似然函数。</p>

<p>以Skip-gram模型为例，其原始的似然函数对应着一个Multinomial的分布。在用最大似然法求解这个似然函数时，我们得到一个cross-entropy的损失函数：<br/>
\[J(\theta)=-\frac{1}{T}\sum_{t=1}^T{\sum_{-c \leq j \leq c, j \neq 0}{\log p(w_{t+j}|w_t)}}\]<br/>
式中的\(p(w_{t+j}|w_t)\)是一个在整个字典上归一化了的概率。</p>

<p>而在NCE算法中，我们构造了这样一个问题：对于一组训练样本<context, word>，我们想知道，target word的出现，是来自于context的驱动，还是一个事先假定的背景噪声的驱动？显然，我们可以用一个逻辑回归的函数来回答这个问题：<br/>
\[p(D=1|w, context)=\frac{p(w|context)}{p(w|context)+kp_n(w)}=\sigma (\log p(w|context) - \log kp_n(w))\]<br/>
这个式子给出了一个target word \(w\)来自于context驱动的概率。其中，\(k\)是一个先验参数，表明噪声的采样频率。\(p(w|context)\)是一个非归一化的概率分布，这里采用softmax归一化函数中的分子部分。\(p_n(w)\)则是背景噪声的词分布。通常采用word的unigram分布。</p>

<p>通过对噪声分布的\(k\)采样，我们得到一个新的数据集：<context, word, label>。其中，label标记了数据的来源（真实数据分布还是背景噪声分布？）。在这个新的数据集上，我们就可以用最大化上式中逻辑回归的似然函数来求解模型的参数。</p>

<p>而Mikolov在2013年的论文里提出的负采样算法， 是NCE的一个简化版本。在这个算法里，Mikolov抛弃了NCE似然函数中对噪声分布的依赖，直接用原始softmax函数里的分子定义了逻辑回归的函数，进一步简化了计算：<br/>
\[p(D=1|w_o, w_i)=\sigma (U_o \cdot V_i)\]<br/>
此时，模型相应的目标函数变为：<br/>
\[J(\theta) = \log \sigma(U_o \cdot V_i) + \sum_{j=1}^k{E_{w_j \sim p_n(w)}[\log \sigma(- U_j \cdot V_i)]}\]</p>

<p>除了这里介绍的层次Softmax和负采样的优化算法，Mikolov在13年的论文里还介绍了另一个trick：下采样（subsampling）。其基本思想是在训练时依概率随机丢弃掉那些高频的词：<br/>
\[p_{discard}(w) = 1 - \sqrt{\frac{t}{f(w)}}\]<br/>
其中，\(t\)是一个先验参数，一般取为\(10^{-5}\)。\(f(w)\)是\(w\)在语料中出现的频率。</p>

<p>实验证明，这种下采样技术可以显著提高低频词的词向量的准确度。</p>

<h2 id="toc_6">Beyond the Word Vector</h2>

<p>介绍完word2vec模型的算法和原理，我们来讨论一些轻松点的话题——模型的应用。</p>

<p>13年word2vec模型横空出世后，人们最津津乐道的是它学到的向量在语义和语法相似性上的应用——尤其是这种相似性居然对数学上的加减操作有意义<sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup>！最经典的一个例子是，\(v(&quot;King&quot;) - v(&quot;Man&quot;) + v(&quot;Woman&quot;) = v(&quot;Queen&quot;)\)。然而，这种例子似乎并没有太多实际的用途。</p>

<p>除此之外，word2vec模型还被应用于机器翻译和推荐系统领域。</p>

<h3 id="toc_7">Machine Translation<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup></h3>

<p>与后来提出的在sentence level上进行机器翻译的RNN模型不同，word2vec模型主要是用于词粒度上的机器翻译。</p>

<p>具体来说，我们首先从大量的单语种语料中学习到每种语言的word2vec表达，再借助一个小的双语语料库学习到两种语言word2vec表达的线性映射关系\(W\)。构造的损失函数为：<br/>
\[J(W)=\sum_{i=1}^n{||Wx_i - z_i||^2}\]</p>

<p>在翻译的过程中，我们首先将源语言的word2vec向量通过矩阵\(W\)映射到目标语言的向量空间上；再在目标语言的向量空间中找出与投影向量距离最近的word做为翻译的结果返回。</p>

<p>其原理是，不同语言学习到的word2vec向量空间在几何上具有一定的同构性。映射矩阵\(W\)本质上是一种空间对齐的线性变换。</p>

<h3 id="toc_8">Item2Vec<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup></h3>

<p>本质上，word2vec模型是在word-context的co-occurrence矩阵基础上建立起来的。因此，任何基于co-occurrence矩阵的算法模型，都可以套用word2vec算法的思路加以改进。</p>

<p>比如，推荐系统领域的协同过滤算法。</p>

<p>协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果我们将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达，在更高阶上计算item间的相似度。</p>

<p>关于word2vec更多应用的介绍，可以进一步参考这篇文献<sup id="fnref9"><a href="#fn9" rel="footnote">9</a></sup>。</p>

<h2 id="toc_9">Word Embedding</h2>

<p>最后，我想简单阐述下我对word embedding的几点思考。不一定正确，也欢迎大家提出不同的意见。</p>

<p>Word embedding最早出现于Bengio在03年发表的开创性文章中<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup>。通过嵌入一个线性的投影矩阵（projection matrix），将原始的one-hot向量映射为一个稠密的连续向量，并通过一个语言模型的任务去学习这个向量的权重。这一思想后来被广泛应用于包括word2vec在内的各种NLP模型中。</p>

<p>Word embedding的训练方法大致可以分为两类：一类是无监督或弱监督的预训练；一类是端对端（end to end）的有监督训练。</p>

<p>无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，用少量人工标注的样本去fine-tune整个模型。</p>

<p>相比之下，端对端的有监督模型在最近几年里越来越受到人们的关注。与无监督模型相比，端对端的模型在结构上往往更加复杂。同时，也因为有着明确的任务导向，端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，可以学习到语义更丰富的词向量表达。</p>

<p>Word embedding的另一个研究方向是在更高层次上对sentence的embedding向量进行建模。</p>

<p>我们知道，word是sentence的基本组成单位。一个最简单也是最直接得到sentence embedding的方法是将组成sentence的所有word的embedding向量全部加起来——类似于CBoW模型。</p>

<p>显然，这种简单粗暴的方法会丢失很多信息。</p>

<p>另一种方法借鉴了word2vec的思想——将sentence或是paragraph视为一个特殊的word，然后用CBoW模型或是Skip-gram进行训练<sup id="fnref10"><a href="#fn10" rel="footnote">10</a></sup>。这种方法的问题在于，对于一篇新文章，总是需要重新训练一个新的sentence2vec。此外，同word2vec一样，这个模型缺少有监督的训练导向。</p>

<p>个人感觉比较靠谱的是第三种方法——基于word embedding的端对端的训练。Sentence本质上是word的序列。因此，在word embedding的基础上，我们可以连接多个RNN模型或是卷积神经网络，对word embedding序列进行编码，从而得到sentence embedding。</p>

<p>这方面的工作已有很多。有机会，我会再写一篇关于sentence embedding的综述。</p>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

<li id="fn2">
<p>Turney, P. D., &amp; Pantel, P. (2010). From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research, 37(1).&nbsp;<a href="#fnref2" rev="footnote">&#8617;</a></p>
</li>

<li id="fn3">
<p>Mikolov, T., Karafiát, M., Burget, L., &amp; Cernocký, J. (2010). Recurrent neural network based language model. Interspeech.&nbsp;<a href="#fnref3" rev="footnote">&#8617;</a></p>
</li>

<li id="fn4">
<p>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats.&nbsp;<a href="#fnref4" rev="footnote">&#8617;</a></p>
</li>

<li id="fn5">
<p>Mnih, A., &amp; Kavukcuoglu, K. (2013). Learning word embeddings efficiently with noise-contrastive estimation, 2265–2273.&nbsp;<a href="#fnref5" rev="footnote">&#8617;</a></p>
</li>

<li id="fn6">
<p>Mikolov, T., Yih, W., &amp; Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Hlt-Naacl.&nbsp;<a href="#fnref6" rev="footnote">&#8617;</a></p>
</li>

<li id="fn7">
<p>Mikolov, T., Le, Q. V., &amp; Sutskever, I. (2013, September 17). Exploiting Similarities among Languages for Machine Translation. arXiv.org.&nbsp;<a href="#fnref7" rev="footnote">&#8617;</a></p>
</li>

<li id="fn8">
<p>Barkan, O., &amp; Koenigstein, N. (2016, March 14). Item2Vec: Neural Item Embedding for Collaborative Filtering. arXiv.org.&nbsp;<a href="#fnref8" rev="footnote">&#8617;</a></p>
</li>

<li id="fn9">
<p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12(Aug), 2493–2537.&nbsp;<a href="#fnref9" rev="footnote">&#8617;</a></p>
</li>

<li id="fn10">
<p>Le, Q. V., &amp; Mikolov, T. (2014, May 16). Distributed Representations of Sentences and Documents. arXiv.org.&nbsp;<a href="#fnref10" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[用Docker部署Jupyter Notebook]]></title>
    <link href="whiskytina.github.io/14902606130831.html"/>
    <updated>2017-03-23T17:16:53+08:00</updated>
    <id>whiskytina.github.io/14902606130831.html</id>
    <content type="html"><![CDATA[
<p>想在公司的服务器上搭建一套Jupyter Notebook的开发环境，这样无论走到哪里都可以打开浏览器敲代码了（有木有很屌丝）。然而公司的服务器各种不给力。于是想到了run it anywhere的Docker。</p>

<span id="more"></span><!-- more -->

<p>采用Docker部署Jupyter Notebook还有一个优势：一次生成镜像后，可以很方便地二次部署到其他服务器上（比如Amazon的AWS），省去了很多重复性的工作。</p>

<p>由于是第一次使用Docker，踩了不少坑。所以这里记录下了Docker的一些主要使用方法。</p>

<p>下面进入正题。</p>

<p>首先，创建一个anaconda镜像的容器：</p>

<pre><code>docker run -itd -p 8888:8888 -v /your/local/dir:/docker/dir --name anaconda continuumio/anaconda /bin/bash
</code></pre>

<p>简单解释一下这行代码：<br/>
- <code>run</code>: 创建一个容器<br/>
- <code>-i</code>: 打开容器的标准输入<br/>
- <code>-t</code>: 分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上<br/>
- <code>-d</code>: 让容器后台运行<br/>
- <code>-p hostPort:containerPort</code>: 将本地的hostPort端口映射到容器的containerPort端口上。此处是对8888端口进行绑定（8888端口也是Jupyter Notebook的默认端口）<br/>
- <code>-v /your/local/dir:/docker/dir</code>: 挂载一个本地目录到容器中去，以实现本地和容器的文件共享。<br/>
- <code>--name</code>: 指定容器的名字，方便后面对容器的管理<br/>
- <code>continuumio/anaconda</code>: 要加载的镜像仓库地址<br/>
- <code>/bin/bash</code>: 默认容器创建完成后的执行命令。这里为了对容器进行进一步的修改，启动了bash应用。（这也是很常用的一个命令）</p>

<p>如果是第一次创建，docker会首先从服务器上获取镜像（相当于执行命令：<code>docker pull continuumio/anaconda</code>）。</p>

<p>创建完成后，可以用<code>docker ps</code>查看正在运行的容器（<code>docker ps -a</code>查看创建过的所有容器；<code>docker images</code>查看所有下载到本地的镜像）。</p>

<p>以下是几个与容器（镜像）有关的常用命令：<br/>
- <code>docker rm [container]</code>: 删除一个已经停止的容器<br/>
- <code>docker rmi [image]</code>: 删除一个本地的镜像<br/>
- <code>docker start [container]</code>: 启动一个容器<br/>
- <code>docker stop [container]</code>: 停止一个容器<br/>
- <code>docker inspect [container]</code>: 列出容器的相关信息<br/>
- <code>docker attach [container]</code>: 进入一个容器</p>

<p>接下来，我们将用最后一个命令进入到容器里面，并安装Google的深度学习工具包：Tensorflow。</p>

<pre><code>docker attach anaconda
</code></pre>

<p>首先安装pip：</p>

<pre><code>apt-get update
apt-get install python-pip python-dev
</code></pre>

<p>用pip安装Tensorflow：</p>

<pre><code>export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl
pip install --upgrade $TF_BINARY_URL
</code></pre>

<p>你也可以继续定制属于你自己的Jupyter（<a href="https://github.com/ipython/ipython/wiki/IPython-kernels-for-other-languages">让Jupyter支持更多的语言</a>）。笔者就安装了R和Octave两种语言的kernel。</p>

<p>最后，开启Notebook服务：</p>

<pre><code>jupyter notebook --port=8888 --ip=* --no-browser
</code></pre>

<p>至此，打开浏览器，开启你的Jupyter之旅吧！</p>

<p>对容器进行更改后，我们需要将其保存为镜像，方便日后的使用。</p>

<p>退出刚才进入的容器（快捷键：Ctrl+P+Q），并执行命令：</p>

<pre><code>docker commit anaconda weitian/myanaconda
</code></pre>

<p>即生成了一个名为weitian/myanaconda的镜像（可以用images命令查看）。</p>

<p>我们可以将这个镜像push到docker hub上：</p>

<pre><code>docker login #首次上传需要先登录docker hub
docker push weitian/myanaconda
</code></pre>

<p>也可以导出到本地：</p>

<pre><code>docker save -o myanaconda.tar weitian/myanaconda
# 本地加载
docker load --input myanaconda.tar
</code></pre>

<p>更多关于Docker的使用介绍可以参见Gitbook上的一本书：<a href="https://yeasy.gitbooks.io/docker_practice/content/">Docker —— 从入门到实践</a>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[恋在Sri Lanka]]></title>
    <link href="whiskytina.github.io/14902633244327.html"/>
    <updated>2017-03-23T18:02:04+08:00</updated>
    <id>whiskytina.github.io/14902633244327.html</id>
    <content type="html"><![CDATA[
<p>Sri Lanka，古称锡兰，一个带有浓郁的热带风情的名字，一个被誉为印度洋上的眼泪的国家。也正是因着这两个因素，我第一眼就被这个神秘的海上岛国吸引住了，不能自拔。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902633497322.jpg" alt="印度洋上的眼泪" class="mw_img_center" style="width:364px;display: block; clear:both; margin: 0 auto;"/></p>

<span id="more"></span><!-- more -->

<p>然而，因为5月份Lanka突发洪水和泥石流，出于安全的考虑，我们一度甚至差点放弃了此次的出游。这个顾虑在看到那些正在Lanka的游客们分享的微博照片后，烟消云散。事实证明，这将是一场近乎完美的旅行。</p>

<p>因此，我将尽可能完整地记录下这次旅行的点滴。也许会显得有些啰嗦。然而，我希望自己日后每一次阅读这篇游记，都将被带回到那段时光，重温美好的锡兰之旅。</p>

<h2 id="toc_0">06.05 出发！</h2>

<p>早上十点，我们便来到了首都机场T3航站楼。途径香港转机，抵达斯里兰卡的首都科伦坡时已是当地时间夜里11点。Lanka和北京有着两个半小时的时差，此时已是北京的凌晨一点半。然而我却没有丝毫的困意，肆意地感受着热带国家特有的燥热，还有海风中扑面而来的腥味。到住的地方还有一个多小时的车程。此时的车窗外黑乎乎的一片，并没有多少路灯。像极了上个世纪中国的农村。</p>

<p>抵达酒店已是当地的深夜一点。困极。偏偏这是一个花园式的酒店，房间稀稀拉拉地分散在诺大的园林中。绕了许久才找到我们的房间。</p>

<p>至此，结束了第一天的旅程。</p>

<h2 id="toc_1">06.06 大象孤儿院&amp;狮子岩</h2>

<p>也许是因为时差，也许是因为激动，第一晚我一直睡的不沉。早上五点依稀听到了不知名的鸟叫，还有滂渤的雨声。不禁为今天的行程捏了把汗。然而早上七点醒来，已是艳阳高照（事实上，我们在这次的旅途中不时遇到这种奇观：前一秒还是倾盆大雨，后一秒便是晴空万里）。</p>

<p>早餐是酒店的自助。品种倒也齐全，热带的水果尤其丰盛。然而，驱之不散的苍蝇实在是大倒胃口（在接下来几天的行程中，几乎每顿饭上总能看到苍蝇的身影）。</p>

<p>我们的第一站是位于宾纳瓦拉的大象孤儿院，距离首都科伦坡差不多有3个小时的车程。我们到那已是中午了。便决定先去附近的饭店填饱自己的肚子。午餐的地方紧邻着一片小溪，也是象群洗澡的地方。我们在阁楼上吃着当地的“美味佳肴”（主要是以咖喱和香料烹饪的蔬菜和鸡肉），看着阁楼下的“香艳风光”，颇为惬意。</p>

<ul>
<li><p>溪流上的风光<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902634254464.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902634384053.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>洗澡的象群<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635094648.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635174692.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>一只离群的贪食小象<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635455827.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>吃完饭，我们步行来到不远处的大象孤儿院。所谓孤儿院，原本是为了保护那些无家可归的幼象，给它们一片生存的空间。其实不过是在丛林中划出了一片空地，把这些孤独的象群圈养起来。游客向管理员支付一笔小费，便可以和大象合影留恋，甚至骑在大象的背上。在我看来，这不过又是一个开放式的公园罢了。</p>

<p>大象孤儿院很小，差不多十几分钟便可以游览完。</p>

<ul>
<li>孤独的象群
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635758267.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635902243.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902635987967.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>参观完象群，我们便发车赶往下一景点——被誉为世界第八大奇迹的狮子岩。这是一块建立在巨大岩石上的空中宫殿，是由摩利耶王朝的国王卡西雅伯所建造的。卡西雅伯国王弑父登基后，为了逃避其同父异母的弟弟的追杀，在此地（丹布拉）建立了一座易守难攻的空中碉堡。</p>

<ul>
<li><p>山脚下的猴子<br/>
在斯里兰卡，除了被誉为“神鸟”的乌鸦，我们见到最多的就是生活在山上和丛林里的猴子们了。这里的猴子并不畏生，却也不像峨眉山的猴子那样肆无忌惮。他们跟随着人群，盯着人们手里的食物，却矜持地保持着高傲的姿态，并不向人类发出主动的乞求。<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902636237939.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902636394421.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902636519172.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>远处的狮子岩<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902636869340.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>开始攀登！<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902637118504.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>悬空的栈道<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902637235186.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>回首平原上的风景<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902637696952.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>登顶！<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902638323389.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>君临天下<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902638553860.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902638629004.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>岩顶的皇家泳池（据说里面蓄的是雨水？）<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902638895086.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>身着汉服的女子<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639013870.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>在登顶的过程中发生了一段不愉快的小插曲。还在山下的时候，导游就跟我们说，如果有当地人上来要搀扶你们，一定不要答应。那是要小费的。然而，老婆还是傻傻地被生硬地拽上了山。短短的一截路，被索取了500 rps。纯洁的斯里兰卡也因为这些人而蒙上了一层不洁：当他们微笑着向你伸出援助之手的时候，眼睛却在死死盯着你的钱包。</p>

<p>（也因为这件小插曲，我在岩顶上大为火光。既没有心情欣赏美丽的岩顶风光，自然也没有拍出几张让人满意的照片。面对这个世界第八大奇迹，心里却在大生闷气，实在是人生一大憾事！）</p>

<h2 id="toc_2">06.07 佛教文化之旅：丹不勒石窟寺&amp;佛牙寺</h2>

<p>今天游览的主题是位于丹不勒的石窟寺和康提湖畔的佛牙寺。</p>

<p>斯里兰卡是一个佛教国家，马路两边随处可见用于供奉的佛像。僧人在这里也受到很高的礼遇。我们今天要游览的这两个地方，也是当地信徒们的朝圣中心。</p>

<p>吃完早饭，我们乘车来到丹不勒石窟寺。首先映入眼前的，是一尊巨大的金色塑身佛像。</p>

<ul>
<li>位于正门的金色大佛
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639169002.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639237742.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>石窟寺建于一座巨石山上，其实只是一个小山丘。与狮子岩的高度不可并论。不过在攀爬的过程中，也少不了有当地的“原住民”——猴儿和流浪狗的热情陪伴。</p>

<ul>
<li><p>寺里的猴子<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639349848.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639407731.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>近在咫尺的小山峰<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639574765.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>石窟里的卧佛（位于第一石窟“天王窟”，长达14米，是石窟寺里最大的卧佛）<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639681671.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>山外的水（像不像一座海岛？）<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639795865.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>下一站是佛牙寺。途径一座宝石店。斯里兰卡的蓝宝石很有名气。团里的大妈们顿时开启了疯狂的购物模式。</p>

<p>此时，恰赶上雨过天晴。一弯彩虹浮现在空中，美不胜收。屋里的大妈们忙着对着宝石比价砍价，我站在屋外欣赏对面的风景。谁的收获更多呢？</p>

<ul>
<li>窗外的彩虹
<img src="http://on8zjjnhp.bkt.clouddn.com/14902639911958.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>在车上干等了一个多小时，志得意满的大妈们才恋恋不舍地走出了宝石店，一路上还互相攀比着自己买到的宝石有多么便宜。中国大妈所到之处，果然是寸草不生。</p>

<p>终于抵达康提湖畔。</p>

<ul>
<li>康提湖畔
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640037320.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640095122.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>湖畔边上，便是著名的佛牙寺了。这里供奉着斯里兰卡的国宝——佛祖释迦牟尼的牙舍利。</p>

<ul>
<li><p>草坪上的乌鸦<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640197653.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>寺前的小沙弥<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640306395.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>每日早中晚，这里都会举行盛大的敬拜仪式。远来的信徒排队依次走过存放有佛牙的金塔（只能远远的瞻仰）。各地的僧众也会拿出早已准备好供奉给佛祖的辎重，等候在另一边的通道。每年七八月间还有盛大的佛牙节，对斯里兰卡的老百姓而言，这更是一个堪比新年的节日。然而真正的佛牙，据说每五年才会拿出来展示在众人面前。</p>

<ul>
<li><p>虔诚的信徒<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640435431.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>佛灯<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640533954.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>游览完佛牙寺，今天的行程便结束了。回到酒店住下。明天还有六个小时的车程在等着呢。</p>

<h2 id="toc_3">06.08 晚安，印度洋</h2>

<p>今天的行程比较简单。先去游览康提市内的皇家植物园，之后便是近六个小时的车程——前往南部的海滨。</p>

<p>对于到处都是绿植的热带国家而言，特地圈出一块地作为皇家的园林，似乎略显多余。然而，园林里笔直的棕榈大道，铺天盖地的蝙蝠，以及各种奇形怪状或高大或矮小的不知名树木，还是给我留下了深刻的印象。</p>

<ul>
<li><p>耸入云天的棕榈大道<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640667749.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902640817029.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>一花一世界<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641172776.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>漫天飞舞的蝙蝠（为了拍摄这一张照片，一个斯里兰卡小伙卖力地挥舞着树枝，一边拍打着地面，嘴里一边发出呼嚯的怪叫——惊起了树林中大片休憩的蝙蝠。在我还没来得及向他表示感谢时，他已经“友好地”向我伸出了手——一双索要Money和香烟的手）<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641276699.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>天地间一棵孤零零的老树<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641361885.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>离开皇家植物园，我们便开始了长达六个小时的“迁徙”——前往南部海滨！</p>

<ul>
<li>等车的斯里兰卡女孩
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641477485.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>抵达海滨已是晚上7点。虽然黑暗中并看不清波涛汹涌的大海，但光是听到那咆哮的海浪声，便足以让第一次见到印度洋的我兴奋不已。</p>

<p>明天要早起看日出！晚安了，印度洋！</p>

<h2 id="toc_4">06.09 红树林，加勒古堡，高跷渔夫</h2>

<p>早上五点半的闹钟一响，我便挣扎着从床上爬起来。拉开窗帘——我们住的是一个沿海的海景房——只见一道红色的霞光浮出海面。一秒钟前的睡意顿时荡然无存。来不及洗漱，简单抹了把脸，便背着相机来到了沙滩上。</p>

<p>此时东方既白。几个年纪稍大一点的游客已经早早地占据了日出最佳的拍摄位置。然而，太阳却跟我们开了一个大大的玩笑——她躲在了一朵大大的云层后面。</p>

<p>于是只好沿着沙滩踏浪而行，感受那来自印度洋不安分的咆哮。</p>

<ul>
<li><p>含羞的朝阳<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641588467.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>晨曦的沙滩<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641664593.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>咆哮的印度洋<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641784619.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>沙滩上随处可见被当地人奉为神鸟的乌鸦。这些乌鸦和当地的渔民相依为命，大海是他们共同的衣食父母。</p>

<ul>
<li>被誉为“神鸟”的乌鸦
<img src="http://on8zjjnhp.bkt.clouddn.com/14902641904467.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642006712.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>当地的渔民更是早已守候在沙滩上——或是等待出海，或是在海边垂钓。</p>

<ul>
<li><p>海边垂钓的渔民<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642121500.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>出海的渔民<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642223169.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>早晨最大的收获，当属这只小小的寄居蟹了。小心翼翼地将它拾起，却见它伸出柔弱的小爪子，笨拙而又无力地拨弄着你的指尖。瞪着一双无辜的大眼睛，憨态可掬。</p>

<ul>
<li>小小的寄居蟹
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642325879.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<h3 id="toc_5">红树林</h3>

<p>今天行程的第一站是红树林。红树的名字源自其树干的颜色。其实是一片生长在入海口的水上灌木丛。水域极宽，像极了柬埔寨的洞里萨湖。然而没有水上人家，只有若干岛屿，散落在湖面上。</p>

<p>也因此，红树林没有给我留下太深的印象。只记得那矮小的要低着头才能通过的桥洞；那枝繁叶茂盘根交错的红树搭成的树洞；那湖面上抱着猴子索要小费的卖艺者；那以钓鱼养虾为生勤劳的渔民；那枝头上不知名的水鸟和岸边匆匆而过的蜥蜴。自然还有在岛上过着原生态生活的当地居民。他们以向游客兜售新鲜的桂皮和香料为生。</p>

<p>不过岛上的环境极差。桌上的餐具爬满了红褐色的大蚂蚁，屋外的窗户下甚至还可以看到五颜六色的大蜘蛛。难以想象，真的会有人在这样的环境下生存。</p>

<ul>
<li><p>红树林<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642419726.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642482584.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>水面上的居民<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642580023.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642645050.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642704894.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>岛上的“桂皮”女孩<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642820726.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<h3 id="toc_6">加勒古堡</h3>

<p>离开红树林，便赶往下一站——加勒古堡。途中休息吃饭的时候，随行的大妈们又发现了一家珠宝店，旁若无人的闲逛了起来。我便四处走走。这里恰好沿海，无意中发现一片有趣的海上走廊。后来才知道，这里和加勒古堡已经很近了。</p>

<ul>
<li>无意中发现的海上走廊
<img src="http://on8zjjnhp.bkt.clouddn.com/14902642951485.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643003958.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>加勒古堡是殖民时期的荷兰人所建。欧式风格浓厚。然而我对大海的兴趣要远超出对建筑的兴趣，便只是沿着海岸线走了一圈，没有深入到古堡小镇里去探索。</p>

<ul>
<li>古堡一瞥
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643104946.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643152728.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643202689.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643265918.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<h3 id="toc_7">高跷渔夫</h3>

<p>出发之前便听说了这里的高跷渔夫颇为有名。不过，如今的高跷渔夫已不再是以打鱼为生。因为他们发现，向前来观赏的游客索取拍照的小费，要比钓鱼容易多了。</p>

<p>然而不得不说，这里的海鸥很美。</p>

<ul>
<li><p>风起云涌<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643369084.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>海边的高跷渔夫（不得不说，这是当时唯一一个坚守岗位有责任心的好渔夫）<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643470296.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643518208.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
<li><p>空中的海鸥<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643625782.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643683753.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643731585.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p></li>
</ul>

<p>告别高跷渔夫和海鸥，五点多钟便回到了酒店。本想再去沙滩上走走，却被同行的大叔拉去了外面空转了一圈，悻悻而归。</p>

<p>晚饭自理。酒店倒是提供新鲜的海鲜大餐。然而前两天的自助已然把我的肠胃吃坏了，不敢再乱吃。便点了一份炒面和啤酒。感觉是这些天吃到的最美味的大餐了！</p>

<p>今天没有看到日出，明天还要早起！</p>

<h2 id="toc_8">06.10 海上小火车</h2>

<p>五点半挣扎着从床上爬起。然而今天甚至下起了暴雨！太阳依然躲在了厚厚的云层下。其实因为大海在酒店的南面，即便是晴天，也无法观赏到海上的日出或是日落。</p>

<p>还是去沙滩上抓几只寄居蟹吧！</p>

<p>今天的行程安排是回到斯里兰卡的首都科伦坡，有两条线路可以选择：大巴车和小火车。我们选择了后者。事实证明，这是一条正确的选择：一路上看到了不一样的风景，也遇到了不一样的人。</p>

<ul>
<li>斯里兰卡的小火车
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643849806.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643899674.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902643946048.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902644011487.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902644065586.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902644124947.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902644172491.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902644229410.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></li>
</ul>

<p>抵达科伦坡后，便是随车观光游览。比如小白宫，大使馆等地。我对这些现代的建筑没什么兴趣，大都是留在车上休息。</p>

<p>晚上回到机场，准备登机回国。</p>

<h2 id="toc_9">06.11 再见！斯里兰卡</h2>

<p>七天的时间一晃而过。我把我生命里的七天留在了斯里兰卡；斯里兰卡也在我的一生留下了回忆的影子。平心而论，我喜欢斯里兰卡。这是一个神奇而又梦幻的国家。你喜欢山，她便给你山；你喜欢海，她便给你海；你喜欢古堡，她便给你古堡。有人说，除了雪，上帝和佛陀给了她一切。然而，我不喜欢她的人民。贫穷使得他们失去了为人的尊严。当他微笑着向你提供帮助的时候，眼睛却在盯着你的钱包。</p>

<p>然而，这依然是一个有趣的国家。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AlphaGo原理解读（Mastering the game of Go with deep neural networks and tree search)]]></title>
    <link href="whiskytina.github.io/14902595540843.html"/>
    <updated>2017-03-23T16:59:14+08:00</updated>
    <id>whiskytina.github.io/14902595540843.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">背景：完全信息博弈与MCTS算法</h2>

<p>要完全弄清AlphaGo背后的原理，首先需要了解一下AI在博弈游戏中常用到的蒙特卡洛树搜索算法——MCTS。</p>

<p>在一个完全信息下的博弈游戏中，如果所有参与者都采取最优策略，那么对于游戏中的任意一个局面\(s\)，总有一个确定性的估值函数\(v^*(s)\)可以直接计算出最终的博弈结果。理论上，我们可以通过构建一棵博弈树，递归地求解出\(v^*(s)\)。这就是<a href="https://en.wikipedia.org/wiki/Minimax">Minimax</a>算法。然而在有些问题中，这棵搜索树往往十分巨大（例如在围棋游戏中达到了\(250^{150}\)的搜索空间），以至于穷举的算法并不可行。</p>

<span id="more"></span><!-- more -->

<p>有两种策略可以有效地降低搜索空间的复杂度：1. 通过一个evalutaion function对当前局面进行价值的评估以降低搜索的深度；2. 剪枝以降低搜索的宽度。然而，这些策略都需要引入一些先验的知识。</p>

<p>于是，人们提出了蒙特卡洛树搜索（MCTS）算法。MCTS是一类通用博弈算法——理论上，它不需要任何有关博弈的先验知识。</p>

<p>想象一下，你站在一堆老虎机面前，每一台老虎机的reward都服从一个随机的概率分布。然而，一开始，你对这些概率分布一无所知。你的目标是寻找一种玩老虎机的策略，使得在整个游戏的过程中你能获得尽可能多的reward。很明显，你的策略需要能够在尝试尽可能多的老虎机（explore）与选择已知回报最多的老虎机（exploit）之间寻求一种平衡。</p>

<p>一种叫做<em>UCB1</em>的策略可以满足这种需求。该策略为每台老虎机构造了一个关于reward的置信区间：<br/>
\[x_i\pm\sqrt{\frac{2\ln{n}}{n_i}}\]<br/>
其中，\(x_i\)是对第\(i\)台老虎机统计出来的平均回报；\(n\)是试验的总次数；\(n_i\)是在第\(i\)台老虎机上试验的次数。你要做的，就是在每一轮试验中，选择置信上限最大对应的那台老虎机。显然，这个策略平衡了explore与exploit。你的每一次试验，都会使被选中的那台老虎机的置信区间变窄，而使其他未被选中的老虎机的置信区别变宽——变相提升了这些老虎机在下一轮试验中被选中的概率。</p>

<p>蒙特卡洛树搜索（MCTS）就是在UCB1基础上发展出来的一种解决多轮序贯博弈问题的策略。它包含四个步骤：<br/>
1. Selection。从根节点状态出发，迭代地使用UCB1算法选择最优策略，直到碰到一个叶子节点。叶子节点是搜索树中存在至少一个子节点从未被访问过的状态节点。<br/>
2. Expansion。对叶子节点进行扩展。选择其一个从未访问过的子节点加入当前的搜索树。<br/>
3. Simulation。从2中的新节点出发，进行Monto Carlo模拟，直到博弈结束。<br/>
4. Back-propagation。更新博弈树中所有节点的状态。进入下一轮的选择和模拟。</p>

<p>可以看出，通过Selection步骤，MCTS算法降低了搜索的宽度；而通过Simulation步骤，MCTS算法又进一步降低了搜索的深度。因此，MCTS算法是一类极为高效地解决复杂博弈问题的搜索策略。</p>

<p>关于MCTS算法更多详细的介绍，可参见博客：<a href="http://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/">Introduction to Monte Carlo Tree Search</a></p>

<h2 id="toc_1">AlphaGo的基本原理</h2>

<p>围棋是一类完全信息的博弈游戏。然而，其庞大的搜索空间，以及局面棋势的复杂度，使得传统的剪枝搜索算法在围棋面前都望而却步。在AlphaGo出现之前，MCTS算法算是一类比较有效的算法。它通过重复性地模拟两个players的对弈结果，给出对局面\(s\)的一个估值\(v(s)\)（Monte Carlo rollouts）；并选择估值最高的子节点作为当前的策略（policy）。基于MCTS的围棋博弈程序已经达到了业余爱好者的水平。</p>

<p>然而，传统的MCTS算法的局限性在于，它的估值函数或是策略函数都是一些局面特征的浅层组合，往往很难对一个棋局有一个较为精准的判断。为此，AlphaGo的作者训练了两个卷积神经网络来帮助MCTS算法制定策略：用于评估局面的value network，和用于决策的policy network。（后面会看到，这两个网络的主要区别是在输出层：前者是一个标量；后者则对应着棋盘上的一个概率分布。）</p>

<p>首先，Huang等人利用人类之间的博弈数据训练了两个有监督学习的policy network：\(p_\sigma\)（SL policy network）和\(p_\pi\)（fast rollout policy network）。后者用于在MCTS的rollouts中快速地选择策略。接下来，他们在\(p_\sigma\)的基础上通过自我对弈训练了一个强化学习版本的policy network：\(p_\rho\)（RL policy network）。与用于预测人类行为的\(p_\sigma\)不同，\(p_\rho\)的训练目标被设定为最大化博弈收益（即赢棋）所对应的策略。最后，在自我对弈生成的数据集上，Huang等人又训练了一个value network：\(v_\theta\)，用于对当前棋局的赢家做一个快速的预估。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902596804531.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>因此，用一句话简单概括一下AlphaGo的基本原理：在MCTS的框架下引入两个卷积神经网络policy network和value network以改进纯随机的Monte Carlo模拟，并借助supervised learning和reinforcement learning训练这两个网络。</p>

<p>接下来将对AlphaGo的细节进行展开讨论。</p>

<h3 id="toc_2">有监督学习的Policy Networks</h3>

<p>Huang等人首先训练了一个有监督的Policy Network用来模拟人类专家的走子。SL policy network是一个卷积神经网络；其输出层是一个Softmax分类器，用来计算在给定的棋面状态\(s\)下每一个位置的落子概率\(p_\sigma(a|s)\)。对一个棋面状态\(s\)的描述如下：<br/>
<img src="http://on8zjjnhp.bkt.clouddn.com/14902599201040.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/><br/>
（这里的Features对应着卷积神经网络里的Channels。）</p>

<p>经过人类高手三千万步围棋走法的训练后，SL policy network模拟人类落子的准确率已经达到了57%；相应地，网络的棋力也得到大大的提升。但是，如果直接用这个网络与人类高手，甚至是MCTS的博弈程序进行对弈，依然是输面居多。而且，这个网络的走子太慢了！平均每步\(3ms\)的响应时间，使得这个网络很难被直接用于MCTS的rollout中进行策略的随机。因此，Huang等人通过提取一些pattern features又训练了一个更快速（响应时间达到了\(2\mu s\)）但准确率有所降低（24.2%）的rollout policy network： \(p_\pi\)。</p>

<h3 id="toc_3">强化学习的Policy Networks</h3>

<p>接下来，为了进一步提高policy network的对弈能力，Huang等人又采用一种policy gradient reinforcement learning的技术，训练了一个RL policy network：\(p_\rho\)。这个网络的结构与SL policy network的网络结构相同，依然是一个输出为给定状态下落子概率的卷积神经网络。网络的参数被初始化为\(p_\sigma\)的参数；接下来，通过不断地自我对弈（与历史版本），网络的权重向着收益最大化的方向进化。此时，网络的学习目标不再是模拟人类的走法，而是更为终极的目标：赢棋。</p>

<p>具体来说，我们定义了一个reward function \(r(s_t)\)：对于非终止的时间步\(t&lt;T\)，总有\(r(s_t)=0\)。每一步的收益\(z(t)\)被定义为\(\pm r(s_T)\)：即对当前玩家而言对弈的最终结果（\(+1\)代表赢棋；\(-1\)代表输棋）。网络的权重通过随机梯度上升法进行调整：<br/>
\[\Delta\rho\propto\frac{\partial\log{p_\rho(a_t|s_t)}}{\partial\rho}z_t\]</p>

<p>通过这种方式训练出来的RL policy network，在与SL policy network对弈时已有80%的赢面。即便是与依赖Monte Carlo搜索的围棋博弈程序相比，不依赖任何搜索的RL policy network，也已经达到了85%的赢面。</p>

<h3 id="toc_4">强化学习的Value Networks</h3>

<p>最后，Huang等人又开始寻求一个能快速预估棋面价值（棋势）的Value Network。一个棋面的价值函数\(v^p(s)\)，被定义为在给定的一组对弈策略\(p\)的情况下，从状态\(s\)出发，最终的期望收益（也即赢棋的概率）：<br/>
\[v^p(s)=E[z_t|s_t=s,a_{t...T}\in p]\]</p>

<p>显然，理想情况下，我们想知道的是在双方均采用最优策略的条件下得到的最优期望收益\(v^*(s)\)。然而，我们并不知道什么才是最优的策略。因此，在实际应用中，Huang等人采用了目前最强的策略函数\(p_\rho\)（RL policy network ）来计算一个棋面的价值\(v^{p_\rho}(s)\)，并训练了一个value network \(v_\theta(s)\)来拟合这个价值函数：\(v_\theta(s) \approx v^{p_\rho}(s) \approx v^*(s)\)。</p>

<p>Value Network的网络结构与前面的Policy Network类似，也是一个卷积神经网络，只是输出层变成了一个单神经元的标量。我们可以通过构造一组\((s,z)\)的训练数据，并用随机梯度下降法最小化网络的输出\(v_\theta(s)\)与目标收益\(z\)的均方差，来调整网络的参数：<br/>
\[\Delta\theta\propto\frac{\partial{v_\theta(s)}}{\partial\theta}(z-v_\theta(s))\]</p>

<p>在构造训练数据时有一些技巧。如果我们从人类对弈的完整棋局中抽取足够数量的训练数据，很容易出现过拟合的问题。这是因为，在同一轮棋局中的两个棋面的相关性很强（往往只相差几个棋子）；此时，网络很容易记住这些棋面的最终结果，而对新棋面的泛化能力很弱。为了解决这个问题，Huang等人再次祭出强化学习的大杀器：通过RL policy network的自我对弈，产生了三千万个从不同棋局中提取出来的棋面－收益组合的训练数据。基于这份数据训练出来的Value Network，在对人类对弈结果的预测中，已经远远超过了使用fast rollout policy network的MCTS的准确率；即便是与使用RL policy network的MCTS相比，也已是不遑多让（而Value Network的计算效率更高）。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902600204991.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<h3 id="toc_5">整合</h3>

<p>到这里，我们手头上已经有一个牛逼但是巨慢的SL policy network；有一个不那么牛逼但是很快的fast policy network；有一个一心只想着如何赢棋的RL policy network；还有一个能一眼洞穿棋局的value network。那么，将这些networks放在一起互相补足，会得到什么呢？</p>

<p>答案就是AlphaGo。而把这些networks整合在一起的框架，就是MCTS算法。</p>

<p>与经典的MCTS算法类似，APV-MCTS（asynchronous policy and value MCTS）的每一轮模拟也包含四个步骤：<br/>
1. Selection：APV-MCTS搜索树中的每条连边\((s,a)\)都包含三个状态：决策收益\(Q(s,a)\)，访问次数\(N(s,a)\)，和一个先验概率\(P(s,a)\)。这三个状态共同决定了对一个节点下行为的选择：<br/>
\[a_t=\arg\max_a{(Q(s_t,a)+u(s_t,a))}\]<br/>
其中，\(u(s,a)\propto\frac{P(s,a)}{1+N(s,a)}\)<br/>
2. Expansion：步骤1中的selection终止于叶子节点。此时，要对叶子节点进行扩展。这里采用SL policy network \(p_\sigma\)计算出叶子节点上每个行为的概率，并作为先验概率\(P(s_L,a)\)存储下来。<br/>
3. Evaluation。使用value network \(v_\theta(s)\)和fast rollout policy network \(p_\pi\)模拟得到的博弈结果对当前访问到的叶子节点进行估值：\[V(s_L)=(1-\lambda)v_\theta(s_L)+\lambda z_L\]<br/>
4. Backup。更新这一轮模拟中所有访问到的路径的状态：<br/>
\[N(s,a)=\sum_{i=1}^n{1(s,a,i)}\]<br/>
\[Q(s,a)=\frac{1}{N(s,a)}\sum_{i=1}^n{1(s,a,i)V(s_L^i)}\]<br/>
其中，\(n\)是模拟的总次数；\(1(s,a,i)\)标示第\(i\)轮模拟中是否经过边\((s,a)\)；\(s_L^i\)是第\(i\)轮模拟中访问到的叶子节点。</p>

<p>下图展示了一轮模拟的动态过程。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902600515196.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>模拟结束后，算法会选择访问次数\(N(s,a)\)最大的策略\(a\)作为当前的走子策略。</p>

<p>值得注意的是，在整个模拟的过程中，我们见到了SL policy network（用于Expansion中先验概率的计算）；见到了fast rollout policy network（用于Evaluation中的快速走子）；见到了value network（用于Evaluation中对棋势的预估）。等等，RL policy network去哪了？为什么不用RL policy network替代SL policy network？明明RL policy network有着更强的棋力啊（85%的赢面）？</p>

<p>这是因为，与RL policy network相比，由人类专家走法训练出来的SL policy network在策略上的多样性更强；因此更适用于MCTS中的搜索。但是，用RL policy network的自我对弈结果训练出来的value network的泛化能力就要比SL policy network训练出来的value network要强得多了。</p>

<h2 id="toc_6">结语</h2>

<p>回顾一下，我们发现AlphaGo本质上是CNN、RL、MCTS三者相结合的产物。其中，MCTS是AlphaGo的骨骼，支撑起了整个算法的框架；CNN是AlphaGo的眼睛和大脑，在复杂的棋局面前寻找尽可能优的策略；RL是AlphaGo的血液，源源不断地提供新鲜的训练数据。三者相辅相成，最终4:1战胜了人类围棋世界冠军李世石。其实还有很多细节我没有详细的展开，包括如何在分布式的机器上更高效地训练；如何更新MCTS中的权重等等。然而，其背后的基本原理差不多就是这些了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PRML读书笔记（一）：概率论，决策论，与信息论]]></title>
    <link href="whiskytina.github.io/14902527326377.html"/>
    <updated>2017-03-23T15:05:32+08:00</updated>
    <id>whiskytina.github.io/14902527326377.html</id>
    <content type="html"><![CDATA[
<p>什么是模式识别（Pattern Recognition）？</p>

<p>按照Bishop的定义，模式识别就是用机器学习的算法从数据中挖掘出有用的pattern。</p>

<p>人们很早就开始学习如何从大量的数据中发现隐藏在背后的pattern。例如，16世纪的Kepler从他的老师Tycho搜集的大量有关于行星运动的数据中发现了天体运行的规律，并直接导致了牛顿经典力学的诞生。然而，这种依赖于人类经验的、启发式的模式识别过程很难复制到其他的领域中。例如手写数字的识别。这就需要机器学习的技术了。（顺便提一下，开普勒定律在物理学中只是一种唯象的理论，它只对物理事实抽象出概括性的描述，而没有解释内在的原因，也就是牛顿的万有引力定律（其实在更高的层次上，万有引力也是一种唯象的理论，它的解释由量子引力理论给出）。这也意味着，在大数据的机器学习时代，我们挖掘出来的知识更多的也只是一种相关性的规律。）</p>

<span id="more"></span><!-- more -->

<p>在手写数字识别的问题中，一部分手写图片作为<strong>训练集（training set）</strong>，被用来学习一个事先给定的模型的参数。另一部分图片作为<strong>测试集（test set）</strong>，被用来评估学习到的模型在新数据上的<strong>泛化能力（generalization）</strong>。训练集与测试集里的手写图片对应的数字都是已知的。这些数字又被称之为<strong>目标向量（target vector）</strong>，是模型学习的分类目标。一个机器学习算法最终得到的结果，就是一个将输入变量\(x\)（这里的手写数字图片）映射到目标向量空间里的函数\(y(x)\)。</p>

<p>一般来说，在模型训练之前，都有一个<strong>数据预处理（preprocess）</strong>的过程。例如，在识别手写数字的问题中，我们需要先将训练集里的图片变换到同一个尺寸下，从而使模型的训练更加统一。这个过程也被称之为<strong>特征提取（feature extraction）</strong>。同时，预处理后的特征往往也会使模型的计算更加高效。要注意的是，如果我们对训练集里的数据进行了预处理，那么对于测试集里的数据，我们也要进行同样的操作。</p>

<p>对于那些训练样本中带有目标向量的机器学习问题，我们称之为<strong>有监督学习（supervised learning）</strong>。例如前面提到的手写数字识别问题。进一步地，如果目标向量是离散的，我们称之为<strong>分类（classification）</strong>问题；反之，如果目标向量是连续的，我们称之为<strong>回归（regression）</strong>问题。</p>

<p>另一类机器学习问题的训练样本仅有输入变量的特征，我们称之为<strong>无监督学习</strong>。例如对数据中相似样本的归并（聚类）问题。</p>

<p>最后一类机器学习问题被称之为<strong>强化学习（reinforcement learning）</strong>。它是寻求在一个给定状态下的行为决策，以最大化最终的收益。象棋AI就是一个典型的强化学习的例子。与有监督学习不同，强化学习要解决的问题没有标记好的输出；但往往会通过与环境的互动来改变自己的状态和收益，并因此而学习到什么是一个好的输出。注意，强化学习的每一步决策不仅会影响到当下的收益，还会影响未来的回报。这就涉及到一个<strong>信用分配（credit assignment）</strong>的问题：即如何将最终的回报分配到导致该回报的每一步的决策上？强化学习另一个特征是在<strong>exploration</strong>和<strong>exploitation</strong>之间的trade－off：对任意一个的过度偏向都会产生较差的结果。</p>

<p>接下来，我们将从一个具体的例子入手展现机器学习过程里的一些基本概念；然后，我们将简单介绍机器学习理论的三大基础：<strong>概率论（probability theory）</strong>，<strong>决策论（decision theory）</strong>，和<strong>信息论（information theory）</strong>。</p>

<h2 id="toc_0">一个例子：多项式拟合</h2>

<p>作为例子，我们通过一个带随机噪声的产生函数\(sin(2{\pi}x)\)来构造训练数据，并期望一个机器学习算法能从有限的数据中学习到这个正弦函数。这种数据的构造方式暗合真实世界的法则：即数据在遵循一条潜在的规律同时也充满了不确定性。<em>（后面我们会看到，我们是如何在一个概率论的框架下描述这种不确定性，同时给出一个最好的预测。）</em></p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902528277140.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>我们可以用一个多项式函数来拟合训练数据（这是因为任意一个连续可导的函数总可以通过多项式展开来逼近）：\[y(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^{M}{w_jx^j}\]</p>

<p>注意，尽管这个函数是关于\(x\)的非线性函数，但却是关于系数\(w\)的线性函数。类似多项式函数这种相对于未知参数是线性的模型，具有一些重要的性质，我们称之为<strong>线性模型（linear model）</strong>。</p>

<p>我们通过最小化一个<strong>误差函数（error function）</strong>来求解模型的参数。误差函数是关于参数\(w\)的一个函数；它衡量我们的预测值\(y(x,w)\)与真实值\(t\)的差距。一个常用的误差函数是误差平方和，它的基本形式如下：\[E(w)=\frac{1}{2}\sum_{n=1}^{N}{\{y(x_n,w)-t_n\}^2}\]<br/>
（<em>在后面介绍到最大似然法时，我们会看到误差函数为什么会取这种形式</em>）</p>

<p>令这个函数对\(w\)的一阶偏导等于0，得到\(M＋1\)个一阶方程组，就可以求解出最优拟合参数\(w^*\)了。</p>

<p>然而，我们还有一个问题：如何确定模型的阶数\(M\)？这也被称之为<strong>模型选择（model comparison or model selection）</strong>问题。我们可以通过评估模型在新数据集上的泛化能力来衡量模型的好坏。具体来说，我们从训练集中抽取一部分数据作为<strong>验证集（validation set）</strong>；对于\(M\)的每一种可能取值，我们先用余下的训练集找到最优的参数\(w^*\)，然后再在验证集上评估这个模型的误差\(E(w^*)\)（或者是root-mean-square error：\(E_{RMS}=\sqrt{{2E(w^*)}/{N}}\)）。我们将选择泛化误差最小的模型作为最终的模型。</p>

<p>通过这种方式，我们发现，\(M\)取值过低，模型有可能会<strong>欠拟合（under-fitting）</strong>；而\(M\)取值过高，模型又有可能会<strong>过拟合（over-fitting）</strong>。</p>

<p>这是为什么呢？</p>

<p>理论上，一个低阶的多项式函数只是更高阶的多项式函数的一个特例；而一个高阶的多项式也更加逼近真实的产生函数\(sin(2{\pi}x)\)。因此，高阶的模型似乎没道理比低阶模型的表现更差。</p>

<p>但是，如果我们仔细观察那些过拟合的高阶模型学习到的参数\(w\)，会发现它们的量纲往往异乎寻常的大。这使得模型虽然可以准确地预估训练集中的数据，但也展现出了巨大的波动性。换句话说，强大的高阶模型过多地学习了训练数据中的噪声。如果我们引入更多的训练数据，这种过拟合的问题会得到一定程度的缓解。</p>

<p>简而言之，虽然模型越复杂，解释能力也越强，但我们往往需要更多的数据来训练一个复杂的模型。<em>（后面会发现，这种过拟合本质上是由于最大似然法这类对模型参数进行点估计的算法所带来的bias造成的。通过一种贝叶斯的方法，我们可以从根本上避免过拟合的问题。）</em></p>

<p>一种在有限的数据集下也能训练一个高阶模型的技术是<strong>正则化（regularization）</strong>，即在传统的误差函数中引入一个对参数\(w\)量纲的惩罚项，以抑制\(w\)的过度发散：\[\tilde{E}(w)=\frac{1}{2}\sum_{n=1}^{N}{\{y(x_n,w)-t_n\}^2}+\frac{\lambda}{2}||w||^2\]<br/>
其中，\(||w||^2=w_1^2+w_2^2+...+w_M^2\)，参数\(\lambda\)控制正则项和误差项的相对权重。对于一个合适的\(\lambda\)取值，我们用较少的数据也可以训练出一个泛化的高阶模型。</p>

<p>然而，过低或过高的\(\lambda\)依然会导致过拟合或欠拟合的问题。本质上，<strong>正则化只是将对模型复杂度的控制由模型的阶数\(M\)转移到了惩罚因子\(\lambda\)上</strong>。我们依然需要一个验证集来优化模型的复杂度。</p>

<h2 id="toc_1">概率论</h2>

<p>在模式识别和机器学习的研究领域里，<strong>不确定性（uncertainty）</strong>是一个非常核心的概念。接下来要介绍的概率论为不确定性的量化提供了一个统一的框架，因此也构成了整个机器学习研究的理论基础。</p>

<h3 id="toc_2">基本知识点</h3>

<p>概率的基本定义由频率学派给出：在一系列试验中某一事件发生的频率称之为该事件发生的概率。从这个定义出发，我们很容易得到概率论的两个基本法则：<strong>sum rule</strong>和<strong>product rule</strong>：<br/>
\[p(X)=\sum_{Y}{p(X,Y)}\]<br/>
\[p(X,Y)=p(Y|X)p(X)\]<br/>
其中，\(p(X)\)对应\(X\)发生的<strong>边际概率（marginal probability）</strong>；\(p(X,Y)\)对应\(X\)和\(Y\)同时发生的<strong>联合概率（joint probability）</strong>；\(p(Y|X)\)对应给定\(X\)下\(Y\)发生的<strong>条件概率（conditional probability）</strong>。这两个公式构成了我们将要用到的所有概率论知识的基础。</p>

<p>由product rule出发，同时结合联合概率的对称性，我们立即得到一个十分重要的公式，<strong>贝叶斯定理（Bayes&#39; theorem）</strong>：<br/>
\[p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}\]</p>

<p>贝叶斯定理给出了<strong>后验概率（posterior probability）</strong>——\(p(Y|X)\)和<strong>先验概率（prior probability）</strong>——\(p(Y)\)之间的关系。</p>

<h3 id="toc_3">贝叶斯概率</h3>

<p>然而，并非所有随机事件的发生都是可重复的。因此，我们很难用频率学派的观点解释诸如明天有雨这类事件发生的概率。对这类事件的不确定性的衡量，就是概率的贝叶斯解释（Bayesian view）。</p>

<p>回到之前的多项式拟合的例子。从频率学派的角度来看，我们将目标变量\(t\)视为一个随机变量似乎更加合理（可以通过固定输入变量\(x\)的值统计\(t\)的频率分布）。而从贝叶斯学派的观点来看，我们可以将模型的参数\(w\)（甚至是整个模型）视为一个随机变量，衡量它们的不确定性（尽管\(w\)是不可重复的）。</p>

<p>具体来说，假设我们知道了\(w\)的先验概率分布\(p(w)\)，那么，通过贝叶斯公式，我们可以将这一先验概率转化为给定观测数据后的后验概率\(p(w|D)\)：<br/>
\[p(w|D)=\frac{p(D|w)p(w)}{p(D)}\]<br/>
从而，我们得到了一个已知数据\(D\)下关于模型参数\(w\)的不确定性的定量表达。</p>

<p>在这个公式里，概率分布\(p(D|w)\)又被称为<strong>似然函数（likilihood function）</strong>，它给出了不同\(w\)下数据集\(D\)发生的概率。从而，贝叶斯定理又可以写成：\[posterior \propto likelihood \times prior\]</p>

<p>无论是在频率学派还是贝叶斯学派的框架下，似然函数都扮演着重要的角色。不同的是，频率学派认为，模型的参数是固定的，观测到的数据则是给定参数下的一个随机事件。因此，他们通过一种叫做<strong>最大似然法（maximum likelihood）</strong>的方法来预估参数：即寻找使观测到的数据集\(D\)发生的概率最大的参数\(w\)（<em>后面我们将会看到最大似然法与前面提到的最小化误差函数的关系</em>）。而贝叶斯学派则认为模型的参数服从一个概率分布，因此，似然函数只是获取后验概率的桥梁。</p>

<p><em>（关于频率学派和贝叶斯学派孰优孰劣的问题，各家各执一言，此处不做讨论。只需知道，PRML这本书更多的是介绍贝叶斯学派的观点，而Andrew在Coursera上的课则是频率学派的经典观点。）</em></p>

<h3 id="toc_4">高斯分布</h3>

<p>关于高斯分布的定义和性质，很多概率论的书上都有介绍，这里不再赘述。本小节主要介绍如何运用频率学派的最大似然法从一堆服从高斯分布的数据中拟合出分布的参数：\(\mu\)与\(\sigma^2\)。</p>

<p>由独立同分布假定，我们的数据集\(\mathbf{x} = (x_1,...,x_D)^T\)在给定参数\(\mu\)和\(\sigma^2\)下的条件概率分布为：<br/>
\[p(\mathbf{x}|\mu,\sigma^2)=\prod_{n=1}^N{\mathcal{N}(x_n|\mu,\sigma^2)}\]<br/>
这就是高斯分布的似然函数。</p>

<p><img src="http://on8zjjnhp.bkt.clouddn.com/14902528500482.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>对这个似然函数取对数（<em>后面我们会看到如何从信息熵的角度来解释这一行为</em>），我们得到：\[\ln{p(\mathbf{x}|\mu,\sigma^2)}=-\frac{1}{2\sigma^2}\sum_{n=1}^N{(x_n-\mu)^2}-\frac{N}{2}\ln{\sigma^2}-\frac{N}{2}\ln{2\pi}\]</p>

<p>将上式分别对\(\mu\)和\(\sigma^2\)求导，并令其等于0，我们得到了\(\mu\)和\(\sigma^2\)的最大似然预估：<br/>
\[\mu_{ML}=\frac{1}{N}\sum_{n=1}^N{x_n}\]<br/>
\[\sigma^2_{ML}=\frac{1}{N}\sum_{n=1}^N{(x_n-\mu_{ML})^2}\]<br/>
等式右边都是关于数据集\(\mathbf{x}\)的函数。</p>

<p>可以证明，通过这种方法得到的参数预估是<strong>有偏（bias）</strong>的：<br/>
\[E[\mu_{ML}]=\mu\]<br/>
\[E[\sigma^2_{ML}]=(\frac{N-1}{N})\sigma^2\]<br/>
也就是说，我们会<strong>低估（underestimate）</strong>真实的分布方差。一般来说，模型的参数越多（复杂度越高），由最大似然法预估出来的参数越不准确。而这种有偏的预估，是导致模型过拟合的根本原因（过度拟合了训练样本，而偏离了真实世界）。不过，注意到随着样本数\(N\)的增长，这种偏差会越来越小。这就解释了，为什么增大训练样本的容量可以一定程度上缓解过拟合的问题。</p>

<h3 id="toc_5">回到多项式拟合</h3>

<p>接下来，我们将回到多项式拟合的例子，看看在统计学的视角下，频率学派和贝叶斯学派各自是如何解决这个问题的。</p>

<p>在多项式拟合的问题中，我们的目标是，<strong>在由\(N\)个输入变量\(\mathbf{x}=(x_1,...,x_N)^T\)及其对应的目标值\(\mathbf{t}=(t_1,...,t_N)^T\)构成的训练集的基础上，对新输入变量\(x\)的目标值\(t\)做出预测。</strong>用概率语言来描述，我们可以将目标变量\(t\)的这种不确定性用一个概率分布来表达。我们假定，<strong>\(t\)服从以\(y(x,w)\)为均值的高斯分布</strong>：<br/>
\[p(t|x,w,\beta)=\mathcal{N}(t|y(x,w),\beta^{-1})\]</p>

<p>有了\(t\)的概率分布，我们就可以用最大似然法在训练集上求解模型的参数（\(w\)和\(\beta\)）:<br/>
\[\ln{p(\mathbf{t}|\mathbf{x},w,\beta)} = -\frac{\beta}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{N}{2}\ln{\beta}-\frac{N}{2}\ln{2\pi}\]<br/>
我们发现，略去与\(w\)无关的后两项后，我们得到了前文提到的误差函数！也就是说，对于\(w\)的求解而言，<strong>最大似然法等价于最小化误差平方和！我们的误差函数是在目标变量服从高斯分布的假定下用最大似然法推导出来的一个自然结果！（注意这一结论与\(y(x,w)\)的具体形式无关）</strong></p>

<p>一旦得到了\(w\)和\(\beta\)的最大似然估计，我们就可以对新变量\(x\)的目标值做出预估。注意，这里是对\(t\)的分布预估（predictive distribution），而非点估计（point estimate）。<em>（在后面的decision theory章节中会介绍如何从预估的分布得到预测值）</em></p>

<p>以上是频率学派对多项式拟合算法的解释。</p>

<p>而在贝叶斯学派看来，参数\(w\)也是一个随机变量。假定\(w\)的先验分布是一个服从均值为0的多元高斯分布：<br/>
\[p(w|\alpha)=\mathcal{N}(w|0,\alpha^{-1}I)\]<br/>
其中，\(\alpha\)被称为模型的<strong>超验参数（hyperparameters）</strong>。<br/>
则由贝叶斯定理，\(w\)在给定训练集上的后验分布为：<br/>
\[p(w|\mathbf{x},\mathbf{t},\alpha,\beta) \propto p(\mathbf{t}|\mathbf{x},w,\alpha,\beta) \times p(w|\alpha)\]</p>

<p>我们可以通过最大化这个后验概率来找出最合适的\(w\)。这个方法被称之为<strong>最大后验概率法（maximum posterior，简称MAP）</strong>。对上式取对数，并代入似然函数和先验概率的分布函数后，我们发现，最大后验概率等价于最小化带正则项的平方误差函数：<br/>
\[\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{\alpha}{2\beta}w^Tw\]<br/>
（这句话也可以这么理解：通过引入均值为0的高斯分布先验函数，我们对\(w\)的大小进行了限定。）</p>

<p>也就是说，<strong>带正则项的误差函数，是在目标变量、模型参数均服从高斯分布的假定下，用最大后验概率法推导出来的一个自然结果！</strong>同样的，这个结论与\(y(x,w)\)的具体形式无关。</p>

<p>不过，虽然我们引入了\(w\)的先验概率分布，MAP依然只是对\(w\)的点估计（类似最大似然法，这种方法最大的问题在于给出的估计值往往是有偏的，即\(E[w_{MAP}] \neq w\)）。而贝叶斯学派的精髓在于，从\(w\)的后验概率分布出发，我们可以进一步得到在给定训练集\(\mathbf{x}\)和\(\mathbf{t}\)的条件下，新变量\(x\)的目标值\(t\)的后验概率分布\(p(t|x,\mathbf{x},\mathbf{t})\)：<br/>
\[p(t|x,\mathbf{x},\mathbf{t})=\int{p(t|x,w)p(w|\mathbf{x},\mathbf{t})dw}\]<br/>
（假定\(\alpha\)和\(\beta\)都是模型的超验参数）</p>

<p>总结一下频率学派和贝叶斯学派的区别：</p>

<table>
<thead>
<tr>
<th>----</th>
<th>频率学派</th>
<th>贝叶斯学派</th>
</tr>
</thead>

<tbody>
<tr>
<td>目标变量\(t\)</td>
<td>\(p(t\|x,w)\)</td>
<td>\(p(t\|x,\mathbf{x},\mathbf{t})\)</td>
</tr>
<tr>
<td>模型参数\(w\)</td>
<td>显式；常量</td>
<td>隐式；随机变量</td>
</tr>
<tr>
<td>模型优化算法</td>
<td>最大似然法</td>
<td>最大后验概率法 <sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></td>
</tr>
<tr>
<td>优化目标函数</td>
<td>\(\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}\)</td>
<td>\(\frac{1}{2}\sum_{n=1}^N{\{y(x_n,w)-t_n\}^2}+\frac{\alpha}{2\beta}w^Tw\)</td>
</tr>
</tbody>
</table>

<h2 id="toc_6">决策理论（Decision Theory）</h2>

<p>一个具体的机器学习问题的解决包括两个过程：<strong>推断（inference）</strong>和<strong>决策（decision）</strong>。前者在概率学的框架下告诉我们\(p(\mathbf{x},t)\)的分布；后者则借助<strong>决策理论（decision theory）</strong>告诉我们在这个联合概率分布下的最优反应（例如对\(t\)的预估等）。</p>

<p>一旦解决了推断的问题，决策的过程就显得异常简单。</p>

<h3 id="toc_7">分类问题的决策</h3>

<p>对于分类问题而言，我们寻求一个将输入变量\(\mathbf{x}\)映射到某一个分类上的法则。这个法则将\(\mathbf{x}\)的向量空间划分成了不同的区域\(\mathcal{R}_k\)：我们称之为<strong>decision region</strong>；位于\(\mathcal{R}_k\)里的点\(\mathbf{x}\)都被映射到类\(C_k\)上。相应的，decision region之间的边界我们称之为<strong>decision boundaries</strong>或<strong>decision surfaces</strong>。</p>

<p>如何找到这个决策面呢？</p>

<p>假定我们的决策目标是使得误分类的概率尽可能地小，借助概率论，我们有：<br/>
\[p(mistake)=1-p(correct)=1-\sum_{k=1}^K{\int_{\mathcal{R}_k}}{p(\mathbf{x},C_k)d\mathbf{x}}\]<br/>
显然，为了让\(p(mistake)\)尽可能地小，\(\mathcal{R}_k\)应是由那些使得\(p(\mathbf{x},C_k)\)最大的点构成的集合（即对于\(\mathbf{x} \in \mathcal{R}_k\)，满足\(p(\mathbf{x},C_k) \geq p(\mathbf{x},C_j)\)）。由于\(p(\mathbf{x},C_k)＝p(C_k|\mathbf{x})p(\mathbf{x})\)，也就是说我们应将\(\mathbf{x}\)映射到后验概率\(p(C_k|\mathbf{x})\)最大的类\(C_k\)上。</p>

<p>但真实的问题也许要更复杂一些。例如对癌症患者的诊断：错误的将一个癌症患者诊断为健康，远比将一个健康的人误诊为癌症要严重的多。因此，就有了<strong>损失矩阵（loss function）</strong>的概念：损失矩阵\(L\)中的元素\(L_{kj}\)用来评估当类\(C_k\)被误分类为\(C_j\)时带来的损失。显然，对任意的类\(C_k\)，总有\(L_{kk}=0\)。</p>

<p>此时，我们的决策目标是最小化期望损失函数。同样地借助概率论的知识，我们有：<br/>
\[E[L] = \sum_k \sum_j \int_{\mathcal{R}_j}{L_{kj}p(\mathbf{x},C_k)d\mathbf{x}}\]<br/>
类似地，我们选取那些使\(\sum_k L_{kj}p(\mathbf{x},C_k)\)最小的点\(\mathbf{x}\)构成的集合为类\(C_j\)的decision region \(\mathcal{R}_j\)（即对于\(\mathcal{R}_j\)里的\(\mathbf{x}\)，总有\(\sum_k L_{kj}p(\mathbf{x},C_k) \leq \sum_k L_{ki}p(\mathbf{x},C_k)\)）。</p>

<h3 id="toc_8">再次回顾推断过程</h3>

<p>在前面对分类问题决策阶段的讨论中，我们发现，我们可以在推断阶段预估\({\mathbf{x}}\)和\(C_k\)的联合概率分布\(p(\mathbf{x},C_k)\)，也可以预估\(C_k\)的后验条件概率\(p(C_k|\mathbf{x})\)。事实上，我们也可以将两个阶段整合为一个过程，直接学习\({\mathbf{x}}\)到\(C_k\)的映射。这三种不同的解决问题的思路，对应了三种不同的模型：</p>

<ol>
<li><strong>产生式模型（generative models）</strong>：产生式模型是指那些在推断阶段隐式或显式地对输入变量\(\mathbf{x}\)的分布进行了建模的模型。由贝叶斯公式，我们可以用\(\mathbf{x}\)的条件概率\(p(\mathbf{x}|C_k)\)和\(C_k\)的先验概率\(p(C_k)\)计算\(C_k\)的后验概率：\[p(C_k|\mathbf{x})=\frac{p(\mathbf{x}|C_k)p(C_k)}{p(\mathbf{x})}\]在得到\(C_k\)的后验概率后，进入决策阶段分类。这个过程等价于对联合概率分布\(p(\mathbf{x},C_k)\)进行建模（上式中的分子）。</li>
<li><strong>判别式模型（discriminative models）</strong>：判别式模型是指那些在推断阶段直接对目标变量的后验概率进行建模的模型。例如前面提到的多项式拟合模型。</li>
<li><strong>判别式函数（discriminant function）</strong>：通过一个判别式函数\(f(\mathbf{x})\)可以将输入变量\(\mathbf{x}\)直接映射到一个类标上；省去了对概率分布的预估。Andrew在Coursera上的课程里提到的假说（Hypothesis）就是这类函数。</li>
</ol>

<p>总的来说，三类模型各有利弊。相对来讲，产生式模型最为复杂，也需要最多的训练数据；但是由于可以对输入变量的分布进行预估，功能上也最为强大。而我们用的最多的一般是判别式模型。由于引入了对后验概率的预估，使得我们可以方便的处理非平衡样本问题，以及多模型的融合。这是简单的判别式函数所不具备的。</p>

<h3 id="toc_9">回归问题的决策</h3>

<p>与分类问题类似，回归问题的决策阶段也是要从目标变量的概率分布中选择一个特定的预估值，以最小化某个损失函数。回归问题的损失函数为：\[E[L] = \int\int L(t,y(\mathbf{x}))p(\mathbf{x},t)d\mathbf{x}dt\] 积分项的意思是，对于输入变量\(\mathbf{x}\)和真实目标值\(t\)，我们的预估目标值\(y(\mathbf{x})\)带来的期望损失。一般\(L(t,y(\mathbf{x}))\)被定义为平方误差和的形式。</p>

<p>最小化这个损失函数我们得到解：\(y(\mathbf{x})=E_t[t|\mathbf{x}]\)。这个条件期望也被称之为<strong>回归函数（regression function）</strong>。在前面的多项式拟合的问题中，我们假定了目标变量\(t\)服从一个高斯分布，并用最大似然法（或Bayesian的方法）估计出了模型的参数；而通过这里的回归函数，我们才最终得到了\(t\)的一个预估值。</p>

<p>类似地，回归问题也有三种建模方式：产生式模型，判别式模型，和判别式函数。这里不再赘述。</p>

<h2 id="toc_10">信息论（Information Theory）</h2>

<p>如果说概率论给出了对不确定性的一个描述，信息论则给出了对不确定性程度的一种度量。这种度量就是信息熵。</p>

<h3 id="toc_11">信息熵</h3>

<p>我们可以从三种不同的角度理解信息熵的定义。</p>

<ol>
<li>概率学。如果我们问一个随机事件的发生会传递多少信息量？显然，一个确定事件的发生传递的信息量为0，而一个小概率事件的发生会传递更多的信息量。因此，我们对信息量（记作\(h(x)\)）的定义应该与随机事件发生的概率\(p(x)\)有关。另一方面，两个独立事件同时发生所传递的信息量应该是这两个事件各自发生的信息量之和，即\(h(x,y)=h(x)+h(y)\)。由这两个性质出发，我们可以证明，信息量\(h(x)\)必然遵循\(p(x)\)的对数形式：\[h(x) ＝ -log_{2}p(x)\] 进一步地，我们定义一个随机变量\(x\)的信息熵为其传递的信息量的期望值：\[H[x]=-\sum p(x)log_{2}p(x)\]</li>
<li>信息学。在对信息熵的第一种解读中，我们并没有解释为什么对信息量的定义采用了以自然数2为底的对数。Shannon给出了一个信息学的解释：信息熵等价于编码一个随机变量的状态所需的最短位数（bits）。对于一个均匀分布的随机变量，这是显而易见的；而对于非均匀分布的随机变量，我们可以通过对高频事件采用短编码，低频事件采用长编码的方式，来缩短平均编码长度。</li>
<li>物理学。在热力学中我们也接触到熵的定义：一种对系统无序程度的度量。熵越大，系统的无序程度越高。这里，熵被定义为一个宏观态对应微观态个数的对数。与热力学熵类似，信息熵度量了一个随机变量的不确定程度。信息熵越大，随机变量的不确定性越高。</li>
</ol>

<p>我们也可以将信息熵的定义拓展到连续型的随机变量，定义为<strong>微分熵（differential entropy）</strong>：\[H[x]=-\int{p(x)\ln{p(x)}}dx\] （如果从信息熵的原始定义出发，我们会发现微分熵的定义和信息熵相差了一个极大项：\(-\ln{\Delta}\)；这意味着连续变量本质上不可能做到精确的编码。）</p>

<p>结合拉格朗日法，我们可以推导出信息熵和微分熵最大时对应的分布：对于离散的随机变量，信息熵最大时对应着一个均匀分布；对于连续的随机变量，微分熵最大时对应着一个高斯分布（注意对于微分熵的最大值求解需要增加一阶矩和二阶矩两个约束条件）。</p>

<h3 id="toc_12">条件熵，交叉熵，与互信息</h3>

<p>从信息熵的第一定义，我们很容易写出<strong>条件信息熵</strong>的计算公式：\[H[y|x]=-\int\int p(y,x)\ln{p(y|x)}dydx\]<br/>
容易证明，\(H[x,y]=H[y|x]+H[x]\)。也就是说，\(x\)、\(y\)联合分布的不确定度，等于\(x\)的不确定度与知道\(x\)后\(y\)的不确定度之和。</p>

<p><strong>交叉熵（relative entropy）</strong>，又被称为<strong>KL divergence</strong>，衡量了两个分布的不相似性。它定义了用一个预估分布\(q(x)\)近似一个未知分布\(p(x)\)时，对\(x\)进行编码所需要的额外期望信息量：\[KL[p\|q]=-\int p(x)\ln{q(x)}dx-(-\int p(x)\ln{p(x)}dx)=-\int p(x)\ln{\{\frac{q(x)}{p(x)}\}dx}\]</p>

<p>交叉熵越大，两个分布越不相似。可以证明，交叉熵不满足对称性（\(KL[p\|q] \neq KL[q\|p]\)）；且\(KL[p\|q]\geq 0\)（当且仅当\(q(x) = p(x)\)时等号成立）。</p>

<p>我们可以用交叉熵的概念来对一个机器学习模型的参数进行预估。假设我们有一堆抽样自某未知分布\(p(x)\)的数据，我们试图用一个带参数的模型\(q(x|\theta)\)去拟合它。一条可行的思路是选择使\(p(x)\)和\(q(x|\theta)\)的交叉熵尽可能小的参数\(\theta\)。虽然我们不知道\(p(x)\)的真实分布，但由蒙特卡洛方法，我们可以从抽样数据中得到\(KL(p\|q)\)的近似值：<br/>
\[KL(p\|q) \simeq \sum_{n=1}^{N}\{-\ln{q(x_n|\theta)}+\ln{p(x_n)}\}\]<br/>
略去与\(\theta\)无关项，我们发现，我们得到的是一个似然函数的负对数！也就是说，<strong>最小化模型预估分布与真实分布的交叉熵等价于前面提到的最大似然法！</strong>这也从信息熵的角度解释了为什么要对似然函数取对数的原因。</p>

<p>另一个重要的概念是<strong>互信息（mutual information）</strong>，用来衡量两个随机变量\(x\)、\(y\)的相关性。互信息有两种定义方式：<br/>
1. \(p(x,y)\)和\(p(x)p(y)\)的交叉熵定义：\[I[x,y]=KL(p(x,y)\|p(x)p(y))=-\int\int{p(x,y)\ln{(\frac{p(x)p(y)}{p(x,y)})}}dxdy\] 当\(x\)、\(y\)相互独立时，\(p(x,y)=p(x)p(y)\)；此时，\(I[x,y]=0\)。<br/>
2. 由KL定义出发，我们可以推导出互信息的条件熵定义：\[I[x,y]=H[x]-H[x|y]=H[y]-H[y|x]\] 因此，\(x\)、\(y\)的互信息可以理解为知道其中一个随机变量的取值后，另一个随机变量的不确定度的降低（当\(I[x,y]=H[x]\)时意味着\(y\)的发生确定了\(x\)的发生，即两个随机变量完全相关）。</p>

<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>通常，贝叶斯学派不会对模型的参数进行点估计，因此也不会用MAP算法优化模型。把最大后验概率算法和下面的优化目标函数放在这里，只是为了和频率学派的最大似然法进行对比。&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nature Review： Deep Learning]]></title>
    <link href="whiskytina.github.io/14903538616912.html"/>
    <updated>2017-03-24T19:11:01+08:00</updated>
    <id>whiskytina.github.io/14903538616912.html</id>
    <content type="html"><![CDATA[
<p>如今，机器学习的技术在我们的生活中扮演着越来越重要的角色。从搜索引擎到推荐系统，从图像识别到语音识别。而这些应用都开始逐渐使用一类叫做深度学习（Deep Learning）的技术。</p>

<span id="more"></span><!-- more -->

<p>传统机器学习算法的局限性在于，它们往往很难处理那些未被加工过的自然数据（natural data），例如一张原始的RGB图像。因此，构建一个传统的机器学习系统，往往需要一些有经验的工程师设计一个特征提取器，将原始数据转化成机器能识别的feature representation。</p>

<p>有一类叫做representation learning的算法可以实现让机器自发地从输入的原始数据中发现那些有用的feature。Deep Learning正是这样的一类算法。</p>

<p>下面是Lecun等人给出的Deep Learning的正式定义：</p>

<blockquote>
<p>Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. </p>
</blockquote>

<p>从这段话中可以看出，Deep Learning有三个核心的要素：</p>

<ol>
<li>a kind of representation learning methods<br/>
深度学习的精髓在于，各个layer上的特征不是由人类工程师设计的，而是通过一类general-purpose的learning procedure从数据中主动地习得。</li>
<li>with multiple levels of representation from raw to abstract<br/>
以图片为例，原始数据只是一些毫无意义的像素点构成的矩阵。而深度学习学习到的第一层特征能够检测图片中是否存在指向某个方向的线条；更高层的特征则通过组合低层级的特征，在更抽象的水平上——例如特定的花纹——进行检测。</li>
<li>non-linear transformation of representation<br/>
理论上，通过组合足够数量的非线性变换，可以对任意函数进行拟合。</li>
</ol>

<p>可见，Deep Learning非常擅长于挖掘高维数据中的内在结构，也因此在很多领域上取得了令人惊异的成果。</p>

<h2 id="toc_0">有监督学习</h2>

<p>Supervised learning，有监督学习，是机器学习一种常见的形式。它的任务是训练一个模型，使其能在给定的输入下，输出预期的value。为此，我们需要一个error function来计算输出值与期望值的误差，并通过调节模型内部的参数来减小这个误差。梯度下降（Gradient Descent）和随机梯度下降（SGD）是两种常见的参数调节的算法。</p>

<p>目前，针对有监督学习问题，大部分机器学习系统都是在人工挑选的feature上运行一个线性分类器。然而，线性分类器的缺陷在于，它只能将输入空间划分为一些简单的region，因此在诸如图像识别和语言识别的问题上往往无能为力（这些问题需要模型对一些特定特征的微小变化极其敏感，而对不相关特征的变化极不敏感）。例如，在像素层面上，同一只Samoyed在不同背景下的两张图片的差别很大，而相同背景下的Samoyed和Wolf的图片差异却很小。这对于传统的线性分类器，或是任一个浅层（Shallow）分类器，想在区分后一组图片中的Samoyed和Wolf的同时，把前一组图片中的Samoyed放在同一个类别下，几乎是一个impossible mission。这也被称之为<strong>selectivity–invariance dilemma</strong>：我们需要一组特征，它们能够选择性地响应图片中的重要部分，而对图片中不重要部分的变化保持不变性。</p>

<p>这一问题传统的解决方案是人工设计一些特征提取器。然而，借助Deep Learning，我们有希望从数据中自发地学习到这些特征。</p>

<h2 id="toc_1">反向传播算法</h2>

<p>我们可以用随机梯度下降算法（SGD）来训练一个multilayer networks的模型。这一算法也被称之为反向传播算法（Backpropagation）。该算法的背后不过是微积分第一堂课里就学到的链式求导法则。我们将误差函数对layer中一个模块的输入的偏导，表示成该误差函数对下一层layer的输入的偏导的函数，并在此基础上求出模型参数的梯度。</p>

<p>前向反馈神经网络（feedforwrad neural network）正是这样一个multilayer network。许多深度学习的模型都采用了与之类似的网络结构。在前向传播的过程中，每一层神经元都对上一层神经元的输出进行加权求和，并通过一个非线性的变换传递给下一层神经元。目前在深度学习网络中被广泛使用的非线性变换是ReLU（rectified linear unit）：\(f(z)=max(z,0)\)。与传统的平滑非线性变换（\(tanh(z)\)或logistic函数）相比，ReLU的学习速度更快。通过每一个隐藏层上对输入空间的非线性变换，我们最终得到了一个线性可分的特征空间。</p>

<p>然而，在上个世纪90年代末期，神经网络的发展遇到了极大的阻碍。人们认为，梯度下降算法会使得模型很容易陷入一些远离真实值的局部最优解。事实上，近期的一些研究表明，这些最优解大都是分布在误差空间上的鞍点；它们有着相近的误差函数值。因此，我们并不需要关心算法最终落到了哪个最优解上。</p>

<p>深度神经网络的复兴发生在2006年。CIFAR的一批研究者提出了一种逐层训练的无监督学习算法；每一个隐藏层上的神经元都试图去重构上一层神经元习得的特征，从而学习到更高级的特征表达。最终，通过一个输出层的反向传播过程来对模型的参数进行微调，得到一个有监督的学习模型。</p>

<h2 id="toc_2">卷积神经网络</h2>

<p>与全连接的前向反馈神经网络相比，卷积神经网络（Convolutional Neural Networks）更加易于训练。事实上，当整个神经网络的研究都处于低谷的时候，CNN却独树一帜，在解决许多实际的问题中都有着不俗的表现。最近几年，CNN更在计算机视觉（CV）领域中得到广泛的应用。</p>

<p>CNN一般被用于处理multiple arrays形式的数据输入。例如一段文本（1D array）；一张图像（2D array）；或是一段视频（3D array）。CNN之所以能够有效的处理这些原生态的数据，离不开它的四个核心要素：<br/>
1. 局部连接（local connections）<br/>
2. 共享权重（shared weights）<br/>
3. 池化（pooling）<br/>
4. 多层网络结构（multiple layers）</p>

<p>下图是一个卷积神经网络的典型结构，主要由两种类型的layer构成：卷积层（convolutional layer）和池化层（pooling layer）。</p>

<p><img src="media/14903538616912/14903540021967.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>卷积层由多个feature maps构成（类似原始输入数据里的通道），每一个feature maps里的神经元都通过一组权重（filter bank）与前一层所有feature maps里的部分神经元相连（local connection），并对前一层相连神经元的输出加权求和，传递给一个非线性的变换器（通常是ReLU）。值得注意的是，同一个feature map里的神经元共享同一个filter bank；不同feature maps之间的filter bank并不相同（shared weights）。这么做出于两点考虑：1. 在列状数据（array data）中，相邻的数据点一般是高度相关的；局域的连接更有利于特征的检测；2. 这种局域的统计特征往往与位置无关，从而使得不同位置的神经元可以通过共享权重检测同一个特征。数学上，一个feature map对输入特征的操作，等效于一个离散的卷积过程。这也是卷积神经网络名字的由来。</p>

<p>卷积层的作用是组合上一层的局域特征并进行检测；而池化层的作用是将检测到的距离相近的特征合并为一，从而降低特征相对位置的变化对最终结果的影响。一种常见的池化操作是maximum pooling，它对一个local patch里的神经元的状态取最大值并输出。池化操作可以有效地降低特征的维度，并增强模型的泛化能力。</p>

<p>将2-3个由卷积层、非线性变换、和池化层构成的stage堆叠在一起，与一个全连接的输出层相连，就组成了一个完整的卷积神经网络。反向传播算法依然可被用来训练这个网络中的连接权重。</p>

<p>同许多深度神经网络一样，卷积神经网络成功地利用了自然信号中内在的层级结构属性：高层级的特征由低层级的特征组成。例如，一张图片中的物体可以拆分成各个组件；每个组件又可以进一步拆分成一些基本的图案；而每个基本的图案又是由更基本的线条组成。</p>

<h2 id="toc_3">Image Understanding与深度卷积网络</h2>

<p>虽然早在2000年，卷积神经网络在图像识别的领域中就已经取得了不错的成绩；然而直到2012年的ImageNet比赛后，CNN才被计算机视觉和机器学习的主流科学家们所接受。CNN的崛起依赖于四个因素：GPU的高性能计算；ReLU的提出；一种叫做dropout的正则化技术；和一种对已有数据进行变形以生成更多的训练样本的技术。一个深度卷积神经网络通常有10-20个卷积层，数亿的权重和连接。得益于计算硬件和并行计算的高速发展，使得深度卷积神经网络的训练成为了可能。如今，深度CNN带来了计算机视觉领域的一场革命，被广泛应用于几乎所有与图像识别有关的任务中（例如无人车的自动驾驶）。最近的一项研究表明，如果将深度CNN学习到的高维特征与RNN结合在一起，甚至可以教会计算机“理解”图片里的内容。</p>

<h2 id="toc_4">Distributed Representation与自然语言处理</h2>

<p>深度学习理论指出，与传统的浅层学习模型相比，深度学习网络有两个指数级的优势：<br/>
1. 分布式的特征表达（distributed representation）使得模型的泛化空间成指数倍的增长（即便是训练空间中未出现的样本也可以通过分布式特征组合出来）；<br/>
2. 层级结构的特征表达在深度上加速了这种指数倍的增长。</p>

<p>下面以深度神经网络在自然语言处理中的一个应用，来解释distributed representation的概念。</p>

<p>假设我们需要训练一个深度神经网络来预测一段文本序列的下一个单词。我们用一个one-of-N的0-1向量来表示上下文中出现的单词。神经网络将首先通过一个embedding层为每一个输入的0-1向量生成一个word vector，并通过剩下的隐藏层将这些word vector转化为目标单词的word vector。这里的word vector就是一种distributed representation。向量中的每一个元素都对应着原始单词的某一个语义特征。这些特征互不排斥，共同表达了原始文本里的单词。要注意的是，这些语义特征即非显式地存在于原始的输入数据中，也非由专家事先指定，而是通过神经网络从输入输出的结构联系中自动挖掘出来。因此，对于我们的单词预测问题，模型学习到的word vector可以很好地表示两个单词在语义上的相似度（例如，在这个问题下，Tuesday和Wednesday这两个单词给出的word vector相似度就很高）。而传统的统计语言模型就很难做到这一点（它们通常是把单词作为一个不可分的最小单元）。</p>

<p>如今，这种从文本中学习word vector的技术被广泛应用于各种自然语言处理的问题中。</p>

<h2 id="toc_5">递归神经网络</h2>

<p>递归神经网络（Recurrent Neural Network）通常用于处理一些序列的输入（例如语音或文本）。它的基本思想是，一次只处理输入序列中的一个元素，但在hidden units中维护一个状态向量，隐式地编码之前输入的历史信息。如果我们将不同时刻的隐藏单元在空间上展开，就得到了一个（时间）深度网络。显然，我们可以在这个深度网络上运用反向传播算法来训练一个RNN模型。</p>

<p><img src="media/14903538616912/14903540247202.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>在RNN模型中，每一个时刻的状态向量\(s_t\)都由上一时刻的状态向量\(s_{t-1}\)和当前时刻的输入\(x_t\)所决定。通过这种递归的方式，RNN将每一时刻的输入\(x_t\)都映射为一个依赖其历史所有输入的输出\(o_t\)。注意，模型中的参数（\(U,V,W\)）是与序列时刻无关的权重。</p>

<p>RNN在自然语言处理上有很多应用。例如，可以训练一个RNN模型，将一段英文“编码”成一个语义向量，再训练另一个RNN模型，将语义向量“解码”成一段法文。这就实现了一个基于深度学习的翻译系统。除此之外，在“编码”阶段，我们还可以用一个深度卷积网络将一张原始的图片转化为高级的语义特征，并在此基础上训练一个RNN“解码器”，就可以实现“看图说话”的功能。</p>

<p><img src="media/14903538616912/14903540399287.jpg" alt="" class="mw_img_center" style="display: block; clear:both; margin: 0 auto;"/></p>

<p>尽管RNN设计的初衷是为了学习长记忆依赖，然而一些理论和实验的研究表明，“it is difficult to learn to store information for very long”。为此，人们提出了long short-term memory(LSTM)模型。LSTM模型通过在RNN模型的基础上引入一些特殊的中间神经元（门变量）来控制长短期记忆的均衡，被证明要比传统的RNN模型更加高效和强大。</p>

<p>还有一类模型是通过引入一个记忆存储单元来增强RNN模型的记忆能力。Neural Turing Machine和memory networks就是这一类模型。它们在处理一些知识问答的推断系统中被证明十分有效。</p>

<h2 id="toc_6">Deep Learning的未来</h2>

<ol>
<li><p>无监督学习：可以说，正是对无监督学习的研究才催化了深度学习的复兴。然而，如今无监督学习似乎已被有监督学习的巨大光芒所掩盖。考虑到人和动物大部分是通过无监督的学习来了解这个世界，长期来看，对无监督学习的研究将会愈发的重要。</p></li>
<li><p>深度学习与强化学习的结合：在CNN和RNN的基础上，结合Reinforcement Learning让计算机学会进一步的决策。这方面的研究虽尚处于萌芽，但已有一些不俗的表现。例如前段时间的AlphaGo。</p></li>
<li><p>自然语言的理解。虽然RNN已被广泛应用于自然语言处理，然而在教会机器理解自然语言的目标上，还有很长的一段路要走。</p></li>
<li><p>特征学习和特征推断的结合。这或许会极大地推动人工智能的发展</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[（转自MWeb官方文档）Markdown 语法和 MWeb 写作使用说明]]></title>
    <link href="whiskytina.github.io/14902751600359.html"/>
    <updated>2017-03-23T21:19:20+08:00</updated>
    <id>whiskytina.github.io/14902751600359.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Markdown 的设计哲学</h2>

<blockquote>
<p>Markdown 的目標是實現「易讀易寫」。<br/>
不過最需要強調的便是它的可讀性。一份使用 Markdown 格式撰寫的文件應該可以直接以純文字發佈，並且看起來不會像是由許多標籤或是格式指令所構成。<br/>
Markdown 的語法有個主要的目的：用來作為一種網路內容的<em>寫作</em>用語言。</p>
</blockquote>

<span id="more"></span><!-- more -->

<h2 id="toc_1">本文约定</h2>

<p>如果有写 <code>效果如下：</code>， 在 MWeb 编辑状态下只有用 <code>CMD + R</code> 预览才可以看效果。</p>

<h2 id="toc_2">标题</h2>

<p>Markdown 语法：</p>

<pre><code># 第一级标题 `&lt;h1&gt;` 
## 第二级标题 `&lt;h2&gt;` 
###### 第六级标题 `&lt;h6&gt;` 
</code></pre>

<p>效果如下：</p>

<h1 id="toc_3">第一级标题 <code>&lt;h1&gt;</code></h1>

<h2 id="toc_4">第二级标题 <code>&lt;h2&gt;</code></h2>

<h6 id="toc_5">第六级标题 <code>&lt;h6&gt;</code></h6>

<h2 id="toc_6">强调</h2>

<p>Markdown 语法：</p>

<pre><code>*这些文字会生成`&lt;em&gt;`*
_这些文字会生成`&lt;u&gt;`_

**这些文字会生成`&lt;strong&gt;`**
__这些文字会生成`&lt;strong&gt;`__
</code></pre>

<p>在 MWeb 中的快捷键为： <code>CMD + U</code>、<code>CMD + I</code>、<code>CMD + B</code><br/>
效果如下：</p>

<p><em>这些文字会生成<code>&lt;em&gt;</code></em><br/>
<u>这些文字会生成<code>&lt;u&gt;</code></u></p>

<p><strong>这些文字会生成<code>&lt;strong&gt;</code></strong><br/>
<strong>这些文字会生成<code>&lt;strong&gt;</code></strong></p>

<h2 id="toc_7">换行</h2>

<p>四个及以上空格加回车。<br/>
如果不想打这么多空格，只要回车就为换行，请勾选：<code>Preferences</code> - <code>Themes</code> - <code>Translate newlines to &lt;br&gt; tags</code></p>

<h2 id="toc_8">列表</h2>

<h3 id="toc_9">无序列表</h3>

<p>Markdown 语法：</p>

<pre><code>* 项目一 无序列表 `* + 空格键`
* 项目二
    * 项目二的子项目一 无序列表 `TAB + * + 空格键`
    * 项目二的子项目二
</code></pre>

<p>在 MWeb 中的快捷键为： <code>Option + U</code><br/>
效果如下：</p>

<ul>
<li>项目一 无序列表 <code>* + 空格键</code></li>
<li>项目二

<ul>
<li>项目二的子项目一 无序列表 <code>TAB + * + 空格键</code></li>
<li>项目二的子项目二</li>
</ul></li>
</ul>

<h3 id="toc_10">有序列表</h3>

<p>Markdown 语法：</p>

<pre><code>1. 项目一 有序列表 `数字 + . + 空格键`
2. 项目二 
3. 项目三
    1. 项目三的子项目一 有序列表 `TAB + 数字 + . + 空格键`
    2. 项目三的子项目二
</code></pre>

<p>效果如下：</p>

<ol>
<li>项目一 有序列表 <code>数字 + . + 空格键</code></li>
<li>项目二 </li>
<li>项目三

<ol>
<li>项目三的子项目一 有序列表 <code>TAB + 数字 + . + 空格键</code></li>
<li>项目三的子项目二</li>
</ol></li>
</ol>

<h3 id="toc_11">任务列表（Task lists）</h3>

<p>Markdown 语法：</p>

<pre><code>- [ ] 任务一 未做任务 `- + 空格 + [ ]`
- [x] 任务二 已做任务 `- + 空格 + [x]`
</code></pre>

<p>效果如下：</p>

<ul>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> 任务一 未做任务 <code>- + 空格 + [ ]</code>
</li>
<li class="task-list-item"><input disabled="disabled" type="checkbox" checked /> 任务二 已做任务 <code>- + 空格 + [x]</code>
</li>
</ul>

<h2 id="toc_12">图片</h2>

<p>Markdown 语法：</p>

<pre><code>![GitHub set up](http://zh.mweb.im/asset/img/set-up-git.gif)
格式: ![Alt Text](url)
</code></pre>

<p><code>Control + Shift + I</code> 可插入Markdown语法。<br/>
如果是 MWeb 的文档库中的文档，还可以用拖放图片、<code>CMD + V</code> 粘贴、<code>CMD + Option + I</code> 导入这三种方式来增加图片。<br/>
效果如下：</p>

<p><img src="http://zh.mweb.im/asset/img/set-up-git.gif" alt="GitHub set up"/></p>

<h2 id="toc_13">链接</h2>

<p>Markdown 语法：</p>

<pre><code>email &lt;example@example.com&gt;
[GitHub](http://github.com)
自动生成连接  &lt;http://www.github.com/&gt;
</code></pre>

<p><code>Control + Shift + L</code> 可插入Markdown语法。<br/>
如果是 MWeb 的文档库中的文档，拖放或<code>CMD + Option + I</code> 导入非图片时，会生成连接。<br/>
效果如下：</p>

<p>Email 连接： <a href="mailto:example@example.com">example@example.com</a><br/>
<a href="http://github.com">连接标题Github网站</a><br/>
自动生成连接像： <a href="http://www.github.com/">http://www.github.com/</a> 这样</p>

<h2 id="toc_14">区块引用</h2>

<p>Markdown 语法：</p>

<pre><code>某某说:
&gt; 第一行引用
&gt; 第二行费用文字
</code></pre>

<p><code>CMD + Shift + B</code> 可插入Markdown语法。<br/>
效果如下：</p>

<p>某某说:</p>

<blockquote>
<p>第一行引用<br/>
第二行费用文字</p>
</blockquote>

<h2 id="toc_15">行内代码</h2>

<p>Markdown 语法：</p>

<pre><code>像这样即可：`&lt;addr&gt;` `code`
</code></pre>

<p><code>CMD + K</code> 可插入Markdown语法。<br/>
效果如下：</p>

<p>像这样即可：<code>&lt;addr&gt;</code> <code>code</code></p>

<h2 id="toc_16">多行或者一段代码</h2>

<p>Markdown 语法：</p>

<pre><code>```js
function fancyAlert(arg) {
  if(arg) {
    $.facebox({div:&#39;#foo&#39;})
  }

}
```
</code></pre>

<p><code>CMD + Shift + K</code> 可插入Markdown语法。<br/>
效果如下：</p>

<pre><code class="language-js">function fancyAlert(arg) {
    if(arg) {
        $.facebox({div:&#39;#foo&#39;})
    }
    
}
</code></pre>

<h2 id="toc_17">顺序图或流程图</h2>

<p>Markdown 语法：</p>

<pre><code>```sequence
张三-&gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&gt;张三: 忙得吐血，哪有时间写。
```

```flow
st=&gt;start: 开始
e=&gt;end: 结束
op=&gt;operation: 我的操作
cond=&gt;condition: 确认？

st-&gt;op-&gt;cond
cond(yes)-&gt;e
cond(no)-&gt;op
```
</code></pre>

<p>效果如下（ <code>Preferences</code> - <code>Themes</code> - <code>Enable sequence &amp; flow chart</code> 才会看到效果 ）：</p>

<pre><code class="language-sequence">张三-&gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&gt;张三: 忙得吐血，哪有时间写。
</code></pre>

<pre><code class="language-flow">st=&gt;start: 开始
e=&gt;end: 结束
op=&gt;operation: 我的操作
cond=&gt;condition: 确认？

st-&gt;op-&gt;cond
cond(yes)-&gt;e
cond(no)-&gt;op
</code></pre>

<p>更多请参考：<a href="http://bramp.github.io/js-sequence-diagrams/">http://bramp.github.io/js-sequence-diagrams/</a>, <a href="http://adrai.github.io/flowchart.js/">http://adrai.github.io/flowchart.js/</a></p>

<h2 id="toc_18">表格</h2>

<p>Markdown 语法：</p>

<pre><code>第一格表头 | 第二格表头
--------- | -------------
内容单元格 第一列第一格 | 内容单元格第二列第一格
内容单元格 第一列第二格 多加文字 | 内容单元格第二列第二格
</code></pre>

<p>效果如下：</p>

<table>
<thead>
<tr>
<th></th>
<th>第一格表头</th>
<th>第二格表头</th>
</tr>
</thead>

<tbody>
<tr>
<td>helo</td>
<td>内容单元格 第一列第一格</td>
<td>内容单元格第二列第一格</td>
</tr>
<tr>
<td>world</td>
<td>内容单元格 第一列第二格 多加文字</td>
<td>内容单元格第二列第二格</td>
</tr>
</tbody>
</table>

<h2 id="toc_19">删除线</h2>

<p>Markdown 语法：</p>

<pre><code>加删除线像这样用： ~~删除这些~~
</code></pre>

<p>效果如下：</p>

<p>加删除线像这样用： <del>删除这些</del></p>

<h2 id="toc_20">分隔线</h2>

<p>以下三种方式都可以生成分隔线：</p>

<pre><code>***

*****

- - -
</code></pre>

<p>效果如下：</p>

<hr/>

<hr/>

<hr/>

<h2 id="toc_21">MathJax</h2>

<p>Markdown 语法：</p>

<pre><code>块级公式：
$$  x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$

\\[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =
1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}
{1+\frac{e^{-8\pi}} {1+\ldots} } } } \\]

行内公式： $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$
</code></pre>

<p>效果如下（<code>Preferences</code> - <code>Themes</code> - <code>Enable MathJax</code> 才会看到效果）：</p>

<p>块级公式：<br/>
\[  x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a} \]</p>

<p>\[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =<br/>
1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}<br/>
{1+\frac{e^{-8\pi}} {1+\ldots} } } } \]</p>

<p>行内公式： \(\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N\)</p>

<h2 id="toc_22">脚注（Footnote）</h2>

<p>Markdown 语法：</p>

<pre><code>这是一个脚注：[^sample_footnote]
</code></pre>

<p>效果如下：</p>

<p>这是一个脚注：<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup></p>

<h2 id="toc_23">注释和阅读更多</h2>

<!-- comment -->

<!-- more -->

<p>Actions-&gt;Insert Read More Comment <em>或者</em> <code>Command + .</code><br/>
<strong>注</strong> 阅读更多的功能只用在生成网站或博客时。</p>

<h2 id="toc_24">TOC</h2>

<p>Markdown 语法：</p>

<pre><code>[TOC]
</code></pre>

<p>效果如下：</p>

<ul>
<li>
<a href="#toc_0">Markdown 的设计哲学</a>
</li>
<li>
<a href="#toc_1">本文约定</a>
</li>
<li>
<a href="#toc_2">标题</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">第一级标题 <code>&lt;h1&gt;</code></a>
<ul>
<li>
<a href="#toc_4">第二级标题 <code>&lt;h2&gt;</code></a>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<a href="#toc_5">第六级标题 <code>&lt;h6&gt;</code></a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_6">强调</a>
</li>
<li>
<a href="#toc_7">换行</a>
</li>
<li>
<a href="#toc_8">列表</a>
<ul>
<li>
<a href="#toc_9">无序列表</a>
</li>
<li>
<a href="#toc_10">有序列表</a>
</li>
<li>
<a href="#toc_11">任务列表（Task lists）</a>
</li>
</ul>
</li>
<li>
<a href="#toc_12">图片</a>
</li>
<li>
<a href="#toc_13">链接</a>
</li>
<li>
<a href="#toc_14">区块引用</a>
</li>
<li>
<a href="#toc_15">行内代码</a>
</li>
<li>
<a href="#toc_16">多行或者一段代码</a>
</li>
<li>
<a href="#toc_17">顺序图或流程图</a>
</li>
<li>
<a href="#toc_18">表格</a>
</li>
<li>
<a href="#toc_19">删除线</a>
</li>
<li>
<a href="#toc_20">分隔线</a>
</li>
<li>
<a href="#toc_21">MathJax</a>
</li>
<li>
<a href="#toc_22">脚注（Footnote）</a>
</li>
<li>
<a href="#toc_23">注释和阅读更多</a>
</li>
<li>
<a href="#toc_24">TOC</a>
</li>
</ul>


<div class="footnotes">
<hr/>
<ol>

<li id="fn1">
<p>这里是脚注信息&nbsp;<a href="#fnref1" rev="footnote">&#8617;</a></p>
</li>

</ol>
</div>

]]></content>
  </entry>
  
</feed>
